{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38664bitcollavenv498c6419dc4548969908896262922b0c",
   "display_name": "Python 3.8.6 64-bit ('colla': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql.dataframe import DataFrame\\nfrom pyspark.sql.column import Column\\nfrom pyspark.sql.types import StructType, ArrayType, StringType, DataType\\n\\nfrom pyspark.sql import SparkSession, SQLContext\\n\\n\\nspark = SparkSession.builder.appName(\"colla-tokenizer\").getOrCreate()\\nsc = spark.sparkContext\\nsqlContext = SQLContext(sc)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "from trec_car.read_data import iter_annotations\n",
    "\n",
    "from tools.dataloaders import WikipediaCBOR, PageFormat\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, DataType\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"colla-tokenizer\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded from cache\n"
     ]
    }
   ],
   "source": [
    "wikipedia_cbor = WikipediaCBOR(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\",\n",
    "                                \"wikipedia/car-wiki2020-01-01/partitions\",\n",
    "                                # repreprocess=True, page_lim=1000\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "from typing import cast, Any, Optional, Tuple, Union, NamedTuple\n",
    "\n",
    "from trec_car.read_data import (AnnotationsFile, Page, Para,\n",
    "                                Section, List as ParaList, ParaLink, ParaText, ParaBody)\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from collections import defaultdict\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "print(\"Test\")\n",
    "\n",
    "def preprocess_page_new(enumerated_page: Tuple[int, Page]):\n",
    "    \"\"\"\n",
    "    Transform a list of pages into a flattened representation that can\n",
    "    then be easily (de)serialized.\n",
    "    \"\"\"\n",
    "\n",
    "    id, page = enumerated_page\n",
    "\n",
    "    # For the sake of easy link spans they are byte oriented to make\n",
    "    # it easier for the rust std\n",
    "    # page_content = StringIO()\n",
    "    split_content = []\n",
    "    orig_page_content = StringIO()\n",
    "    links = []\n",
    "\n",
    "    splitter = Whitespace()\n",
    "    # splitter = 0\n",
    "\n",
    "    # Encode a link. Cast to padding if the link was not \"common\".\n",
    "    # Call this method only after preprocessing has been done!\n",
    "    def encode_link(link):\n",
    "        #return self.key_encoder.get(link, 0)\n",
    "        return wikipedia_cbor.key_encoder.get(link, 0)\n",
    "\n",
    "    # People say pattern matching is overrated.\n",
    "    # I beg to differ.\n",
    "    # (It's also true that a tree structure for tokenization makes\n",
    "    # absolutely no sense - but I don't get to decide things apparently).\n",
    "    def handle_section(skel: Section):\n",
    "        for subskel in skel.children:\n",
    "            visit_section(subskel)\n",
    "\n",
    "    def handle_list(skel: ParaList):\n",
    "        visit_section(skel.body)\n",
    "\n",
    "    def handle_para(skel: Para):\n",
    "        paragraph = skel.paragraph\n",
    "        bodies = paragraph.bodies\n",
    "\n",
    "        for body in bodies:\n",
    "            visit_section(body)\n",
    "\n",
    "    def handle_paratext(body: ParaBody):\n",
    "        split_body = splitter.pre_tokenize_str(body.get_text())\n",
    "        #print('from handle_paratext: \"' + body.get_text() + '\"')\n",
    "\n",
    "        # take care of the space...\n",
    "        running_prefix = 0\n",
    "        if len(split_content) > 0:\n",
    "            running_prefix = split_content[-1][1][1]\n",
    "        \n",
    "        while running_prefix < len(orig_page_content.getvalue()) and orig_page_content.getvalue()[running_prefix] == \" \":\n",
    "            running_prefix += 1\n",
    "\n",
    "        orig_page_content.write(body.get_text())\n",
    "        \n",
    "        #if len(split_content) > 0:\n",
    "        #    running_prefix -= split_content[0][1][0]\n",
    "\n",
    "\n",
    "        #print(f\"After skipping: {orig_page_content.getvalue()[running_prefix:]}\")\n",
    "\n",
    "        split_body = [(text, (begin_offset + running_prefix,\n",
    "                                end_offset + running_prefix)) for text, (begin_offset, end_offset) in split_body]\n",
    "\n",
    "        # print(split_body)\n",
    "\n",
    "        for tok, (begin, end) in split_body:\n",
    "            assert tok == orig_page_content.getvalue()[begin:end], f\"generated {orig_page_content.getvalue()[begin:end]} but expected {tok}\"\n",
    "        split_content.extend(split_body)\n",
    "\n",
    "    def handle_paralink(body: ParaLink):\n",
    "        encoded_link = encode_link(body.page)\n",
    "        split_body = splitter.pre_tokenize_str(body.get_text())\n",
    "\n",
    "        #print('from handle_paralink: \"' + body.get_text() + '\"')\n",
    "\n",
    "        #running_prefix = running_length_count# + int(running_length_count != 0)\n",
    "        if len(split_content) > 0:\n",
    "            running_prefix = split_content[-1][1][1]\n",
    "        \n",
    "        while running_prefix < len(orig_page_content.getvalue()) and orig_page_content.getvalue()[running_prefix] == \" \":\n",
    "            running_prefix += 1\n",
    "\n",
    "        orig_page_content.write(body.get_text())\n",
    "        \n",
    "        \n",
    "        if len(split_content) > 0:\n",
    "            running_prefix -= split_content[0][1][0]\n",
    "\n",
    "\n",
    "        split_body = [(text, (begin_offset + running_prefix,\n",
    "                                end_offset + running_prefix)) for text, (begin_offset, end_offset) in split_body]\n",
    "\n",
    "        split_content.extend(split_body)\n",
    "\n",
    "        if len(split_body) > 0:\n",
    "            end_byte_span = split_body[-1][1][1] - 1\n",
    "            start_mention_idx = len(split_content) - len(split_body)\n",
    "            links.append((encoded_link, start_mention_idx, len(split_content)))\n",
    "\n",
    "        for tok, (begin, end) in split_body:\n",
    "            assert tok == orig_page_content.getvalue()[begin:end], f\"generated {orig_page_content.getvalue()[begin:end]} but expected {tok}\"\n",
    "\n",
    "\n",
    "    def nothing():\n",
    "        return lambda body: None\n",
    "\n",
    "    handler = defaultdict(nothing, {Section: handle_section,\n",
    "                                    Para: handle_para,\n",
    "                                    ParaList: handle_list,\n",
    "                                    ParaLink: handle_paralink,\n",
    "                                    ParaText: handle_paratext})\n",
    "\n",
    "    def visit_section(skel):\n",
    "        # Recur on the sections\n",
    "        handler[type(skel)](skel)\n",
    "\n",
    "    for skel in page.skeleton:\n",
    "        visit_section(skel)\n",
    "\n",
    "    return id, page.page_name, orig_page_content.getvalue(), split_content, links\n",
    "\n",
    "    # return PageFormat(id, page.page_name, page_content.getvalue(), links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# a = wikipedia_cbor[0]\n",
    "k = 5\n",
    "with open(wikipedia_cbor.cbor_path, \"rb\") as cbor_file:\n",
    "    topk = itertools.islice(iter_annotations(cbor_file), k)\n",
    "    topk = map(preprocess_page_new, enumerate(topk))\n",
    "\n",
    "    df = pd.DataFrame(topk, columns=[\"id\", \"title\", \"text\", \"tokenized_text\", \"links\"])\n",
    "    #df = sqlContext.createDataFrame(topk)\n",
    "    # page = next(iter_annotations(cbor_file))\n",
    "    # dataloader_page = wikipedia_cbor.preprocess_page(page)\n",
    "    # spacy_collect_mentions(dataloader_page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id      title                                               text  \\\n",
       "0   0  Anarchism  Anarchism is an anti-authoritarian political a...   \n",
       "1   1     Albedo  Albedo () (, meaning 'whiteness') is the measu...   \n",
       "2   2     Autism  Autism is a developmental disorder characteriz...   \n",
       "3   3          A  A or a is the first letter and the first vowel...   \n",
       "4   4   Achilles  In Greek mythology, Achilles ( ) or Achilleus ...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [(Anarchism, (0, 9)), (is, (10, 12)), (an, (13...   \n",
       "1  [(Albedo, (0, 6)), ((), (7, 9)), ((,, (10, 12)...   \n",
       "2  [(Autism, (0, 6)), (is, (7, 9)), (a, (10, 11))...   \n",
       "3  [(A, (0, 1)), (or, (2, 4)), (a, (5, 6)), (is, ...   \n",
       "4  [(In, (0, 2)), (Greek, (3, 8)), (mythology, (9...   \n",
       "\n",
       "                                               links  \n",
       "0  [(3818014, 3, 6), (4210689, 6, 7), (5408297, 8...  \n",
       "1  [(3849449, 12, 14), (6970944, 15, 17), (612884...  \n",
       "2  [(5748136, 3, 5), (215297, 49, 50), (5759583, ...  \n",
       "3  [(4796107, 6, 7), (4772344, 10, 12), (4357688,...  \n",
       "4  [(4207422, 1, 3), (5079607, 16, 18), (6013052,...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>text</th>\n      <th>tokenized_text</th>\n      <th>links</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Anarchism</td>\n      <td>Anarchism is an anti-authoritarian political a...</td>\n      <td>[(Anarchism, (0, 9)), (is, (10, 12)), (an, (13...</td>\n      <td>[(3818014, 3, 6), (4210689, 6, 7), (5408297, 8...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Albedo</td>\n      <td>Albedo () (, meaning 'whiteness') is the measu...</td>\n      <td>[(Albedo, (0, 6)), ((), (7, 9)), ((,, (10, 12)...</td>\n      <td>[(3849449, 12, 14), (6970944, 15, 17), (612884...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Autism</td>\n      <td>Autism is a developmental disorder characteriz...</td>\n      <td>[(Autism, (0, 6)), (is, (7, 9)), (a, (10, 11))...</td>\n      <td>[(5748136, 3, 5), (215297, 49, 50), (5759583, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>A</td>\n      <td>A or a is the first letter and the first vowel...</td>\n      <td>[(A, (0, 1)), (or, (2, 4)), (a, (5, 6)), (is, ...</td>\n      <td>[(4796107, 6, 7), (4772344, 10, 12), (4357688,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Achilles</td>\n      <td>In Greek mythology, Achilles ( ) or Achilleus ...</td>\n      <td>[(In, (0, 2)), (Greek, (3, 8)), (mythology, (9...</td>\n      <td>[(4207422, 1, 3), (5079607, 16, 18), (6013052,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df.head()\n",
    "\n",
    "#df[2][0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.dataloaders import TokenizedText\n",
    "from trie_search import RecordTrieSearch\n",
    "\n",
    "class MyRecordTrie(RecordTrieSearch):\n",
    "    def __init__(self, records):\n",
    "        super().__init__(\"<Q\", records)\n",
    "\n",
    "    def search_all_patterns(self, tokens: TokenizedText):\n",
    "        words = [tok[0] for tok in tokens]\n",
    "\n",
    "        for i, (word, span) in enumerate(tokens):\n",
    "            for pattern in self._TrieSearch__search_prefix_patterns(word, words[i+1:]):\n",
    "                weight = trie[pattern][0][0]\n",
    "                yield pattern, span[0], weight, i # exact token position\n",
    "    \n",
    "    def search_longest_patterns(self, tokens):\n",
    "        # avoid overlapping mentions\n",
    "        all_patterns = self.search_all_patterns(tokens)\n",
    "        check_field = [0] * len(tokens)\n",
    "        for pattern, start_idx, weight, idx in sorted(\n",
    "                all_patterns, key=lambda x: len(x[0]), reverse=True):\n",
    "            pattern_length = pattern.count(\" \") + 1\n",
    "            target_field = check_field[idx:idx + pattern_length]\n",
    "            check_sum = sum(target_field)\n",
    "            if check_sum == 0:\n",
    "                for i in range(pattern_length):\n",
    "                    check_field[idx + i] = 1\n",
    "                yield pattern, start_idx, weight, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "#from pyspark.sql.functions import pandas_udf\n",
    "# from spacy.attrs import LOWER, ENT_TYPE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from trie_search import RecordTrieSearch\n",
    "from heapq import merge\n",
    "\n",
    "def autolink(\n",
    "    page_id: int,\n",
    "    title: str,\n",
    "    text: str,\n",
    "    tokenized_text: List[Tuple[str, Tuple[int, int]]],\n",
    "    links: List[Tuple[int, int, int]]\n",
    ") -> List[Tuple[int, int, int]]:\n",
    "    # title_toks = nlp(title)\n",
    "    title_toks = Whitespace().pre_tokenize_str(title)\n",
    "\n",
    "    link_idx = 0\n",
    "\n",
    "    exact_mentions = {}\n",
    "    mention_builder = []\n",
    "\n",
    "    # TODO: deal with ambiguities...\n",
    "    exact_mentions[\" \".join(str(tok[0]) for tok in title_toks)] = page_id\n",
    "\n",
    "    #print(exact_mentions)\n",
    "\n",
    "    remapped_links = []\n",
    "    for link in links:\n",
    "        # revert the tokenization algorithm\n",
    "        start_byte = tokenized_text[link[1]][1][0]\n",
    "        end_byte = tokenized_text[link[2]-1][1][1]\n",
    "\n",
    "        #print(text[start_byte:end_byte])\n",
    "\n",
    "        exact_mentions[text[start_byte:end_byte]] = link[0]\n",
    "        remapped_links.append((link[0], start_byte, end_byte))\n",
    "    \n",
    "\n",
    "    #print(list(map(lambda x: (x[0], (x[1],)), exact_mentions.items())))\n",
    "\n",
    "    trie = MyRecordTrie(map(lambda x: (x[0], (x[1],)), exact_mentions.items()))\n",
    "\n",
    "    #print(trie.keys())\n",
    "\n",
    "    # print(trie.items())\n",
    "    patterns = sorted(trie.search_longest_patterns(tokenized_text), key=lambda x: x[1]) # sort by apparition\n",
    "\n",
    "\n",
    "    link = None\n",
    "    link_idx = 0\n",
    "    new_links = []\n",
    "\n",
    "    for title, new_link_id, idx, token_position in patterns:\n",
    "        new_link = (title, new_link_id, idx, idx + len(title))\n",
    "        #print(new_link)\n",
    "\n",
    "        if link_idx < len(links):\n",
    "            link = links[link_idx]\n",
    "        \n",
    "        while link_idx < len(links) - 1 and idx > link[2]:\n",
    "            link_idx += 1\n",
    "            link = links[link_idx]\n",
    "\n",
    "        if link_idx >= len(links):\n",
    "            link = None\n",
    "        \n",
    "        if link is None or idx < link[1]:\n",
    "            new_links.append(new_link)\n",
    "    \n",
    "    # return exact_mentions\n",
    "\n",
    "    return list(merge(remapped_links, new_links, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "page_id = 0\n",
    "title = \"Anarchism\"\n",
    "tokenized_text = df.iloc[0][\"tokenized_text\"]\n",
    "\n",
    "text = df.iloc[0][\"text\"]\n",
    "links = df.iloc[0][\"links\"]\n",
    "\n",
    "autolink(page_id, title, text, tokenized_text, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autolink(0, \"Anarchy\", *list(df.iloc[0])[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trie_search import RecordTrieSearch\n",
    "\n",
    "patterns = [(\"Albert Einstein\", (1,)), (\"Einstein\", (1,)), (\"Albert Hitchcock\", (2,))]\n",
    "t = RecordTrieSearch(\"<Q\", patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(1,)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['This', 'is', 'a', 'a', 'little', 'lesson', 'in', 'trickery', ',', 'this', 'is', 'going', 'down', 'in', 'history', '!.']\n('trickery', 29, 3)\n('history', 61, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "splitter = Whitespace()\n",
    "\n",
    "\n",
    "\n",
    "for p in search_all_patterns(\"This is a a little lesson in trickery, this is going down in history!.\"):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\\\w+|[^\\\\w\\\\s]+'"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "trie.splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}