{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikidata lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikidata/L536.json already downloaded. Skipping...\n"
     ]
    }
   ],
   "source": [
    "from tools.datasets import *\n",
    "\n",
    "book = \"L536\"\n",
    "\n",
    "book_df = fetch_dataset(book, provider=\"wikidata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata lexemes breakdown structure\n",
    "\n",
    "(cfr. Wikidata.ipynb). The online documentation on lexemes is pretty limited.\n",
    "\n",
    "A lexeme is a unit of lexical meaning. Morphologically speaking it can only belong to one grammatical category. Homographical lexemes (P5402) are stored as different lexemes. In this case, L536 refers to book as a noun\n",
    "\n",
    "- `lemmas`: array of lemmas of a lexeme.\n",
    "    - `#lang` (e.g. `en`): contains the basic lemma in one or more language (lang->value). In general, the word could be valid in more languages.\n",
    "    - `lexicalCategory`: an entity describing the grammatical category (verb, noun...)\n",
    "    - `language`: a lexema only corresponds to a single language. Even here, just an entity\n",
    "- `claims`: Structured like normal wikidata claims, contains grammatical features of the main lexeme and other relationships not related to senses, glosses or morphological forms. For example, `P5185` is the grammatical gender, `P5402` is a homograph lexeme.\n",
    "- `forms`: an array of morphological forms. Each form is called L{ENTITY_NAME}-F{NO} with NO starting from 1.\n",
    "    - `ìd`\n",
    "    - `representations`: like for `#lang` above, but this time it represents a morphological variation.\n",
    "    - `grammaticalFeatures`: an array of grammatical features\n",
    "- `senses`: array of senses (either a translation or a definition, depending on the start and end language)\n",
    "    - `claims`: the structure is similar to a normal claim in wikidata, but the number of predicates is circumscribed to:\n",
    "        - `P5972: translation`: associate with other lexeme forms to provide translation. The values follow the form `wd:LX-SN` where `LX` is a lexeme and `SN` is the sense number starting from S1. Human-readable annotations can be found by querying their label (`rdfs:label` or `skos:definition` on the dataset).\n",
    "        - `P5137: item for this sense`: the corresponding Wikidata Entity\n",
    "        - a few others (`P18: image`, ...)\n",
    "    - `glosses`: categorises the noun. Mainly used to disambiguate word senses. Like for `#lang` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "book_pd = pd.json_normalize(book_df[\"entities\"][\"L536\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pageid', 'ns', 'title', 'lastrevid', 'modified', 'type', 'id',\n",
       "       'lexicalCategory', 'language', 'forms', 'senses', 'lemmas.en.language',\n",
       "       'lemmas.en.value', 'claims.P5402'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L536-S1': [{'language': 'en', 'value': 'document'},\n",
       "  {'language': 'ru', 'value': 'документ'},\n",
       "  {'language': 'es', 'value': 'documento'},\n",
       "  {'language': 'it', 'value': 'documento'},\n",
       "  {'language': 'pt', 'value': 'documento'},\n",
       "  {'language': 'pt-br', 'value': 'documento'},\n",
       "  {'language': 'nl',\n",
       "   'value': 'een ingebonden bundel bedrukte of beschreven vellen papier'},\n",
       "  {'language': 'zh', 'value': '書'},\n",
       "  {'language': 'zh-hant', 'value': '書'},\n",
       "  {'language': 'zh-tw', 'value': '書'},\n",
       "  {'language': 'nan', 'value': 'tsu'},\n",
       "  {'language': 'de', 'value': 'Buch'},\n",
       "  {'language': 'tg', 'value': 'ҳуҷҷат'},\n",
       "  {'language': 'fr', 'value': 'document'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def senses(entity_df):\n",
    "    available_senses = {}\n",
    "    if \"senses\" in entity_df:\n",
    "        for sense in entity_df[\"senses\"][0]:\n",
    "            available_senses[sense[\"id\"]] = list(sense[\"glosses\"].values())\n",
    "    return available_senses\n",
    "    \n",
    "\n",
    "senses(book_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikidata/L12.json already downloaded. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portrait_df = fetch_dataset(\"L12\", provider=\"wikidata\")\n",
    "senses(portrait_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data\n",
    "\n",
    "We are collecting the top 1000 most used worst according to Wikidictionary. The counts are based on the absolute word frequency extracted from TV series and movie scripts in public domain till 2006. More details [here](https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000). From here on, we'll refer to them as WDTV.\n",
    "\n",
    "Similarly, we compare with an extraction from Project Gutemberg (synced 2006 - is there anything more modern?) (WDPG) and hunspell-en-gb -ise (HUN-GB).\n",
    "\n",
    "We also test against Wordnet and the full wiktionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape wdtv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "def scrape_wiktionary(url):\n",
    "    r = get(url)\n",
    "    parsed = BeautifulSoup(r.content, \"html.parser\")\n",
    "    tables = parsed.find_all(\"table\")\n",
    "    table = tables[0]\n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "    cols = [row.find_all(\"td\")[1].find(\"a\").text for row in rows]\n",
    "    \n",
    "    return cols\n",
    "\n",
    "\n",
    "    \n",
    "wdtv_list = scrape_wiktionary(\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/1-1000\")\n",
    "wdpg_list = scrape_wiktionary(\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wrap_open(\"wordlists/en_GB-ise.txt\") as fp:\n",
    "    hun_en_gb = fp.readlines()[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['box', 'pick', 'probably', 'write', 'face', 'twenty', 'relationship', 'my', \"isn't\", 'buddy']\n",
      "['box', 'pick', 'probably', 'write', 'face', 'twenty', 'relationship', 'my', 'be', 'not', 'buddy']\n"
     ]
    }
   ],
   "source": [
    "# This is very stupid. I am well aware of that.\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "wdtv_sample = random.sample(wdtv_list, 10)\n",
    "nlp = spacy.load('en_core_web_sm',\n",
    "                     disable=[\"tagger\", \"parser\", \"ner\", \"entity_linker\"])\n",
    "\n",
    "print(wdtv_sample)\n",
    "sample_lemmas = [tok.lemma_ for tok in nlp(\" \".join(wdtv_sample))]\n",
    "print(sample_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/wikidata/flattened_lexemes.pkl not found, dumping from SPARQL...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# TODO: replace with pyspark pipeline to ditch sparql\n",
    "def all_lexemes():\n",
    "    lexemes_file = get_filename_path(\"wikidata/flattened_lexemes.pkl\")\n",
    "    if not os.path.isfile(lexemes_file):\n",
    "        print(f\"File {lexemes_file} not found, dumping from SPARQL...\")\n",
    "        find_lexemes_query = \"\"\"\n",
    "        SELECT ?lexeme ?lemma ?form ?word\n",
    "        WHERE\n",
    "        {\n",
    "          ?lexeme dct:language wd:Q1860;\n",
    "                  wikibase:lemma ?lemma;\n",
    "                  ontolex:lexicalForm ?form.\n",
    "          ?form ontolex:representation ?word .\n",
    "        }\n",
    "        \"\"\"\n",
    "        result = wikidata_sparql.run_query(find_lexemes_query)\n",
    "        result.to_pickle(lexemes_file)\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"File {lexemes_file} dumped\")\n",
    "        return pd.read_pickle(lexemes_file)\n",
    "\n",
    "samples = all_lexemes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lexeme.type</th>\n",
       "      <th>lexeme.value</th>\n",
       "      <th>lemma.xml:lang</th>\n",
       "      <th>lemma.type</th>\n",
       "      <th>lemma.value</th>\n",
       "      <th>form.type</th>\n",
       "      <th>form.value</th>\n",
       "      <th>word.xml:lang</th>\n",
       "      <th>word.type</th>\n",
       "      <th>word.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L16917</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>hide</td>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L16917-F1</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>hide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L16917</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>hide</td>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L16917-F2</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>hides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L13310</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>compromise</td>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L13310-F1</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>compromise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L13310</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>compromise</td>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L13310-F2</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>compromises</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L133</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>evaluate</td>\n",
       "      <td>uri</td>\n",
       "      <td>http://www.wikidata.org/entity/L133-F1</td>\n",
       "      <td>en</td>\n",
       "      <td>literal</td>\n",
       "      <td>evaluate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lexeme.type                           lexeme.value lemma.xml:lang  \\\n",
       "0         uri  http://www.wikidata.org/entity/L16917             en   \n",
       "1         uri  http://www.wikidata.org/entity/L16917             en   \n",
       "2         uri  http://www.wikidata.org/entity/L13310             en   \n",
       "3         uri  http://www.wikidata.org/entity/L13310             en   \n",
       "4         uri    http://www.wikidata.org/entity/L133             en   \n",
       "\n",
       "  lemma.type lemma.value form.type                                form.value  \\\n",
       "0    literal        hide       uri  http://www.wikidata.org/entity/L16917-F1   \n",
       "1    literal        hide       uri  http://www.wikidata.org/entity/L16917-F2   \n",
       "2    literal  compromise       uri  http://www.wikidata.org/entity/L13310-F1   \n",
       "3    literal  compromise       uri  http://www.wikidata.org/entity/L13310-F2   \n",
       "4    literal    evaluate       uri    http://www.wikidata.org/entity/L133-F1   \n",
       "\n",
       "  word.xml:lang word.type   word.value  \n",
       "0            en   literal         hide  \n",
       "1            en   literal        hides  \n",
       "2            en   literal   compromise  \n",
       "3            en   literal  compromises  \n",
       "4            en   literal     evaluate  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lexeme.type', 'lexeme.value', 'lemma.xml:lang', 'lemma.type',\n",
       "       'lemma.value', 'form.type', 'form.value', 'word.xml:lang', 'word.type',\n",
       "       'word.value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sampled lemma, is there a fetched wikidata lemma covering it?\n",
    "def is_covered_lemma(dataframe, lemma):\n",
    "    found = (dataframe[\"lemma.value\"] == lemma).any()\n",
    "    #if not found:\n",
    "    #    print(f\"Lemma {lemma} not found\")\n",
    "    return found\n",
    "\n",
    "# for each sampled lemma, is there a fetched wikidata form covering it?\n",
    "def is_covered_form(dataframe, form):\n",
    "    found = (dataframe[\"word.value\"] == form).any()\n",
    "    #if not found:\n",
    "    #    print(f\"Form {form} not found\")\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_proper_nouns(tok):\n",
    "    return tok.pos_ != \"PROPN\" and not tok.text[0].isupper()\n",
    "\n",
    "def test_dataset(wordlist, sample_size=100,\n",
    "                    spacy_language_model=\"en_core_web_sm\"):\n",
    "    \"\"\"Test against the given wordlist\"\"\"\n",
    "    \n",
    "    # This is extremely stupid: extracting the lemma from a single word\n",
    "    # *will* lead to ambiguities for homographical words; POS tagging will\n",
    "    # likely break as well, despite we are mainly using it for filtering out\n",
    "    # proper nouns.\n",
    "    # For some languages (e.g. latin) it is a serious issue.\n",
    "    nlp = spacy.load(spacy_language_model,\n",
    "                         disable=[\"parser\", \"ner\", \"entity_linker\"])\n",
    "    \n",
    "    \n",
    "    if sample_size:\n",
    "        wordlist_sample = random.sample(wordlist, sample_size)\n",
    "    else:\n",
    "        wordlist_sample = wordlist\n",
    "        \n",
    "    # only consider the very first token\n",
    "    tokens = list(filter(strip_proper_nouns, [nlp(word)[0] for word in wordlist_sample]))\n",
    "    wordlist_filtered = [tok.text for tok in tokens]\n",
    "    \n",
    "    # Make sure to only consider distinct lemmas\n",
    "    lemmas_sample = list(set([tok.lemma_ for tok in tokens]))\n",
    "    extracted_lexemes = all_lexemes()\n",
    "    \n",
    "    # 1. Group the lexemes by lemma, find how many match\n",
    "    \n",
    "    matched_lemmas = [lemma for lemma in lemmas_sample\n",
    "                                     if is_covered_lemma(extracted_lexemes, lemma)]\n",
    "    \n",
    "    matched_forms = [form for form in wordlist_filtered\n",
    "                                     if is_covered_form(extracted_lexemes, form)]\n",
    "    \n",
    "    distinct_lemmas_count = len(matched_lemmas)\n",
    "    \n",
    "    # 2. Match the original wordlist sample with the forms\n",
    "    distinct_forms_count = len(matched_forms)\n",
    "    \n",
    "    # How many\n",
    "    return (distinct_lemmas_count, len(lemmas_sample),\n",
    "             distinct_forms_count, len(wordlist_filtered))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/wikidata/flattened_lexemes.pkl dumped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(586, 618, 817, 833)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset(wdtv_list, sample_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/wikidata/flattened_lexemes.pkl dumped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(355, 370, 422, 427)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset(wdpg_list, sample_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/wikidata/flattened_lexemes.pkl dumped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(274, 520, 281, 526)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset(hun_en_gb, sample_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
