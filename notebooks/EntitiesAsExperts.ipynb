{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities as Experts\n",
    "\n",
    "This notebook is a code implementation of the paper \"Entities as Experts: Sparse Memory Access with Entity Supervision\" by FÃ©vry, Baldini Soares, FitzGerald, Choi, Kwiatowski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition and high-level model description\n",
    "\n",
    "We want to perform question answering on typical one-shot questions that require external knowledge or context. For example, in order to answer the question \"Which country was Charles Darwin born in?\" one needs some text providing answers on typical structured scenarios.\n",
    "\n",
    "In this case, however, we want to rely on knowledge-graph extracted information. For example, in the question given here, we can prune out unrelated to the antropologist and evolution theorist Charles Darwins, e.g. Charles River, Darwin City etc. \n",
    "\n",
    "In the paper, the authors propose to augment BERT in the task of cloze-type question answering by leveraging an Entity Memory extracted from e.g. a Knoweldge Graph.\n",
    "\n",
    "![Entity as Experts description](images/eae_highlevel.png)\n",
    "\n",
    "The Entity Memory is a simple bunch of embeddings of entities extracted from a Knowledge Graph. Relationships are ignored (see the Facts as Experts paper and notebook to see how they could be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "> We assume access to a corpus $D={(xi,mi)}$,where all entity mentions are detected but not necessarily  all  linked  to  entities.   We  use  English Wikipedia as our corpus, with a vocabulary of 1m entities. Entity links come from hyperlinks, leading to 32m 128 byte contexts containing 17m entity links.\n",
    "\n",
    "In the appendix B, it is explained that:\n",
    "\n",
    "> We build our training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 500 bytes,resulting in a corpus of 32 million contexts withover 17 million entity mentions. We restrict our-selves  to  the  one  million  most  frequent  entities\n",
    "(86% of the linked mentions).\n",
    "\n",
    "Given that the dump 2019-04-14 is not available at the time of writing, we will adopt the revision 2020-11-01.\n",
    "\n",
    "Entities are thus partially extracted by link annotations (e.g. they associate with each token a mention if that token belongs to a wikipedia url).\n",
    "\n",
    "## Mention Detection\n",
    "\n",
    "> In addition to the Wikipedia links, we annotaten each sentence with unlinked mention spans using the mention detector from Section 2.2\n",
    "\n",
    "The mention detection head discussed in Section 2.2 is a simple BIO sequence: each token is annotated with a B (beginning), I (inside) or O (outside) if they are respectivelly beginning, inside or outside of a mention. The reason why we use both BIO and EL is to avoid inconsistencies.\n",
    "\n",
    "There is a catch. In the paper, they explain they used Google NLP APIs to perform entity detection and linking on large-scale Wikipedia entries, that is, to have a properly annotated Wikipedia dataset.\n",
    "\n",
    "Since we cannot technically afford this, we will use spacy's entity detection and linking capabilities as a baseline. Data quality \n",
    "\n",
    "## Chunking\n",
    "\n",
    "- In theory we should split articles by chunks of 500 bytes (assuming unicode encoding), and contexts are only 128 tokens long. For simplicity by now we only limit ourselves to the first paragraph only.\n",
    "\n",
    "## Tokenization:\n",
    "\n",
    "- BERT Tokenizer (e.g. Wordpiece) using lowercase vocabulary, limited to 128 distinct word-piece tokens.\n",
    "\n",
    "## Learning hyperparameters\n",
    "\n",
    "For pretraining:\n",
    "\n",
    "> We use ADAM with a learning rate of 1e-4.  We apply warmup for the first 5% of training, decaying the learning rate afterwards.  We also apply gradient clipping with a norm of 1.0\n",
    "\n",
    "Since the decaying rate is not provided, we test with 3e-5 which seems quite standard.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "To evaluate:\n",
    "\n",
    "- TriviaQA\n",
    "- MetaQA\n",
    "- (Colla?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                        .getOrCreate()\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.dataloaders import WikipediaCBOR, BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from cache\n"
     ]
    }
   ],
   "source": [
    "wikipedia_cbor = WikipediaCBOR(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"wikipedia/car-wiki2020-01-01/partitioned\",\n",
    "                                        num_partitions=100,\n",
    "                                        max_entity_num=100_000,\n",
    "                                        token_length=128\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1999,  2375,  1010,  2019, 10061,  2003,  1037,  4766,  4861,  2008,\n",
       "          3397,  1996,  2087,  2590,  2685,  1997,  1037,  2146,  3423,  6254,\n",
       "          2030,  1997,  2195,  3141,  3423,  4981,  1012,  1996, 10061,  1997,\n",
       "          2516,  1010,  2109,  1999,  2613,  3776, 11817,  1010,  2003,  1996,\n",
       "          2062,  2691,  2433,  1997, 10061,  1012,  2019, 10061,  1997,  2516,\n",
       "          7201,  2035,  1996,  5608,  1997,  1037,  3538,  1997,  2455,  1010,\n",
       "          1037,  2160,  1010,  2030,  1037,  2311,  2077,  2009,  2234,  2046,\n",
       "          6664,  1997,  1996,  2556,  3954,  1012,  1996, 10061,  2036,  2636,\n",
       "          2035, 15046,  1055,  1010, 17234,  1010, 14344,  2015,  1010,  1998,\n",
       "          2060,  5491,  2008,  7461,  6095,  1997,  1996,  3200,  1012,  2019,\n",
       "         10061,  5577,  1037,  4677,  1997, 15210,  2013,  3954,  2000,  3954,\n",
       "          1998,  2151, 10540,  2011,  2280,  5608,  2008,  2024,  8031,  2006,\n",
       "          2101,  5608,  1012,  1037,  3154,  2516,  2000,  3200]),\n",
       " tensor([1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "         0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "         1., 1.]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia_cbor[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "bio_dataset = BIO(\"ner.csv\", 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frankly this code looks horrible - need to delve into pytorch's dataloader tools API\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "\n",
    "bs = 32\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "if not FULL_FINETUNING:\n",
    "    # use only 0.1%  of Wikipedia's dataset\n",
    "    wiki_use_size = int(0.001 * len(wikipedia_cbor))\n",
    "    wikipedia_cbor_limited, _ = random_split(wikipedia_cbor,\n",
    "                                             [wiki_use_size, len(wikipedia_cbor) - wiki_use_size],\n",
    "                                             generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    wiki_train_size = int(0.8*len(wikipedia_cbor_limited))\n",
    "    wiki_validation_size = len(wikipedia_cbor_limited) - wiki_train_size\n",
    "    \n",
    "    wikipedia_cbor_train, wikipedia_cbor_validation = random_split(wikipedia_cbor_limited,\n",
    "                                                                   [wiki_train_size, wiki_validation_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "else:\n",
    "    wiki_train_size = int(0.8*len(wikipedia_cbor))\n",
    "    wiki_validation_size = len(wikipedia_cbor) - wiki_train_size\n",
    "\n",
    "    wikipedia_cbor_train, wikipedia_cbor_validation = random_split(wikipedia_cbor,\n",
    "                                                                   [wiki_train_size, wiki_validation_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "wiki_train_sampler = RandomSampler(wikipedia_cbor_train)\n",
    "wiki_train_dataloader = DataLoader(wikipedia_cbor_train, sampler=wiki_train_sampler, batch_size=bs)\n",
    "\n",
    "wiki_validation_sampler = RandomSampler(wikipedia_cbor_validation)\n",
    "wiki_validation_dataloader = DataLoader(wikipedia_cbor_validation, sampler=wiki_validation_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_data = bio_dataset.get_pytorch_dataset()\n",
    "\n",
    "bio_train_size = int(0.7 * len(bio_data))\n",
    "bio_validation_size = len(bio_data) - bio_train_size\n",
    "\n",
    "bio_train, bio_validation = random_split(bio_data, [bio_train_size, bio_validation_size])\n",
    "\n",
    "bio_train_sampler = RandomSampler(bio_train)\n",
    "bio_train_dataloader = DataLoader(bio_train, sampler=bio_train_sampler, batch_size=bs)\n",
    "\n",
    "validation_sampler = RandomSampler(bio_validation)\n",
    "validation_dataloader = DataLoader(bio_validation, sampler=validation_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In the paper, the authors explain they used a modified BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "for x in bio_train_dataloader:\n",
    "    print(x[0].dtype)\n",
    "    print(x[1].dtype)\n",
    "    print(x[2].dtype)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.float32\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "for x in wiki_train_dataloader:\n",
    "    print(x[0].dtype)\n",
    "    print(x[1].dtype)\n",
    "    print(x[2].dtype)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, Dropout\n",
    "from transformers.modeling_bert import BertEncoder, BertModel, BertForTokenClassification\n",
    "from copy import deepcopy\n",
    "\n",
    "class TruncatedEncoder(Module):\n",
    "    def __init__(self, encoder: BertEncoder, l0: int):\n",
    "        super().__init__()\n",
    "        __doc__ = encoder.__doc__\n",
    "        self.encoder = deepcopy(encoder)\n",
    "        self.encoder.layer = self.encoder.layer[:l0]\n",
    "        \n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = self.encoder.forward.__doc__\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "class TruncatedModel(Module):\n",
    "    def __init__(self, model: BertModel, l0: int = 4):\n",
    "        super().__init__()\n",
    "        self.model = deepcopy(model)\n",
    "        self.model.encoder = TruncatedEncoder(self.model.encoder, l0)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = self.model.forward.__doc__\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "class BioClassifier(Module):\n",
    "    \"\"\"\n",
    "    BIO classifier head\n",
    "    \"\"\"\n",
    "    def __init__(self,  bertmodel: TruncatedModel):\n",
    "        super().__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.dropout = Dropout(p=0.1)\n",
    "        self.classifier = Linear(in_features=768, out_features=4, bias=True)\n",
    "        self.num_labels = 4\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return BertForTokenClassification.forward(self, *args, **kwargs)\n",
    "    \n",
    "class LinkPredictorClassifier(Module):\n",
    "    \"\"\"\n",
    "    Link predictor classifier head\n",
    "    \"\"\"\n",
    "    def __init__(self,  bertmodel: TruncatedModel):\n",
    "        super().__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.dropout = Dropout(p=0.1)\n",
    "        self.classifier = Linear(in_features=768, out_features=wikipedia_cbor.max_entity_num, bias=True)\n",
    "        self.num_labels = wikipedia_cbor.max_entity_num\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return BertForTokenClassification.forward(self, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers',\n",
    "                            'model', 'bert-base-uncased')\n",
    " \n",
    "common_model = TruncatedModel(model)\n",
    "bioclassifier = BioClassifier(common_model).to(device)\n",
    "linkpredictorclassifier = LinkPredictorClassifier(common_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL_FINETUNING = True\n",
    "\n",
    "def get_optimizer(finetuning_level, model):\n",
    "    \"\"\"\n",
    "    Get an optimizer\n",
    "    \"\"\"\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    from transformers import AdamW\n",
    "\n",
    "    if FULL_FINETUNING:\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "\n",
    "    return AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=1e-4,\n",
    "        eps=1e-8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_bio = get_optimizer(True, bioclassifier)\n",
    "optimizer_lp = get_optimizer(True, linkpredictorclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "def get_schedule(epochs, optimizer, train_dataloader):\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    return get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.05*total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "scheduler_bio = get_schedule(10, optimizer_bio, bio_train_dataloader)\n",
    "scheduler_lp = get_schedule(10, optimizer_lp, wiki_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-de9f5e21287d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbioclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bioclassifier.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(bio_train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        bioclassifier.zero_grad()\n",
    "            \n",
    "        \n",
    "        outputs = bioclassifier(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        # Someone has to explain to me why someone put the loss function inside a module\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        clip_grad_norm_(parameters=bioclassifier.parameters(),\n",
    "                        max_norm=max_grad_norm)\n",
    "    \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0.0, 0.0\n",
    "    number_eval_steps, number_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in bio_validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bioclassifier(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    eval_loss = eval_loss / len(validation_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    pred_tags = [[bio_values[p_i] for p_i, l_i in zip(p, l)]\n",
    "                    for p, l in zip(predictions, true_labels)]\n",
    "    \n",
    "    true_tags = [[bio_values[l_i] for l_i in l] for l in true_labels]\n",
    "    \n",
    "    print(f\"Validation Accuracy: {accuracy_score(pred_tags, true_tags)}\")\n",
    "    print(f\"Validation F1-Score: {f1_score(pred_tags, true_tags)}\")\n",
    "    print(f\"Validation loss: {eval_loss}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "for x in wiki_train_dataloader:\n",
    "    print(x[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=100000, bias=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkpredictorclassifier.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "torch.Size([32, 128])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 4920 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-5f53c8bcb4c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             outputs = bioclassifier(b_input_ids, token_type_ids=None,\n\u001b[0;32m---> 61\u001b[0;31m                             attention_mask=b_input_mask, labels=b_labels)\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    746\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e41bac1278ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBertForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLinkPredictorClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m                 )\n\u001b[0;32m-> 1359\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    746\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 967\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2439\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2233\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2235\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2236\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 4920 is out of bounds."
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for epoch in range(10):\n",
    "    \"\"\"\n",
    "    linkpredictorclassifier.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(wiki_train_dataloader):\n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        linkpredictorclassifier.zero_grad()\n",
    "            \n",
    "        \n",
    "        outputs = linkpredictorclassifier(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        # Someone has to explain to me why someone put the loss function inside a module\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        clip_grad_norm_(parameters=linkpredictorclassifier.parameters(),\n",
    "                        max_norm=max_grad_norm)\n",
    "    \n",
    "        optimizer_lp.step()\n",
    "        scheduler_lp.step()\n",
    "    \n",
    "    #avg_train_loss = total_loss / len(wiki_train_dataloader)\n",
    "    avg_train_loss = total_loss / len(wiki_train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    linkpredictorclassifier.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0.0, 0.0\n",
    "    number_eval_steps, number_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \"\"\"\n",
    "    \n",
    "    linkpredictorclassifier.eval()\n",
    "    \n",
    "    for step, batch in enumerate(wiki_validation_dataloader):\n",
    "        print(step)\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        print(b_labels)\n",
    "        print(b_labels.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bioclassifier(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "        # This is still quite slow, we need to get it to work fast\n",
    "        if step >= 100:\n",
    "            break\n",
    "        \n",
    "    eval_loss = eval_loss / len(wiki_validation_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    pred_tags = [[bio_values[p_i] for p_i, l_i in zip(p, l)]\n",
    "                    for p, l in zip(predictions, true_labels)]\n",
    "    \n",
    "    true_tags = [[bio_values[l_i] for l_i in l] for l in true_labels]\n",
    "    \n",
    "    tqdm.tqdm.write(f\"Validation Accuracy: {accuracy_score(pred_tags, true_tags)}\")\n",
    "    tqdm.tqdm.write(f\"Validation F1-Score: {f1_score(pred_tags, true_tags)}\")\n",
    "    tqdm.tqdm.write(f\"Validation loss: {eval_loss}\")\n",
    "    tqdm.tqdm.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', ',', 'this', 'is', 'sponge', '##bo', '##b', '!']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bioclassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5dc1b4cefb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello world, this is Spongebob!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbioclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbioclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bioclassifier' is not defined"
     ]
    }
   ],
   "source": [
    "def transform_sentence(sentence: str):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(tokens)\n",
    "    padded = pad_sequences([tokenizer.convert_tokens_to_ids(tokens)], maxlen=MAX_LEN,\n",
    "                  dtype=\"long\", value=0.0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    attention_mask = [[float(tok != 0.0) for tok in padded_] for padded_ in padded]\n",
    "    \n",
    "    return padded, attention_mask\n",
    "\n",
    "# bioclassifier.forward(tokens,)\n",
    "\n",
    "padded, attention = transform_sentence(\"Hello world, this is Spongebob!\")\n",
    "\n",
    "bioclassifier.eval()\n",
    "res = bioclassifier.forward(torch.tensor(padded).to(device), token_type_ids=None, attention_mask=torch.tensor(attention).to(device), labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Embedding, Dropout, ModuleList, Linear\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "GELU = torch.nn.GELU\n",
    "LayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "l0 = 4\n",
    "l1 = 8\n",
    "\n",
    "    \n",
    "class EntityMemory(Module):\n",
    "    \"\"\"\n",
    "    Entity Memory, as described in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size: int, entity_size: int,\n",
    "                   entity_embedding_size: int):\n",
    "        \"\"\"\n",
    "        :param embedding_size the size of an embedding. In the EaE paper it is called d_emb, previously as d_k\n",
    "            (attention_heads * embedding_per_head)\n",
    "        :param entity_size also known as N in the EaE paper, the maximum number of entities we store\n",
    "        :param entity_embedding_size also known as d_ent in the EaE paper, the embedding of each entity\n",
    "        \n",
    "        \"\"\"\n",
    "        self.N = entity_size\n",
    "        self.d_ent = entity_embedding_size\n",
    "        self.w_f = Linear(d_ent, 2*embedding_size)\n",
    "        \n",
    "    def forward(self, x, entity_spans, num_entities, k=None):\n",
    "        \"\"\"\n",
    "        :param x the (raw) output of the first transformer block. It has a shape:\n",
    "                B x N x (embed_size)\n",
    "        :param entity_spans entities and spans of such entities.\n",
    "                Shape: B x C x 3. Each \"row\" contains a triple (e_k, s_mi, t_mi)\n",
    "                where e_k is an (encoded) entity id, s_mi and t_mi are indices.\n",
    "        :param num_entities the number of found entities for each batch.\n",
    "        :param k the number of nearest entities to consider when softmax-ing.\n",
    "                if k = None, all the entities are used.\n",
    "                In the paper, one should set k for when running the inference\n",
    "        \"\"\"\n",
    "        \n",
    "        mentions = [entity_spans[:, :mentions_per_batch] for mentions_per_batch in num_entities]\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
