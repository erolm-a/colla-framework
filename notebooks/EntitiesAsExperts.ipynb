{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities as Experts\n",
    "\n",
    "This notebook is a code implementation of the paper \"Entities as Experts: Sparse Memory Access with Entity Supervision\" by FÃ©vry, Baldini Soares, FitzGerald, Choi, Kwiatowski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition and high-level model description\n",
    "\n",
    "We want to perform question answering on typical one-shot questions that require external knowledge or context. For example, in order to answer the question \"Which country was Charles Darwin born in?\" one needs some text providing answers on typical structured scenarios.\n",
    "\n",
    "In this case, however, we want to rely on knowledge-graph extracted information. For example, in the question given here, we can prune out unrelated to the antropologist and evolution theorist Charles Darwins, e.g. Charles River, Darwin City etc. \n",
    "\n",
    "In the paper, the authors propose to augment BERT in the task of cloze-type question answering by leveraging an Entity Memory extracted from e.g. a Knoweldge Graph.\n",
    "\n",
    "![Entity as Experts description](images/eae_highlevel.png)\n",
    "\n",
    "The Entity Memory is a simple bunch of embeddings of entities extracted from a Knowledge Graph. Relationships are ignored (see the Facts as Experts paper and notebook to see how they could be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "> We assume access to a corpus $D={(xi,mi)}$,where all entity mentions are detected but not necessarily  all  linked  to  entities.   We  use  English Wikipedia as our corpus, with a vocabulary of 1m entities. Entity links come from hyperlinks, leading to 32m 128 byte contexts containing 17m entity links.\n",
    "\n",
    "In the appendix B, it is explained that:\n",
    "\n",
    "> We build our training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 500 bytes,resulting in a corpus of 32 million contexts withover 17 million entity mentions. We restrict our-selves  to  the  one  million  most  frequent  entities\n",
    "(86% of the linked mentions).\n",
    "\n",
    "Given that the dump 2019-04-14 is not available at the time of writing, we will adopt the revision 2020-11-01.\n",
    "\n",
    "Entities are thus partially extracted by link annotations (e.g. they associate with each token a mention if that token belongs to a wikipedia url).\n",
    "\n",
    "## Mention Detection\n",
    "\n",
    "> In addition to the Wikipedia links, we annotaten each sentence with unlinked mention spans using the mention detector from Section 2.2\n",
    "\n",
    "The mention detection head discussed in Section 2.2 is a simple BIO sequence: each token is annotated with a B (beginning), I (inside) or O (outside) if they are respectivelly beginning, inside or outside of a mention. The reason why we use both BIO and EL is to avoid inconsistencies.\n",
    "\n",
    "There is a catch. In the paper, they explain they used Google NLP APIs to perform entity detection and linking on large-scale Wikipedia entries, that is, to have a properly annotated Wikipedia dataset.\n",
    "\n",
    "Since we cannot technically afford this, we will use spacy's entity detection and linking capabilities as a baseline. Data quality \n",
    "\n",
    "## Chunking\n",
    "\n",
    "- Split articles by chunks of 500 bytes (assuming unicode encoding).\n",
    "- We will elide sentences till the last period to make sure they reach such limit without giving weird effects.\n",
    "\n",
    "## Tokenization:\n",
    "\n",
    "- BERT Tokenizer (e.g. Wordpiece) using lowercase vocabulary, limited to 128 distinct word-piece tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                        .getOrCreate()\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "# from tools.providers import WikipediaProvider\n",
    "# WikipediaProvider.dump_full_dataset(revision=\"20201101\")\n",
    "\n",
    "from trec_car import read_data\n",
    "from tools.dumps import wrap_open\n",
    "from collections import defaultdict\n",
    "#import spacy\n",
    "import torch\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')    # Download vocabulary from S3 and cache.\n",
    "\n",
    "from trec_car.read_data import Page, Section, List, Para, ParaLink, ParaText, ParaBody\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_md\", pipeline=[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_section(skel, toks, links, tokenize):\n",
    "    for subskel in skel.children:\n",
    "        visit_section(subskel, toks, links, tokenize)\n",
    "\n",
    "def handle_list(skel, toks, links, tokenize):\n",
    "    visit_section(skel.body, toks, links, tokenize)\n",
    "\n",
    "def handle_para(skel: Para, toks, links, tokenize):\n",
    "    paragraph = skel.paragraph\n",
    "    bodies = paragraph.bodies\n",
    "\n",
    "    for body in bodies:\n",
    "        visit_section(body, toks, links, tokenize)\n",
    "\n",
    "def handle_paratext(body: ParaBody, toks, links, tokenize):\n",
    "    if tokenize:\n",
    "        lemmas = tokenizer.tokenize(body.get_text())\n",
    "        toks.extend(lemmas)\n",
    "        links.extend([\"PAD\"] * len(lemmas))\n",
    "\n",
    "def handle_paralink(body: ParaLink, toks, links, tokenize):\n",
    "    lemmas = tokenizer.tokenize(body.get_text())\n",
    "    if tokenize:\n",
    "        toks.extend(lemmas)\n",
    "        links.extend([body.page] + [\"PAD\"] * (len(lemmas) - 1))\n",
    "    else:\n",
    "        links.append(body.page)\n",
    "    pass\n",
    "\n",
    "def nothing():\n",
    "    return lambda body, toks, links, tokenize: None\n",
    "\n",
    "handler = defaultdict(nothing, {Section: handle_section,\n",
    "                     Para: handle_para,\n",
    "                     List: handle_list,\n",
    "                     ParaLink: handle_paralink,\n",
    "                     ParaText: handle_paratext})\n",
    "\n",
    "\n",
    "def visit_section(skel, toks, links, tokenize=True):\n",
    "    # Recur on the sections\n",
    "    handler[type(skel)](skel, toks, links, tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's a small example to see if it's working.\n",
    "# It will likely take the first page available, Anarchism\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "with wrap_open(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"rb\") as toc:\n",
    "    for idx, page in enumerate(read_data.iter_annotations(toc)):\n",
    "        links = []\n",
    "        toks = []\n",
    "        for skel in page.skeleton:\n",
    "            visit_section(skel, toks, links, False)\n",
    "            \n",
    "        # print(Counter(links))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.dumps import get_filename_path\n",
    "\n",
    "cbor_path = get_filename_path(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\")\n",
    "\n",
    "from trec_car.read_data import AnnotationsFile, ParagraphsFile\n",
    "\n",
    "cbor_toc_annotations = AnnotationsFile(cbor_path)\n",
    "cbor_toc_paragraphs = ParagraphsFile(cbor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "keys = list(cbor_toc_annotations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7893216"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trec_car.read_data.Page at 0x7f9133788f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbor_toc_annotations.get(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks, links = [], []\n",
    "\n",
    "page = cbor_toc_annotations.get(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_use = cbor_toc_paragraphs.toc.get(b'enwiki:Touch%20(TV%20series)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page = cbor_toc_paragraphs.get(b'enwiki:U.S.%20Route%20277')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tqdm\n",
    "\n",
    "page = cbor_toc_annotations.get(b'enwiki:U.S.%20Route%20277')\n",
    "\n",
    "# cbor_toc_paragraphs.get(46102636493)\n",
    "#print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbor\n",
    "import mmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(cbor_toc_annotations.toc.values())\n",
    "values.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 7893216/7893216 [01:04<00:00, 121955.38it/s]\n"
     ]
    }
   ],
   "source": [
    "key_title = list()\n",
    "\n",
    "with mmap.mmap(cbor_toc_annotations.cbor.fileno(), 0, mmap.MAP_PRIVATE) as cbor_file:\n",
    "    for offset in tqdm.tqdm(values):\n",
    "        key_title.append(extract_from_key(offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.array(keys)\n",
    "key_title_set = set(key_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Wrestling at the 1912 Summer Olympics â Men's Greco-Roman middle\" in key_title_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "key_encoder = dict(zip(key_title, itertools.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7893216\n"
     ]
    }
   ],
   "source": [
    "max_entity_num = len(key_encoder)\n",
    "print(max_entity_num)\n",
    "\n",
    "from typing import List, Union, Iterable\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tools.dumps import get_filename_path\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from trec_car.read_data import AnnotationsFile, ParagraphsFile, Page\n",
    "\n",
    "import concurrent.futures as futures\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "extracted_links = extract_links()\n",
    "#v1 = extract_links_monothreaded(get_pages(np.arange(10)))\n",
    "#v2 = extract_links_monothreaded(get_pages(np.arange(10, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[      0,       0,       0,  ...,       9,       9,\n",
       "                              9],\n",
       "                       [     41,      91,     226,  ..., 7890336, 7890753,\n",
       "                        7893063]]),\n",
       "       values=tensor([68, 34,  1,  ..., 61, 40, 40]),\n",
       "       size=(10, 7893216), nnz=20020, dtype=torch.int32, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-81-071f57c40757>, line 60)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-81-071f57c40757>\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    def self.__get_pages (self, idx: Union[int, Iterable[int]]) -> List[Page]:\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def b2i(x):\n",
    "    return int.from_bytes(x, \"big\")\n",
    "\n",
    "class WikipediaCBOR(Dataset):\n",
    "    \"\"\"\n",
    "    This is a simple CBOR loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, cbor_path, max_entity_num=1_000_000):\n",
    "        \"\"\"\n",
    "        :param cbor_path the path of the wikipedia cbor export\n",
    "        \"\"\"\n",
    "        # Let trec deal with that in my place\n",
    "        \n",
    "        self.cbor_path = get_filename_path(cbor_path)\n",
    "        self.cbor_toc_annotations = AnnotationsFile(self.cbor_path)\n",
    "        self.cbor_toc_paragraphs = ParagraphsFile(self.cbor_path)\n",
    "        \n",
    "        self.keys = np.fromiter(cbor_toc_paragraphs.keys(), dtype='<U64')\n",
    "        self.key_encoder = LabelEncoder().fit(self.keys)\n",
    "        \n",
    "        # preprocess and find the top k unique wikipedia links\n",
    "        self.key_titles = self.extract_radable_key_titles()\n",
    "        \n",
    "        # page frequencies\n",
    "        self.total_freqs = self.__extract_links()\n",
    "        \n",
    "    def extract_readable_key_titles(self):\n",
    "        \"\"\"\n",
    "        Build a list of human-readable names of CBOR entries.\n",
    "        Compared to self.keys, these keys are not binary encoded formats.\n",
    "        \"\"\"\n",
    "        def extract_from_key(offset):\n",
    "            cbor_file.seek(offset)\n",
    "\n",
    "            # We refer to the RFC 8949 about the CBOR structure\n",
    "            # See https://tools.ietf.org/html/rfc8949 for details\n",
    "            len_first_field = b2i(cbor_file.read(1))\n",
    "            field_type = (len_first_field & (0b11100000)) >> 5\n",
    "\n",
    "            # array\n",
    "            if field_type == 0b100:\n",
    "                # ignore the next byte\n",
    "                cbor_file.read(1)\n",
    "                first_elem_header = b2i(cbor_file.read(1))\n",
    "                first_elem_len = first_elem_header & 31\n",
    "                # first_elem_tag = first_elem_header >> 5\n",
    "\n",
    "                if first_elem_len > 23:\n",
    "                    first_elem_len = b2i(cbor_file.read(first_elem_len - 23))\n",
    "\n",
    "                return cbor_file.read(first_elem_len).decode('utf-8')\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Wrong header\")\n",
    "                \n",
    "        # Sorted seeks should make the OS scheduler less confused, hopefully\n",
    "        values = list(self.cbor_toc_annotations.toc.values())\n",
    "        values.sort()\n",
    "        \n",
    "        key_titles = set()\n",
    "        \n",
    "        # If reloaded for a second time, this should be way faster.\n",
    "        with mmap.mmap(self.cbor_toc_annotations.cbor.fileno(), 0, mmap.MAP_PRIVATE) as cbor_file:\n",
    "            for offset in tqdm.tqdm(values):\n",
    "                key_titles.add(extract_from_key(offset))\n",
    "                \n",
    "        return key_titles\n",
    "     \n",
    "\n",
    "    def self.__get_pages (self, idx: Union[int, Iterable[int]]) -> List[Page]:\n",
    "        \"\"\"\n",
    "        Extract some Page's from the CBOR file.\n",
    "        \n",
    "        :arg idx the index of the pages to extract\n",
    "        :returns a list of pages\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        elif type(idx) != list and type(idx) != np.ndarray:\n",
    "            idx = [idx]\n",
    "\n",
    "        return [self.cbor_toc_annotations.get(k) for k in self.keys[idx]]\n",
    "\n",
    "    def __extract_links_monothreaded(self, pages: List[Page]) -> torch.sparse.LongTensor:\n",
    "        \"\"\"\n",
    "        Calculate the frequency of each mention in wikipedia.\n",
    "        \n",
    "        :arg parges the list of Wikipedia pages\n",
    "        :returns a sparse torch tensor\n",
    "        \"\"\"\n",
    "        \n",
    "        toks = []\n",
    "\n",
    "        for page in pages:\n",
    "            # remove spurious None elements\n",
    "            if page is None:\n",
    "                continue\n",
    "            for skel in page.skeleton:\n",
    "                visit_section(skel, [], links, False)\n",
    "\n",
    "        freqs = Counter(links)\n",
    "\n",
    "        # remove mentions that do not have an associated wikipedia page\n",
    "        keys = list(freqs.keys())\n",
    "        for key in keys:\n",
    "            if key not in key_title_set:\n",
    "                del freqs[key]\n",
    "\n",
    "        keys = np.array([[0, key_encoder.get(k, -1)] for k in freqs.keys()]).T.reshape(2, -1)\n",
    "        values = np.fromiter(freqs.values(), dtype=np.int32)\n",
    "\n",
    "        return torch.sparse_coo_tensor(keys, values,\n",
    "                                             size=(1, max_entity_num))\n",
    "\n",
    "    def __extract_links(self, pages_per_worker=100, page_lim=1000):\n",
    "        \"\"\"\n",
    "        Create some page batches and count mention occurrences for each batch.\n",
    "        Summate results.\n",
    "\n",
    "        This method is threaded (hopefully for the common good).\n",
    "        \"\"\"\n",
    "\n",
    "        if page_lim is None:\n",
    "            page_lim = len(keys)\n",
    "\n",
    "        starting_tensor = torch.sparse.LongTensor(1, max_entity_num)\n",
    "        tensors = []\n",
    "        with futures.ThreadPoolExecutor() as executor:\n",
    "            promises = []\n",
    "            for i in range(0, page_lim, pages_per_worker):\n",
    "                pages = self.__get_pages(np.arange(idx, min(idx+pages_per_worker, page_lim)))\n",
    "                promises.append(executor.submit(self.__extract_links_monothreaded, self, pages))\n",
    "            for promise in futures.as_completed(promises):\n",
    "                tensors.append(promise.result())\n",
    "\n",
    "        return torch.sparse.sum(torch.stack(tensors), [1])\n",
    "    \n",
    "    \n",
    "    def __extract_links_monothreaded(self, pages: List[Page]):\n",
    "        \"\"\"\n",
    "        Calculate the frequency of each mention in wikipedia.\n",
    "        Return a sparse torch tensor\n",
    "        \"\"\"\n",
    "        toks = []\n",
    "        \n",
    "        for skel in page.skeleton:\n",
    "            visit_section(skel, [], links, False)\n",
    "        \n",
    "        keys = self.key_ecnoder.transform(np.fromiter(freqs.keys(), dtype='<U64'))\n",
    "        \n",
    "        return torch.sparse_coo_tensor(keys, freqs.values(), self.max_entity_num)\n",
    "    \n",
    "    def __get_pages(self, idx: Union[int, Iterable[int]]):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        elif type(idx) != list and type(idx) != np.array:\n",
    "            idx = [idx]\n",
    "        \n",
    "        return [self.cbor_toc_annotations.get(k.encode('ascii')) for k in self.keys[idx]]\n",
    "        \n",
    "    def __extract_links(self, pages_per_worker=100, page_lim=1000):\n",
    "        \"\"\"\n",
    "        Create some page batches and count mention occurrences for each batch.\n",
    "        Summate results.\n",
    "        \n",
    "        This method is threaded (hopefully for the common good).\n",
    "        \"\"\"\n",
    "        \n",
    "        if page_lim is None:\n",
    "            page_lim = len(self)\n",
    "            \n",
    "        starting_tensor = torch.sparse.LongTensor(1, page_lim)\n",
    "        tensors = []\n",
    "        with futures.ThreadPoolExecutor() as executor:\n",
    "            promises = []\n",
    "            for i in range(0, len(self), pages_per_worker):\n",
    "                pages = self.__get_pages(np.arange(idx, min(idx+pages_per_worker, page_len)))\n",
    "                promises.append(executor.submit(self.__extract_links_monothreaded, self, pages))\n",
    "            for promise in promises.as_completed(futures):\n",
    "                tensor.append(promise.result())\n",
    "                \n",
    "        return torch.tensor(tensors).sum(1)\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __tokenize(self, page: Page):\n",
    "        toks = []\n",
    "        links = []\n",
    "        for skel in page.skeleton:\n",
    "            visit_section(skel, toks, links)\n",
    "        return toks, links\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        elif type(idx) != list:\n",
    "            idx = [idx]\n",
    "        \n",
    "        pages = [self.cbor_toc_annotations.get(k.encode('ascii')) for k in self.keys[idx]]\n",
    "        \n",
    "        # can we parallelize this?\n",
    "        result = [self.__tokenize(page) for page in pages]\n",
    "        print(result)\n",
    "        \n",
    "        return torch.tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'futures' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b0342dd46363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcbor_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikipediaCBOR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbor_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-edeaf1bd9bee>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cbor_path, max_entity_num)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# preprocess and find the top k unique wikipedia links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_freqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__extract_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-edeaf1bd9bee>\u001b[0m in \u001b[0;36m__extract_links\u001b[0;34m(self, pages_per_worker, page_lim)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mstarting_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_lim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpages_per_worker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'futures' referenced before assignment"
     ]
    }
   ],
   "source": [
    "cbor_dataloader = WikipediaCBOR(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\")\n",
    "\n",
    "for idx, batch in enumerate(cbor_dataloader):\n",
    "    print(batch)\n",
    "    \n",
    "    if idx >= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page(Anarchism)\n",
      "Page(Albedo)\n",
      "Page(Autism)\n",
      "Page(A)\n",
      "Page(Achilles)\n",
      "Page(Alabama)\n",
      "Page(Abraham Lincoln)\n",
      "Page(An American in Paris)\n",
      "Page(Aristotle)\n",
      "Page(Academy Award for Best Production Design)\n"
     ]
    }
   ],
   "source": [
    "with wrap_open(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"rb\") as toc:\n",
    "    #toc.seek(46102636493)\n",
    "    \n",
    "    for idx, page in enumerate(read_data.iter_pages(toc)):\n",
    "        if idx >= 10:\n",
    "            break\n",
    "        print(page)\n",
    "        page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')    # Download vocabulary from S3 and cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " ',',\n",
       " '[',\n",
       " 'we',\n",
       " 'might',\n",
       " 'even',\n",
       " 'exclude',\n",
       " 'that',\n",
       " ']',\n",
       " '(',\n",
       " 'nice',\n",
       " 'link',\n",
       " ')']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"this is a test, [we might even exclude that](nice link)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ner.csv.zip\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from tools.dumps import wrap_open\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "!cd data && unzip -f ner.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",lemma,next-lemma,next-next-lemma,next-next-pos,next-next-shape,next-next-word,next-pos,next-shape,next-word,pos,prev-iob,prev-lemma,prev-pos,prev-prev-iob,prev-prev-lemma,prev-prev-pos,prev-prev-shape,prev-prev-word,prev-shape,prev-word,sentence_idx,shape,word,tag\n",
      "\n",
      "['index', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos', 'next-next-shape', 'next-next-word', 'next-pos', 'next-shape', 'next-word', 'pos', 'prev-iob', 'prev-lemma', 'prev-pos', 'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape', 'prev-prev-word', 'prev-shape', 'prev-word', 'sentence_idx', 'shape', 'word', 'tag', 0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (21,25,26,27,28,29,30,31,32,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>...</th>\n",
       "      <th>tag</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1050786</th>\n",
       "      <td>1048565</td>\n",
       "      <td>impact</td>\n",
       "      <td>.</td>\n",
       "      <td>__end1__</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050787</th>\n",
       "      <td>1048566</td>\n",
       "      <td>.</td>\n",
       "      <td>__end1__</td>\n",
       "      <td>__end2__</td>\n",
       "      <td>__END2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__END2__</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050788</th>\n",
       "      <td>1048567</td>\n",
       "      <td>indian</td>\n",
       "      <td>forc</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>said</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>forces</td>\n",
       "      <td>...</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050789</th>\n",
       "      <td>1048568</td>\n",
       "      <td>forc</td>\n",
       "      <td>said</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>they</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>said</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050790</th>\n",
       "      <td>1048569</td>\n",
       "      <td>said</td>\n",
       "      <td>they</td>\n",
       "      <td>respond</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>responded</td>\n",
       "      <td>PRP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>they</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050791</th>\n",
       "      <td>1048570</td>\n",
       "      <td>they</td>\n",
       "      <td>respond</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>responded</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050792</th>\n",
       "      <td>1048571</td>\n",
       "      <td>respond</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050793</th>\n",
       "      <td>1048572</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>attack</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050794</th>\n",
       "      <td>1048573</td>\n",
       "      <td>the</td>\n",
       "      <td>attack</td>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>with</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>attack</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050795</th>\n",
       "      <td>1048574</td>\n",
       "      <td>attack</td>\n",
       "      <td>with</td>\n",
       "      <td>machine-gun</td>\n",
       "      <td>JJ</td>\n",
       "      <td>contains-hyphen</td>\n",
       "      <td>machine-gun</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>with</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           index    lemma next-lemma next-next-lemma next-next-pos  \\\n",
       "1050786  1048565   impact          .        __end1__      __END1__   \n",
       "1050787  1048566        .   __end1__        __end2__      __END2__   \n",
       "1050788  1048567   indian       forc            said           VBD   \n",
       "1050789  1048568     forc       said            they           PRP   \n",
       "1050790  1048569     said       they         respond           VBD   \n",
       "1050791  1048570     they    respond              to            TO   \n",
       "1050792  1048571  respond         to             the            DT   \n",
       "1050793  1048572       to        the          attack            NN   \n",
       "1050794  1048573      the     attack            with            IN   \n",
       "1050795  1048574   attack       with     machine-gun            JJ   \n",
       "\n",
       "         next-next-shape next-next-word  next-pos next-shape  next-word  ...  \\\n",
       "1050786         wildcard       __END1__         .      punct          .  ...   \n",
       "1050787         wildcard       __END2__  __END1__   wildcard   __END1__  ...   \n",
       "1050788        lowercase           said       NNS  lowercase     forces  ...   \n",
       "1050789        lowercase           they       VBD  lowercase       said  ...   \n",
       "1050790        lowercase      responded       PRP  lowercase       they  ...   \n",
       "1050791        lowercase             to       VBD  lowercase  responded  ...   \n",
       "1050792        lowercase            the        TO  lowercase         to  ...   \n",
       "1050793        lowercase         attack        DT  lowercase        the  ...   \n",
       "1050794        lowercase           with        NN  lowercase     attack  ...   \n",
       "1050795  contains-hyphen    machine-gun        IN  lowercase       with  ...   \n",
       "\n",
       "           tag              0                1               2           3  \\\n",
       "1050786      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050787      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050788  B-gpe  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050789      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050790      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050791      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050792      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050793      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050794      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050795      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "\n",
       "                 4             5      6     7    8  \n",
       "1050786  prev-word  sentence_idx  shape  word  tag  \n",
       "1050787  prev-word  sentence_idx  shape  word  tag  \n",
       "1050788  prev-word  sentence_idx  shape  word  tag  \n",
       "1050789  prev-word  sentence_idx  shape  word  tag  \n",
       "1050790  prev-word  sentence_idx  shape  word  tag  \n",
       "1050791  prev-word  sentence_idx  shape  word  tag  \n",
       "1050792  prev-word  sentence_idx  shape  word  tag  \n",
       "1050793  prev-word  sentence_idx  shape  word  tag  \n",
       "1050794  prev-word  sentence_idx  shape  word  tag  \n",
       "1050795  prev-word  sentence_idx  shape  word  tag  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The columns are a bit irregular.\n",
    "names = []\n",
    "with wrap_open(\"ner.csv\", \"r\", encoding=\"latin1\") as f:\n",
    "    print(f.readline())\n",
    "    f.seek(0)\n",
    "    names = [\"index\"] + f.readline().strip().split(\",\")[1:]\n",
    "    names = names + list(range(34 - len(names)))\n",
    "\n",
    "print(names)\n",
    "\n",
    "with wrap_open(\"ner.csv\", \"rb\") as f:\n",
    "    f.readline() # skip the first line\n",
    "    data = pd.read_csv(f, encoding=\"latin1\", names=names).fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          O\n",
       "1          O\n",
       "2          O\n",
       "3          O\n",
       "4          O\n",
       "          ..\n",
       "1050791    O\n",
       "1050792    O\n",
       "1050793    O\n",
       "1050794    O\n",
       "1050795    O\n",
       "Name: bio, Length: 1050796, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simplify_bio(column):\n",
    "    return column[0]\n",
    "\n",
    "data[\"bio\"] = data[\"tag\"].apply(simplify_bio)\n",
    "data[\"bio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 75 ## can replace with 512 as per the original paper\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(s):\n",
    "    return [(w, t) for w, t in zip(s[\"word\"].values.tolist(), s[\"bio\"].values.tolist())]\n",
    "\n",
    "sentences = [s for s in data.groupby(\"sentence_idx\").apply(aggregate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'O'),\n",
       " ('marched', 'O'),\n",
       " ('from', 'O'),\n",
       " ('the', 'O'),\n",
       " ('Houses', 'O'),\n",
       " ('of', 'O'),\n",
       " ('Parliament', 'O'),\n",
       " ('to', 'O'),\n",
       " ('a', 'O'),\n",
       " ('rally', 'O'),\n",
       " ('in', 'O'),\n",
       " ('Hyde', 'B'),\n",
       " ('Park', 'I'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [[w[0] for w in s] for s in sentences]\n",
    "labels = [[w[1] for w in s] for s in sentences]\n",
    "\n",
    "bio_values = list(set(data[\"bio\"].values))\n",
    "bio_values.append(\"PAD\")\n",
    "# Apparently one row is misclassified.\n",
    "bio2idx = {t: i for i, t in enumerate(bio_values)}\n",
    "bio2idx['p'] = bio2idx['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_preserve_labels(sentence, text_labels):\n",
    "    \"\"\"\n",
    "    Tokenize the given sentence. Extend the corresponding label\n",
    "    for all the tokens the word is made of.\n",
    "    \n",
    "    Assumption: len(sentence) == len(text_labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    \n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        \n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "    \n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_labels = [\n",
    "    tokenize_preserve_labels(sent, labs) for sent, labs in zip(utterances, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_labels]\n",
    "tokenized_labels = [token_label_pair[1] for token_label_pair in tokenized_texts_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                                maxlen=MAX_LEN, dtype=\"long\", value=0.0, truncating=\"post\", padding=\"post\")\n",
    "labels = pad_sequences([[bio2idx.get(l) for l in lab] for lab in tokenized_labels],\n",
    "                               maxlen=MAX_LEN, value=bio2idx[\"PAD\"], padding=\"post\", dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this classification task we want to classify every token, thus mask everything else\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frankly this code looks horrible - need to delve into pytorch's dataloader tools API\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', ',', 'this', 'is', 'sponge', '##bo', '##b', '!']\n"
     ]
    }
   ],
   "source": [
    "def transform_sentence(sentence: str):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(tokens)\n",
    "    padded = pad_sequences([tokenizer.convert_tokens_to_ids(tokens)], maxlen=MAX_LEN,\n",
    "                  dtype=\"long\", value=0.0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    attention_mask = [[float(tok != 0.0) for tok in padded_] for padded_ in padded]\n",
    "    \n",
    "    return padded, attention_mask\n",
    "\n",
    "# bioclassifier.forward(tokens,)\n",
    "\n",
    "padded, attention = transform_sentence(\"Hello world, this is Spongebob!\")\n",
    "\n",
    "bioclassifier.eval()\n",
    "res = bioclassifier.forward(torch.tensor(padded).to(device), token_type_ids=None, attention_mask=torch.tensor(attention).to(device), labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(bio_values)[np.argmax(res[0].detach().cpu().numpy(), axis=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['B', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O',\n",
       "        'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "        'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O',\n",
       "        'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "        'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O',\n",
       "        'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']], dtype='<U3')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In the paper, the authors explain they used a modified BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I guess I need to use pytorch hooks to do what I\n",
    "# want. Yeeee, yet one more thing to study\n",
    "\n",
    "from torch.nn import Module, Linear, Dropout\n",
    "from transformers.modeling_bert import BertEncoder, BertModel, BertForTokenClassification\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "class TruncatedEncoder(Module):\n",
    "    def __init__(self, encoder: BertEncoder, l0: int):\n",
    "        super().__init__()\n",
    "        __doc__ = encoder.__doc__\n",
    "        self.encoder = deepcopy(encoder)\n",
    "        self.encoder.layer = self.encoder.layer[:l0]\n",
    "        \n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = self.encoder.forward.__doc__\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "class TruncatedModel(Module):\n",
    "    def __init__(self, model: BertModel, l0: int = 4):\n",
    "        super().__init__()\n",
    "        self.model = deepcopy(model)\n",
    "        self.model.encoder = TruncatedEncoder(self.model.encoder, l0)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = self.model.forward.__doc__\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "\n",
    "class BioClassifier(Module):\n",
    "    def __init__(self,  bertmodel: TruncatedEncoder):\n",
    "        super().__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.dropout = Dropout(p=0.1)\n",
    "        self.classifier = Linear(in_features=768, out_features=4, bias=True)\n",
    "        self.num_labels = 4\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return BertForTokenClassification.forward(self, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "bioclassifier = BioClassifier(TruncatedModel(model)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "param_optimizer = list(bioclassifier.named_parameters())\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "    \n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [00:58<00:00, 17.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.056947913965989645\n",
      "Validation Accuracy: 0.4191223675665646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PAD seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "  0%|          | 2/1011 [00:00<00:58, 17.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.654957695598441\n",
      "Validation loss: 0.084311828248005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.04006332797058199\n",
      "Validation Accuracy: 0.41927451526115594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<00:59, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6568595041322314\n",
      "Validation loss: 0.08999692602495177\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.028179564128208878\n",
      "Validation Accuracy: 0.41935244456814175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<00:59, 16.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6896914182746453\n",
      "Validation loss: 0.10549780948961203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.02010758651828561\n",
      "Validation Accuracy: 0.4192374060673532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<01:01, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6787399252997838\n",
      "Validation loss: 0.11534326749367524\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.014377029335612669\n",
      "Validation Accuracy: 0.4194118192782262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<00:58, 17.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6663807072729006\n",
      "Validation loss: 0.12534657229496315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.010751042502718534\n",
      "Validation Accuracy: 0.4195157250208739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<01:00, 16.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6649154647053205\n",
      "Validation loss: 0.13663531210174604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.008416197471756813\n",
      "Validation Accuracy: 0.419838575006958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<00:59, 16.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6667144529663346\n",
      "Validation loss: 0.141331840603225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.006647726145021859\n",
      "Validation Accuracy: 0.419704981909268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<00:59, 17.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6782409662023959\n",
      "Validation loss: 0.15386315876931217\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1011/1011 [01:01<00:00, 16.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.005283215134538172\n",
      "Validation Accuracy: 0.4195231468596345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1011 [00:00<00:58, 17.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1-Score: 0.6670636149284301\n",
      "Validation loss: 0.15581438350862106\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|ââââ      | 328/1011 [00:19<00:41, 16.41it/s]"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    bioclassifier.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        bioclassifier.zero_grad()\n",
    "            \n",
    "        \n",
    "        outputs = bioclassifier(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        # Someone has to explain to me why someone put the loss function inside a module\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        clip_grad_norm_(parameters=bioclassifier.parameters(),\n",
    "                        max_norm=max_grad_norm)\n",
    "    \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0.0, 0.0\n",
    "    number_eval_steps, number_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bioclassifier(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    eval_loss = eval_loss / len(validation_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    pred_tags = [[bio_values[p_i] for p_i, l_i in zip(p, l)]\n",
    "                    for p, l in zip(predictions, true_labels)]\n",
    "    \n",
    "    true_tags = [[bio_values[l_i] for l_i in l] for l in true_labels]\n",
    "    \n",
    "    print(f\"Validation Accuracy: {accuracy_score(pred_tags, true_tags)}\")\n",
    "    print(f\"Validation F1-Score: {f1_score(pred_tags, true_tags)}\")\n",
    "    print(f\"Validation loss: {eval_loss}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Embedding, Dropout, ModuleList, Linear\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "GELU = torch.nn.GELU\n",
    "LayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "l0 = 4\n",
    "l1 = 8\n",
    "\n",
    "    \n",
    "class EntityMemory(Module):\n",
    "    \"\"\"\n",
    "    Entity Memory, as described in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size: int, entity_size: int,\n",
    "                   entity_embedding_size: int):\n",
    "        \"\"\"\n",
    "        :param embedding_size the size of an embedding. In the EaE paper it is called d_emb, previously as d_k\n",
    "            (attention_heads * embedding_per_head)\n",
    "        :param entity_size also known as N in the EaE paper, the maximum number of entities we store\n",
    "        :param entity_embedding_size also known as d_ent in the EaE paper, the embedding of each entity\n",
    "        \n",
    "        \"\"\"\n",
    "        self.N = entity_size\n",
    "        self.d_ent = entity_embedding_size\n",
    "        self.w_f = Linear(d_ent, 2*embedding_size)\n",
    "        \n",
    "    def forward(self, x, entity_spans, num_entities, k=None):\n",
    "        \"\"\"\n",
    "        :param x the (raw) output of the first transformer block. It has a shape:\n",
    "                B x N x (embed_size)\n",
    "        :param entity_spans entities and spans of such entities.\n",
    "                Shape: B x C x 3. Each \"row\" contains a triple (e_k, s_mi, t_mi)\n",
    "                where e_k is an (encoded) entity id, s_mi and t_mi are indices.\n",
    "        :param num_entities the number of found entities for each batch.\n",
    "        :param k the number of nearest entities to consider when softmax-ing.\n",
    "                if k = None, all the entities are used.\n",
    "                In the paper, one should set k for when running the inference\n",
    "        \"\"\"\n",
    "        \n",
    "        mentions = [entity_spans[:, :mentions_per_batch] for mentions_per_batch in num_entities]\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
