{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities as Experts\n",
    "\n",
    "This notebook is a code implementation of the paper \"Entities as Experts: Sparse Memory Access with Entity Supervision\" by Févry, Baldini Soares, FitzGerald, Choi, Kwiatowski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition and high-level model description\n",
    "\n",
    "We want to perform question answering on typical one-shot questions that require external knowledge or context. For example, in order to answer the question \"Which country was Charles Darwin born in?\" one needs some text providing answers on typical structured scenarios.\n",
    "\n",
    "In this case, however, we want to rely on knowledge-graph extracted information. For example, in the question given here, we can prune out unrelated to the antropologist and evolution theorist Charles Darwins, e.g. Charles River, Darwin City etc. \n",
    "\n",
    "In the paper, the authors propose to augment BERT in the task of cloze-type question answering by leveraging an Entity Memory extracted from e.g. a Knoweldge Graph.\n",
    "\n",
    "![Entity as Experts description](images/eae_highlevel.png)\n",
    "\n",
    "The Entity Memory is a simple bunch of embeddings of entities extracted from a Knowledge Graph. Relationships are ignored (see the Facts as Experts paper and notebook to see how they could be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "> We assume access to a corpus $D={(xi,mi)}$,where all entity mentions are detected but not necessarily  all  linked  to  entities.   We  use  English Wikipedia as our corpus, with a vocabulary of 1m entities. Entity links come from hyperlinks, leading to 32m 128 byte contexts containing 17m entity links.\n",
    "\n",
    "In the appendix B, it is explained that:\n",
    "\n",
    "> We build our training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 500 bytes,resulting in a corpus of 32 million contexts withover 17 million entity mentions. We restrict our-selves  to  the  one  million  most  frequent  entities\n",
    "(86% of the linked mentions).\n",
    "\n",
    "Given that the dump 2019-04-14 is not available at the time of writing, we will adopt the revision 2020-11-01.\n",
    "\n",
    "Entities are thus partially extracted by link annotations (e.g. they associate with each token a mention if that token belongs to a wikipedia url).\n",
    "\n",
    "## Mention Detection\n",
    "\n",
    "> In addition to the Wikipedia links, we annotaten each sentence with unlinked mention spans using the mention detector from Section 2.2\n",
    "\n",
    "The mention detection head discussed in Section 2.2 is a simple BIO sequence: each token is annotated with a B (beginning), I (inside) or O (outside) if they are respectivelly beginning, inside or outside of a mention. The reason why we use both BIO and EL is to avoid inconsistencies.\n",
    "\n",
    "There is a catch. In the paper, they explain they used Google NLP APIs to perform entity detection and linking on large-scale Wikipedia entries, that is, to have a properly annotated Wikipedia dataset. We are going to use simple Wikipedia hyperlinks insteal (TODO: consider adding spacy annotation?).\n",
    "\n",
    "NOTE FOR MYSELF: We don't *actually* perform entity linking here - as in, we don't train a classifier from the first $l_0$ layers. Instead we'll build pseudo embeddings and try to find the entity that best matches a pseudo embedding (see later).\n",
    "\n",
    "HOWEVER, we do need that when training. The whole idea is that we get supervised data when training, however it is not always the case that we have this data at hand (see: TriviaQA, WebQuestions, ...).\n",
    "\n",
    "## Entity Memory\n",
    "\n",
    "The idea is pretty simple: we have as input $X_1$ and the mention spans $m_i = (e_i, s_{m_i}, t_{m_i})$ . Those are given as input. We don't care about e_i for the embedding calculation, but we DO care for the loss definition.\n",
    "\n",
    "(Glossing over the Entity Memory calculation...)\n",
    "\n",
    "When the entity detection is supervised, our obtained entity should be close to the found pseudo entity embedding.\n",
    "\n",
    "$$\n",
    "ELLoss = \\sum_{m_i} \\alpha_i \\cdot \\mathbb{1}_{e_{m_i} \\ne e_{\\emptyset}} , \\qquad \\alpha = softmax(E \\cdot h_{m_i})\n",
    "$$\n",
    "\n",
    "($E$ is our `EntEmbed`, so $E * h_{m_i}$ is a vector, so $\\alpha$ is a vector too (of shape $N$), and $h_{m_i}$ is a \"pseudo entity embedding\" ).\n",
    "\n",
    "## Chunking\n",
    "- In theory we should split articles by chunks of 500 bytes (assuming unicode encoding), and contexts are only 128 tokens long. For simplicity by now we only limit ourselves to the first paragraph only.\n",
    "\n",
    "## Tokenization:\n",
    "\n",
    "- BERT Tokenizer (e.g. Wordpiece) using lowercase vocabulary, limited to 128 distinct word-piece tokens.\n",
    "\n",
    "## Learning hyperparameters\n",
    "\n",
    "For pretraining:\n",
    "\n",
    "> We use ADAM with a learning rate of 1e-4.  We apply warmup for the first 5% of training, decaying the learning rate afterwards.  We also apply gradient clipping with a norm of 1.0\n",
    "\n",
    "Since the decaying rate is not provided, we test with 3e-5 which seems quite standard.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "To evaluate:\n",
    "\n",
    "- TriviaQA\n",
    "- MetaQA\n",
    "- (Colla?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.dataloaders import WikipediaCBOR, BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from cache\n"
     ]
    }
   ],
   "source": [
    "wikipedia_cbor = WikipediaCBOR(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"wikipedia/car-wiki2020-01-01/partitions\",\n",
    "                                       # top 2% most frequent items,  roughly at least 100 occurrences, with a total of  ~ 20000 entities\n",
    "                                       #cutoff_frequency=0.02, recount=True \n",
    "                                    # TODO: is this representative enough?\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freqs = wikipedia_cbor.count_frequency()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "freqs_pd_all = pd.DataFrame({\"key\": list(freqs.keys()), \"freq\": list(freqs.values())})\n",
    "\n",
    "freqs_pd_all.iloc[freqs_pd_all[\"freq\"].idxmax(1)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "freqs_sorted = freqs_pd_all.sort_values(by=\"freq\", ascending=False)\n",
    "\n",
    "threshold = 100\n",
    "# pd.DataFrame.all([freqs_sorted[\"freq\"] > threshold, freqs_sorted[\"key\"] > 0])\n",
    "not_zero = freqs_sorted[freqs_sorted[\"key\"] > 0]\n",
    "picked_freqs = not_zero[not_zero[\"freq\"] > threshold]\n",
    "\n",
    "freq_vals = picked_freqs[\"freq\"].to_numpy()\n",
    "\n",
    "plt.bar(np.arange(len(freq_vals)), freq_vals, log=True)\n",
    "\n",
    "print(len(freq_vals) / len(freqs_sorted))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(len(freq_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from cache\n"
     ]
    }
   ],
   "source": [
    "bio_dataset = BIO(\"ner.csv\", 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "# Frankly this code looks horrible - need to delve into pytorch's dataloader tools API\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "\n",
    "bs = 64\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if n_gpu > 0:\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if not FULL_FINETUNING:\n",
    "    # use only 1%  of our dataset.\n",
    "    wiki_use_size = int(0.01 * len(wikipedia_cbor))\n",
    "    wikipedia_cbor_limited, _ = random_split(wikipedia_cbor,\n",
    "                                             [wiki_use_size, len(wikipedia_cbor) - wiki_use_size],\n",
    "                                             generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    wiki_train_size = int(0.8*len(wikipedia_cbor_limited))\n",
    "    wiki_validation_size = len(wikipedia_cbor_limited) - wiki_train_size\n",
    "    \n",
    "    wikipedia_cbor_train, wikipedia_cbor_validation = random_split(wikipedia_cbor_limited,\n",
    "                                                                   [wiki_train_size, wiki_validation_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "else:\n",
    "    wiki_train_size = int(0.8*len(wikipedia_cbor))\n",
    "    wiki_validation_size = len(wikipedia_cbor) - wiki_train_size\n",
    "\n",
    "    wikipedia_cbor_train, wikipedia_cbor_validation = random_split(wikipedia_cbor,\n",
    "                                                                   [wiki_train_size, wiki_validation_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "wiki_train_sampler = RandomSampler(wikipedia_cbor_train)\n",
    "wiki_train_dataloader = DataLoader(wikipedia_cbor_train, sampler=wiki_train_sampler, batch_size=bs)\n",
    "\n",
    "wiki_validation_sampler = RandomSampler(wikipedia_cbor_validation)\n",
    "wiki_validation_dataloader = DataLoader(wikipedia_cbor_validation, sampler=wiki_validation_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_data = bio_dataset.get_pytorch_dataset()\n",
    "\n",
    "bio_train_size = int(0.7 * len(bio_data))\n",
    "bio_validation_size = len(bio_data) - bio_train_size\n",
    "\n",
    "bio_train, bio_validation = random_split(bio_data, [bio_train_size, bio_validation_size])\n",
    "\n",
    "bio_train_sampler = RandomSampler(bio_train)\n",
    "bio_train_dataloader = DataLoader(bio_train, sampler=bio_train_sampler, batch_size=bs)\n",
    "\n",
    "bio_validation_sampler = RandomSampler(bio_validation)\n",
    "bio_validation_dataloader = DataLoader(bio_validation, sampler=bio_validation_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In the paper, the authors explain they used a modified BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Linear, Dropout\n",
    "from transformers.modeling_bert import BertEncoder, BertModel, BertForTokenClassification\n",
    "from copy import deepcopy\n",
    "\n",
    "class TruncatedEncoder(Module):\n",
    "    def __init__(self, encoder: BertEncoder, l0: int):\n",
    "        super().__init__()\n",
    "        __doc__ = encoder.__doc__\n",
    "        self.encoder = deepcopy(encoder)\n",
    "        \n",
    "        self.encoder.layer = self.encoder.layer[:l0]\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = self.encoder.forward.__doc__\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "class TruncatedModel(Module):\n",
    "    def __init__(self, model: BertModel, l0: int):\n",
    "        super().__init__()\n",
    "        self.model = deepcopy(model)\n",
    "        self.model.encoder = TruncatedEncoder(self.model.encoder, l0)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = self.model.forward.__doc__\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "class BioClassifier(Module):\n",
    "    \"\"\"\n",
    "    BIO classifier head\n",
    "    \"\"\"\n",
    "    def __init__(self,  bertmodel: TruncatedModel):\n",
    "        super().__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.dropout = Dropout(p=0.1)\n",
    "        self.classifier = Linear(in_features=768, out_features=4, bias=True)\n",
    "        self.num_labels = 4\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = BertForTokenClassification.forward.__doc__\n",
    "        return BertForTokenClassification.forward(self, *args, **kwargs)\n",
    "    \n",
    "class LinkPredictorClassifier(Module):\n",
    "    \"\"\"\n",
    "    Link predictor classifier head. Unused.\n",
    "    \"\"\"\n",
    "    def __init__(self,  bertmodel: TruncatedModel):\n",
    "        super().__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.dropout = Dropout(p=0.1)\n",
    "        self.classifier = Linear(in_features=768, out_features=wikipedia_cbor.max_entity_num, bias=True)\n",
    "        self.num_labels = wikipedia_cbor.max_entity_num\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        __doc__ = BertForTokenClassification.forward.__doc__\n",
    "        return BertForTokenClassification.forward(self, *args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/save previously trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly loaded pre-trained models.\n"
     ]
    }
   ],
   "source": [
    "from tools.dumps import get_filename_path, is_file\n",
    "\n",
    "# FIXME: hardcoded paths\n",
    "common_model_path = get_filename_path(\"eae/common_model.pt\")\n",
    "bioclassifier_path = get_filename_path(\"eae/bioclassifier.pt\")\n",
    "\n",
    "def save_models(**kwargs):\n",
    "    \"\"\"\n",
    "    Save the models\n",
    "    \n",
    "    >>> save_models(common_model=common_model, bioclassifier= bioclassifier)\n",
    "    \n",
    "    All the kwarg parameters must be `Model`s.\n",
    "    \"\"\"\n",
    "    \n",
    "    for k, v in kwargs.items():\n",
    "        path = get_filename_path(f\"eae/{k}.pt\")\n",
    "        torch.save(v.state_dict(), path)\n",
    "    \n",
    "def load_models():\n",
    "    \"\"\"\n",
    "    Load the models\n",
    "    \"\"\"\n",
    "    \n",
    "    model = torch.hub.load('huggingface/pytorch-transformers',\n",
    "                            'model', 'bert-base-uncased')\n",
    "    \n",
    "    common_model = TruncatedModel(model, l0).to(device)\n",
    "    bioclassifier = BioClassifier(common_model).to(device)\n",
    "    \n",
    "    common_model.load_state_dict(torch.load(common_model_path))\n",
    "    common_model.to(device)\n",
    "    bioclassifier.load_state_dict(torch.load(bioclassifier_path))\n",
    "    bioclassifier.to(device)\n",
    "    \n",
    "    return {\"common_model\": common_model, \"bioclassifier\": bioclassifier}\n",
    "\n",
    "l0 = 4\n",
    "if is_file(\"eae/bioclassifier.pt\"):\n",
    "    models = load_models()\n",
    "    common_model = models[\"common_model\"]\n",
    "    bioclassifier = models[\"bioclassifier\"]\n",
    "    print(\"Correctly loaded pre-trained models.\")\n",
    "    del models\n",
    "else:\n",
    "    print(\"Pretrained models not found. You need to run the cells below.\")\n",
    "    common_model = TruncatedModel(model, l0).to(device)\n",
    "    bioclassifier = BioClassifier(common_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL_FINETUNING = True\n",
    "\n",
    "def get_optimizer(model):\n",
    "    \"\"\"\n",
    "    Get an optimizer\n",
    "    \"\"\"\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "    from transformers import AdamW\n",
    "\n",
    "    if FULL_FINETUNING:\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "    else:\n",
    "        optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]\n",
    "\n",
    "    return AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=1e-4,\n",
    "        eps=1e-8\n",
    "    )\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "epochs_bio = 1 # Seems to overfit after 1 epoch\n",
    "\n",
    "def get_schedule(epochs, optimizer, train_dataloader):\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    return get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.05*total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "optimizer_bio = get_optimizer(bioclassifier)\n",
    "scheduler_bio = get_schedule(epochs_bio, optimizer_bio, bio_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Embedding, Dropout, ModuleList, Linear\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "#GELU = torch.nn.GELU\n",
    "LayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "l0 = 4\n",
    "l1 = 8\n",
    "    \n",
    "class EntityMemory(Module):\n",
    "    \"\"\"\n",
    "    Entity Memory, as described in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size: int, entity_size: int,\n",
    "                   entity_embedding_size: int):\n",
    "        \"\"\"\n",
    "        :param embedding_size the size of an embedding. In the EaE paper it is called d_emb, previously as d_k\n",
    "            (attention_heads * embedding_per_head)\n",
    "        :param entity_size also known as N in the EaE paper, the maximum number of entities we store\n",
    "        :param entity_embedding_size also known as d_ent in the EaE paper, the embedding of each entity\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.N = entity_size\n",
    "        self.d_emb = embedding_size\n",
    "        self.d_ent = entity_embedding_size\n",
    "        self.W_f = Linear(2*embedding_size, self.d_ent)\n",
    "        self.W_b = Linear(self.d_ent, embedding_size)\n",
    "        self.E = Linear(self.N, self.d_ent)\n",
    "        self.begin = bio_dataset.bio2idx[\"B\"]\n",
    "        self.inner = bio_dataset.bio2idx[\"I\"]\n",
    "        self.out = bio_dataset.bio2idx[\"O\"]\n",
    "        \n",
    "    def forward(self, X, bio_output, entities_output, k=100):\n",
    "        \"\"\"\n",
    "        :param x the (raw) output of the first transformer block. It has a shape:\n",
    "                B x N x (embed_size)\n",
    "        :param entities_output the detected entities\n",
    "        :param k when evaluating take the best performing K entities when supervising.\n",
    "                When training this is ignored.\n",
    "                \n",
    "        :returns a pair (loss, transformer_output)\n",
    "        \"\"\"\n",
    "        \n",
    "        begin_positions = torch.nonzero(bio_output == self.begin)\n",
    "        y = torch.zeros(X.size(0))\n",
    "        loss = torch.zeros((1,))\n",
    "        \n",
    "        \n",
    "        for pos in begin_positions:\n",
    "            end_mention = pos[1]\n",
    "            while end_mention < self.d_emb and bio_output[pos[0], end_mention] == self.inner:\n",
    "                end_mention += 1\n",
    "            end_mention -= 1\n",
    "\n",
    "\n",
    "            first = X[pos[0], pos[1]]\n",
    "            second = X[pos[0], end_mention]\n",
    "            \n",
    "            mention_span = torch.cat([first, second], 0)\n",
    "            pseudo_entity_embedding = self.W_f(mention_span) # d_ent\n",
    "            \n",
    "            # EntEmbed in the paper\n",
    "            # TODO: use a sparse representation for a OH vector\n",
    "            entity_mappings_oh = torch.zeros((self.N,))\n",
    "            entity_mappings_oh[entities_output[pos[0], pos[1]]] = 1\n",
    "            entity_mappings = self.E(entity_mappings_oh)\n",
    "            \n",
    "            #if self.train:\n",
    "            alpha = F.softmax(self.E.weight.T.matmul(pseudo_entity_embedding))\n",
    "            \n",
    "            #print(alpha.size())\n",
    "            #print(self.E.weight.size())\n",
    "            picked_entity = self.E.weight.matmul(alpha)\n",
    "            \n",
    "            #print(picked_entity.size())\n",
    "            \n",
    "            y[pos[0], pos[1]] = self.W_b(picked_entity)\n",
    "            \n",
    "            loss += alpha[entities_output[pos[0], pos[1]]]\n",
    "            \n",
    "        return loss, y\n",
    "\n",
    "class Pretraining(Module):\n",
    "    \"\"\"\n",
    "    This is a mere wrapper used to pretrain contemporarely a bio classifier and EntityMemory.\n",
    "    \n",
    "    We also need TokenPred but we'll ignore it at the moment.\n",
    "    \"\"\"\n",
    "    def __init__(self, bert_first_block: TruncatedModel, bioclassifier: BioClassifier = None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param bert_first_block the first block of BERT\n",
    "        :param bioclassifier pass a pretrained BioClassifier here if available otherwise train\n",
    "                one from scratch. Here for debugging reasons as we don't have mention detection and\n",
    "                entity linking available on the same dataset (moot point - a link must imply a\n",
    "                mention! - but it would cost me some work and I want to avoid it right now). If\n",
    "                provided, it must rely on bert_first_block\n",
    "        :see EntityMemory.__init__\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = bert_first_block\n",
    "        self.entity_memory = EntityMemory(*args, **kwargs)\n",
    "        if bioclassifier is None:\n",
    "            self.bioclassifier = BioClassifier(self.bert)\n",
    "        else:\n",
    "            self.bioclassifier = bioclassifier\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, entity_outputs):\n",
    "        X = self.bert(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "        \n",
    "        # X = (last_hidden_state, tanh_output). We take the first output here.\n",
    "        \n",
    "        bio_outputs, = self.bioclassifier(input_ids, token_type_ids=None, attention_mask=attention_mask, labels=None)\n",
    "        \n",
    "        choices = torch.argmax(bio_outputs, 2)\n",
    "        \n",
    "        entity_loss, entity_outputs = self.entity_memory(X[0], choices, entity_outputs)\n",
    "        \n",
    "        # TODO: here we need to add the second part of BERT for TokenPred\n",
    "        return entity_loss, entity_outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "vec = torch.tensor([1, 2, 3])\n",
    "\n",
    "mat @ vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/110 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  0%|          | 0/110 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2411f200570b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mpretraining_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_attns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_entities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    746\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b1474840b8d8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, entity_outputs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbio_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mentity_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# TODO: here we need to add the second part of BERT for TokenPred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    746\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-b1474840b8d8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, bio_output, entities_output, k)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m#print(picked_entity.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicked_entity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentities_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "embedding_size = 768 # BERT-base\n",
    "entity_embedding_size = 512 # I am not sure this is a decent value but... \n",
    "pretraining_model = Pretraining(common_model, bioclassifier, embedding_size,\n",
    "                                       wikipedia_cbor.max_entity_num, entity_embedding_size) \\\n",
    "                                      .to(device)\n",
    "\n",
    "pretraining_epochs = 1\n",
    "\n",
    "pretraining_optimizer = get_optimizer(pretraining_model)\n",
    "pretraining_schedule = get_schedule(pretraining_epochs, pretraining_optimizer, wiki_train_dataloader)\n",
    "\n",
    "for epoch in range(1):\n",
    "    pretraining_model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(wiki_train_dataloader):\n",
    "        b_input_ids, b_attns, b_entities = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        pretraining_model.zero_grad()\n",
    "        loss, outputs = pretraining_model(b_input_ids, b_attns, b_entities)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # is this necessary?\n",
    "        clip_grad_norm_(parameters=pretraining_model.parameters(),\n",
    "                            max_norm=max_grad_norm)\n",
    "    \n",
    "        optimizer_bio.step()\n",
    "        scheduler_bio.step()\n",
    "        \n",
    "    avg_train_loss = total_loss / len(wiki_train_dataloader)\n",
    "    tqdm.write(f\"Average train loss at epoch {epoch}: {avg_train_loss}\")\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    pretraining_model.eval()\n",
    "    \n",
    "    avg_eval_loss = []\n",
    "    for batch in tqdm(wiki_validation_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = pretrained_model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        eval_loss += outputs[0].mean().item()\n",
    "    \n",
    "    avg_eval_loss.append(eval_loss)\n",
    "    \n",
    "    tqdm.write(f\"Average eval loss at epoch {epoch}: {eval_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EaeModel(Module):\n",
    "    def __init__(self,\n",
    "                   first_transformer_block: TruncatedModel,\n",
    "                   second_transformer_block: TruncatedModel,\n",
    "                   pretrained_bioclassifier: BioClassifier,\n",
    "                   pretrained_entity_memory: EntityMemory):\n",
    "        \"\"\"\n",
    "        :param first_transformer_block the first block\n",
    "        :param second_transformer_block the second transformer block (obviously)\n",
    "        :param pretrained_bioclassifier a pretrained bioclassifier\n",
    "                that uses first_transformer_block.\n",
    "        :param task_specific_heads a head for the given problem\n",
    "        \"\"\"\n",
    "        self.first_transformer_block = first_transformer_block\n",
    "        \n",
    "        # FIXME: THIS IS WRONG!\n",
    "        # We do need the second part of the encoder, NOT embedder + encoder.\n",
    "        self.second_transformer_block = second_transformer_block\n",
    "        self.bioclassifier = trained_bioclassifier\n",
    "        self.layernorm = LayerNorm(768) # TODO: make this parametric\n",
    "        self.entity_memory = pretrained_entity_memory\n",
    "        \n",
    "        \n",
    "    def forward(input_ids, attention_mask, supervised_entities):\n",
    "        \"\"\"\n",
    "        :param input_ids the input tokens\n",
    "        :param attention_mask the attention mask\n",
    "        :param supervised_entities when training provide the detected entities for each token\n",
    "        \n",
    "        We do not apply gradient descent on the \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            _, bio_output = self.bioclassifier(input_ids,\n",
    "                                                  token_type_ids=None,\n",
    "                                                  attention_mask=attention_mask,\n",
    "                                                  labels=None)\n",
    "            \n",
    "        # TODO: make that bio_output can return the transformer\n",
    "        # output before pooling + classifying along with the outputs.\n",
    "        # We could save us some time.\n",
    "        \n",
    "        X_2 = first_transformer_block(input_ids, token_type_ids=None,\n",
    "                                attention_mask=attention_mask)\n",
    "        \n",
    "        # The EM loss function does not take into account W_b but we need it now.\n",
    "        # Thus, we let autograd use it.\n",
    "        _, X_3 self.entity_memory(X_2, bio_output, supervised_entities)\n",
    "            \n",
    "        self.layernorm(X_2 + X_3)\n",
    "        self.second_transformer_block()\n",
    "            \n",
    "            \n",
    "\n",
    "#def get_sentence_output(is_training=True, ):\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [00:57<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.1491738242066392\n",
      "Validation Accuracy: 0.9716910943073838\n",
      "Validation F1-Score: 0.8803799719624213\n",
      "Validation loss: 0.08271155654061475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for epoch in range(epochs_bio):\n",
    "    bioclassifier.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(bio_train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        bioclassifier.zero_grad()\n",
    "            \n",
    "        \n",
    "        outputs = bioclassifier(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        clip_grad_norm_(parameters=bioclassifier.parameters(),\n",
    "                        max_norm=max_grad_norm)\n",
    "    \n",
    "        optimizer_bio.step()\n",
    "        scheduler_bio.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(bio_train_dataloader)\n",
    "    tqdm.write(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    bioclassifier.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0.0, 0.0\n",
    "    number_eval_steps, number_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in bio_validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bioclassifier(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    \n",
    "    eval_loss = eval_loss / len(bio_validation_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    \n",
    "    # flatten pred_tags and true_tags.\n",
    "    #pred_tags = [[p_i for p_i in prediction] for prediction in predictions]\n",
    "    #true_tags = [[t_i for t_i in truth] for truth in true_labels]\n",
    "    \n",
    "    \n",
    "    # Padding is used only for masked values.\n",
    "    pred_tags = [[bio_dataset.bio_values[p_i] for p_i, l_i in zip(p, l) if l_i != bio_dataset.bio2idx[\"PAD\"]]\n",
    "                    for p, l in zip(predictions, true_labels)]\n",
    "    \n",
    "    true_tags = [[bio_dataset.bio_values[l_i] for l_i in l if l_i != bio_dataset.bio2idx[\"PAD\"]] for l in true_labels]\n",
    "    \n",
    "    \n",
    "    tqdm.write(f\"Validation Accuracy: {accuracy_score(pred_tags, true_tags)}\")\n",
    "    tqdm.write(f\"Validation F1-Score: {f1_score(pred_tags, true_tags)}\")\n",
    "    tqdm.write(f\"Validation loss: {eval_loss}\")\n",
    "    tqdm.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f03862dc5c0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(validation_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models(common_model=common_model, bioclassifier=bioclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    linkpredictorclassifier.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(wiki_train_dataloader):\n",
    "        #batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "        linkpredictorclassifier.zero_grad()\n",
    "        outputs = linkpredictorclassifier(b_input_ids, token_type_ids=None,\n",
    "                                attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        # Someone has to explain to me why someone put the loss function inside a module\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        clip_grad_norm_(parameters=linkpredictorclassifier.parameters(),\n",
    "                        max_norm=max_grad_norm)\n",
    "    \n",
    "        optimizer_lp.step()\n",
    "        scheduler_lp.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(wiki_train_dataloader)\n",
    "    tqdm.write(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    linkpredictorclassifier.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0.0, 0.0\n",
    "    number_eval_steps, number_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    linkpredictorclassifier.eval()\n",
    "    \n",
    "    for batch in tqdm(wiki_validation_dataloader):\n",
    "        batch = tuple([t.to(device) for t in batch])\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = linkpredictorclassifier(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    eval_loss = eval_loss / len(wiki_validation_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    \n",
    "    tqdm.write(f\"Validation loss: {eval_loss}\")\n",
    "    tqdm.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f33327a4940>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(validation_loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', ',', 'this', 'is', 'sponge', '##bo', '##b', '!']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bioclassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5dc1b4cefb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello world, this is Spongebob!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbioclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbioclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bioclassifier' is not defined"
     ]
    }
   ],
   "source": [
    "def transform_sentence(sentence: str):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    print(tokens)\n",
    "    padded = pad_sequences([tokenizer.convert_tokens_to_ids(tokens)], maxlen=MAX_LEN,\n",
    "                  dtype=\"long\", value=0.0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    attention_mask = [[float(tok != 0.0) for tok in padded_] for padded_ in padded]\n",
    "    \n",
    "    return padded, attention_mask\n",
    "\n",
    "# bioclassifier.forward(tokens,)\n",
    "\n",
    "padded, attention = transform_sentence(\"Hello world, this is Spongebob!\")\n",
    "\n",
    "bioclassifier.eval()\n",
    "res = bioclassifier.forward(torch.tensor(padded).to(device), token_type_ids=None, attention_mask=torch.tensor(attention).to(device), labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinkPredictorClassifier(\n",
       "  (bert): TruncatedModel(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): TruncatedEncoder(\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=344942, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkpredictorclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
