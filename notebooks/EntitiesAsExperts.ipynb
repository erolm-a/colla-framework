{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities as Experts\n",
    "\n",
    "This notebook is a code implementation of the paper \"Entities as Experts: Sparse Memory Access with Entity Supervision\" by FÃ©vry, Baldini Soares, FitzGerald, Choi, Kwiatowski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition and high-level model description\n",
    "\n",
    "We want to perform question answering on typical one-shot questions that require external knowledge or context. For example, in order to answer the question \"Which country was Charles Darwin born in?\" one needs some text providing answers on typical structured scenarios.\n",
    "\n",
    "In this case, however, we want to rely on knowledge-graph extracted information. For example, in the question given here, we can prune out unrelated to the antropologist and evolution theorist Charles Darwins, e.g. Charles River, Darwin City etc. \n",
    "\n",
    "In the paper, the authors propose to augment BERT in the task of cloze-type question answering by leveraging an Entity Memory extracted from e.g. a Knoweldge Graph.\n",
    "\n",
    "![Entity as Experts description](images/eae_highlevel.png)\n",
    "\n",
    "The Entity Memory is a simple bunch of embeddings of entities extracted from a Knowledge Graph. Relationships are ignored (see the Facts as Experts paper and notebook to see how they could be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "> We assume access to a corpus $D={(xi,mi)}$,where all entity mentions are detected but not necessarily  all  linked  to  entities.   We  use  English Wikipedia as our corpus, with a vocabulary of 1m entities. Entity links come from hyperlinks, leading to 32m 128 byte contexts containing 17m entity links.\n",
    "\n",
    "In the appendix B, it is explained that:\n",
    "\n",
    "> We build our training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 500 bytes,resulting in a corpus of 32 million contexts withover 17 million entity mentions. We restrict our-selves  to  the  one  million  most  frequent  entities\n",
    "(86% of the linked mentions).\n",
    "\n",
    "Given that the dump 2019-04-14 is not available at the time of writing, we will adopt the revision 2020-11-01.\n",
    "\n",
    "Entities are thus partially extracted by link annotations (e.g. they associate with each token a mention if that token belongs to a wikipedia url).\n",
    "\n",
    "## Mention Detection\n",
    "\n",
    "> In addition to the Wikipedia links, we annotaten each sentence with unlinked mention spans using the mention detector from Section 2.2\n",
    "\n",
    "The mention detection head discussed in Section 2.2 is a simple BIO sequence: each token is annotated with a B (beginning), I (inside) or O (outside) if they are respectivelly beginning, inside or outside of a mention. The reason why we use both BIO and EL is to avoid inconsistencies.\n",
    "\n",
    "There is a catch. In the paper, they explain they used Google NLP APIs to perform entity detection and linking on large-scale Wikipedia entries, that is, to have a properly annotated Wikipedia dataset. We are going to use simple Wikipedia hyperlinks insteal (TODO: consider adding spacy annotation?).\n",
    "\n",
    "NOTE FOR MYSELF: We don't *actually* perform entity linking here - as in, we don't train a classifier from the first $l_0$ layers. Instead we'll build pseudo embeddings and try to find the entity that best matches a pseudo embedding (see later).\n",
    "\n",
    "HOWEVER, we do need that when training. The whole idea is that we get supervised data when training, however it is not always the case that we have this data at hand (see: TriviaQA, WebQuestions, ...).\n",
    "\n",
    "## Entity Memory\n",
    "\n",
    "The idea is pretty simple: we have as input $X_1$ and the mention spans $m_i = (e_i, s_{m_i}, t_{m_i})$ . Those are given as input. We don't care about e_i for the embedding calculation, but we DO care for the loss definition.\n",
    "\n",
    "(Glossing over the Entity Memory calculation...)\n",
    "\n",
    "When the entity detection is supervised, our obtained entity should be close to the found pseudo entity embedding.\n",
    "\n",
    "$$\n",
    "ELLoss = \\sum_{m_i} \\alpha_i \\cdot \\mathbb{1}_{e_{m_i} \\ne e_{\\emptyset}} , \\qquad \\alpha = softmax(E \\cdot h_{m_i})\n",
    "$$\n",
    "\n",
    "($E$ is our `EntEmbed`, so $E * h_{m_i}$ is a vector, so $\\alpha$ is a vector too (of shape $N$), and $h_{m_i}$ is a \"pseudo entity embedding\" ).\n",
    "\n",
    "## Chunking\n",
    "- In theory we should split articles by chunks of 500 bytes (assuming unicode encoding), and contexts are only 128 tokens long. For simplicity by now we only limit ourselves to the first paragraph only.\n",
    "\n",
    "## Tokenization:\n",
    "\n",
    "- BERT Tokenizer (e.g. Wordpiece) using lowercase vocabulary, limited to 128 distinct word-piece tokens.\n",
    "\n",
    "## Learning hyperparameters\n",
    "\n",
    "For pretraining:\n",
    "\n",
    "> We use ADAM with a learning rate of 1e-4.  We apply warmup for the first 5% of training, decaying the learning rate afterwards.  We also apply gradient clipping with a norm of 1.0\n",
    "\n",
    "Since the decaying rate is not provided, we test with 3e-5 which seems quite standard.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "To evaluate:\n",
    "\n",
    "- [X] SQuAD\n",
    "- [ ] TriviaQA\n",
    "- [ ] MetaQA\n",
    "- [ ] WebQuestions\n",
    "- [ ] Colla?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merolm_a\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.17 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">super-cloud-63</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/erolm_a/EntitiesAsExperts\" target=\"_blank\">https://wandb.ai/erolm_a/EntitiesAsExperts</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/erolm_a/EntitiesAsExperts/runs/2io8jxzg\" target=\"_blank\">https://wandb.ai/erolm_a/EntitiesAsExperts/runs/2io8jxzg</a><br/>\n",
       "                Run data is saved locally in <code>/nfs/colla-framework/notebooks/wandb/run-20210203_001551-2io8jxzg</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"EntitiesAsExperts\")\n",
    "wandb.config.device = \"cuda\"\n",
    "\n",
    "from tools.dataloaders import WikipediaCBOR\n",
    "from models import EntitiesAsExperts, EaEForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from cache\n"
     ]
    }
   ],
   "source": [
    "wikipedia_cbor = WikipediaCBOR(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"wikipedia/car-wiki2020-01-01/partitions\",\n",
    "                                       # top 2% most frequent items,  roughly at least 100 occurrences, with a total of  ~ 20000 entities\n",
    "                                       #cutoff_frequency=0.02, recount=True \n",
    "                                    # TODO: is this representative enough?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "# https://discuss.pytorch.org/t/random-subset-from-dataloader-unique-random-numbers/43430/6\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "wiki_dev_size = int(0.1*len(wikipedia_cbor))\n",
    "    \n",
    "wiki_dev_indices = np.random.choice(len(wikipedia_cbor), size=wiki_dev_size)\n",
    "\n",
    "# 80/20% split\n",
    "wiki_train_size = int(0.8*wiki_dev_size)\n",
    "wiki_validation_size = wiki_dev_size - wiki_train_size\n",
    "\n",
    "wiki_train_indices, wiki_validation_indices = wiki_dev_indices[:wiki_train_size], wiki_dev_indices[wiki_train_size:]\n",
    "wiki_train_sampler = SubsetRandomSampler(wiki_train_indices,\n",
    "                                         generator=torch.Generator().manual_seed(42))\n",
    "wiki_validation_sampler = SubsetRandomSampler(wiki_validation_indices,\n",
    "                                              generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "wiki_train_dataloader = DataLoader(wikipedia_cbor, sampler=wiki_train_sampler,\n",
    "                                   batch_size=wandb.config.eae_batch_size,\n",
    "                                   num_workers=NUM_WORKERS)\n",
    "wiki_validation_dataloader = DataLoader(wikipedia_cbor,\n",
    "                                        sampler=wiki_validation_sampler,\n",
    "                                        batch_size=wandb.config.eae_batch_size,\n",
    "                                        num_workers=NUM_WORKERS)\n",
    "# wikipedia_cbor_train, wikipedia_cbor_validation = random_split(wikipedia_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertForTokenClassification\n",
    "from models import EntitiesAsExperts, EaEForQuestionAnswering\n",
    "from models.training import load_model, save_models,train_model, get_optimizer, get_schedule\n",
    "model_masked_lm = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7fad625440f0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.device import get_available_device\n",
    "\n",
    "pretraining_model = EntitiesAsExperts(model_masked_lm,\n",
    "                                      wandb.config.eae_l0,\n",
    "                                      wandb.config.eae_l1, \n",
    "                                      wikipedia_cbor.max_entity_num,\n",
    "                                     wandb.config.eae_entity_embedding_size).to(get_available_device())\n",
    "\n",
    "wandb.watch(pretraining_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2888 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:752: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:777: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when outputs are generated by different autograd Nodes \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:787: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "100%|ââââââââââ| 2888/2888 [4:09:58<00:00,  5.19s/it]  \n",
      "  0%|          | 0/722 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"NoneType\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2b1a3e20cfdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m train_model(pretraining_model, wiki_train_dataloader,\n\u001b[1;32m     12\u001b[0m                 \u001b[0mwiki_validation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_load_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretraining_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 pretraining_schedule, wandb.config.eae_pretraining_epochs, None, 8)\n\u001b[0m",
      "\u001b[0;32m/nfs/colla-framework/notebooks/models/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, validation_dataloader, load_from_dataloader, optimizer, scheduler, epochs, metric, gradient_accumulation_factor, automatic_mixed_precision)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmetric_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"NoneType\") to list"
     ]
    }
   ],
   "source": [
    "pretraining_optimizer = get_optimizer(pretraining_model)\n",
    "pretraining_schedule = get_schedule(wandb.config.eae_pretraining_epochs,\n",
    "                                    pretraining_optimizer, wiki_train_dataloader)\n",
    "\n",
    "# TODO: automatically send the model to the device AND provide on/off switches for it\n",
    "\n",
    "def wiki_load_batch(batch):\n",
    "    # Nothing interesting...\n",
    "    return batch, tuple()\n",
    "\n",
    "train_model(pretraining_model, wiki_train_dataloader,\n",
    "                wiki_validation_dataloader, wiki_load_batch, pretraining_optimizer,\n",
    "                pretraining_schedule, wandb.config.eae_pretraining_epochs, None, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) save_models. (TODO: Migrate to W&B)\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "from models.training import save_models\n",
    "\n",
    "save_models(pretraining_eae_one_epoch=pretraining_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
