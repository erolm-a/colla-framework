{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities as Experts\n",
    "\n",
    "This notebook is a code implementation of the paper \"Entities as Experts: Sparse Memory Access with Entity Supervision\" by FÃ©vry, Baldini Soares, FitzGerald, Choi, Kwiatowski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition and high-level model description\n",
    "\n",
    "We want to perform question answering on typical one-shot questions that require external knowledge or context. For example, in order to answer the question \"Which country was Charles Darwin born in?\" one needs some text providing answers on typical structured scenarios.\n",
    "\n",
    "In this case, however, we want to rely on knowledge-graph extracted information. For example, in the question given here, we can prune out unrelated to the antropologist and evolution theorist Charles Darwins, e.g. Charles River, Darwin City etc. \n",
    "\n",
    "In the paper, the authors propose to augment BERT in the task of cloze-type question answering by leveraging an Entity Memory extracted from e.g. a Knoweldge Graph.\n",
    "\n",
    "![Entity as Experts description](images/eae_highlevel.png)\n",
    "\n",
    "The Entity Memory is a simple bunch of embeddings of entities extracted from a Knowledge Graph. Relationships are ignored (see the Facts as Experts paper and notebook to see how they could be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "> We assume access to a corpus $D={(xi,mi)}$,where all entity mentions are detected but not necessarily  all  linked  to  entities.   We  use  English Wikipedia as our corpus, with a vocabulary of 1m entities. Entity links come from hyperlinks, leading to 32m 128 byte contexts containing 17m entity links.\n",
    "\n",
    "In the appendix B, it is explained that:\n",
    "\n",
    "> We build our training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 500 bytes,resulting in a corpus of 32 million contexts withover 17 million entity mentions. We restrict our-selves  to  the  one  million  most  frequent  entities\n",
    "(86% of the linked mentions).\n",
    "\n",
    "Given that the dump 2019-04-14 is not available at the time of writing, we will adopt the revision 2020-11-01.\n",
    "\n",
    "Entities are thus partially extracted by link annotations (e.g. they associate with each token a mention if that token belongs to a wikipedia url).\n",
    "\n",
    "## Mention Detection\n",
    "\n",
    "> In addition to the Wikipedia links, we annotaten each sentence with unlinked mention spans using the mention detector from Section 2.2\n",
    "\n",
    "The mention detection head discussed in Section 2.2 is a simple BIO sequence: each token is annotated with a B (beginning), I (inside) or O (outside) if they are respectivelly beginning, inside or outside of a mention. The reason why we use both BIO and EL is to avoid inconsistencies.\n",
    "\n",
    "There is a catch. In the paper, they explain they used Google NLP APIs to perform entity detection and linking on large-scale Wikipedia entries, that is, to have a properly annotated Wikipedia dataset.\n",
    "\n",
    "Since we cannot technically afford this, we will use spacy's entity detection and linking capabilities as a baseline. Data quality \n",
    "\n",
    "## Chunking\n",
    "\n",
    "- Split articles by chunks of 500 bytes (assuming unicode encoding).\n",
    "- We will elide sentences till the last period to make sure they reach such limit without giving weird effects.\n",
    "\n",
    "## Tokenization:\n",
    "\n",
    "- BERT Tokenizer (e.g. Wordpiece) using lowercase vocabulary, limited to 128 distinct word-piece tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.providers import WikipediaProvider\n",
    "\n",
    "#WikipediaProvider.dump_full_dataset(revision=\"20201101\")\n",
    "from trec_car import read_data\n",
    "from tools.dumps import wrap_open\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "from trec_car.read_data import Page, Section, List, Para, ParaLink, ParaText, ParaBody\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", pipeline=[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_section(skel, toks, links):\n",
    "    for subskel in skel.children:\n",
    "        visit_section(subskel, toks, links)\n",
    "\n",
    "def handle_list(skel, toks, links):\n",
    "    visit_section(skel.body, toks, links)\n",
    "\n",
    "def handle_para(skel: Para, toks, links):\n",
    "    paragraph = skel.paragraph\n",
    "    bodies = paragraph.bodies\n",
    "\n",
    "    for body in bodies:\n",
    "        visit_section(body, toks, links)\n",
    "\n",
    "def handle_paratext(body: ParaBody, toks, links):\n",
    "    lemmas = [x.text for x in nlp(body.get_text())]\n",
    "    toks.extend(lemmas)\n",
    "    links.extend([\"PAD\"] * len(lemmas))\n",
    "    pass\n",
    "\n",
    "def handle_paralink(body: ParaLink, toks, links):\n",
    "    lemmas = [x.text for x in nlp(body.anchor_text)]\n",
    "    toks.extend(lemmas)\n",
    "    links.extend([body.page] + [\"PAD\"] * (len(lemmas) - 1))\n",
    "    pass\n",
    "\n",
    "def nothing():\n",
    "    return lambda body, toks, links: None\n",
    "\n",
    "handler = defaultdict(nothing, {Section: handle_section,\n",
    "                     Para: handle_para,\n",
    "                     List: handle_list,\n",
    "                     ParaLink: handle_paralink,\n",
    "                     ParaText: handle_paratext})\n",
    "\n",
    "\n",
    "def visit_section(skel, toks, links):\n",
    "    # Recur on the sections\n",
    "    handler[type(skel)](skel, toks, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wrap_open(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"rb\") as toc:\n",
    "    for idx, page in enumerate(read_data.iter_annotations(toc)):\n",
    "        links = []\n",
    "        toks = []\n",
    "        for skel in page.skeleton:\n",
    "            visit_section(skel, toks, links)\n",
    "        #print(links)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, \n",
    "import tqdm\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tools.dumps import get_filename_path\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import random\n",
    "\n",
    "from trec_car.read_data import AnnotationsFile, ParagraphsFile\n",
    "\n",
    "class WikipediaCBOR(Dataset):\n",
    "    \"\"\"\n",
    "    This is a simple CBOR loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, cbor_path):\n",
    "        \"\"\"\n",
    "        :param cbor_path the path of the wikipedia cbor export\n",
    "        \"\"\"\n",
    "        # Let trec deal with that in my place\n",
    "        \n",
    "        self.cbor_path = get_filename_path(cbor_path)\n",
    "        self.cbor_toc_annotations = AnnotationsFile(self.cbor_path)\n",
    "        self.cbor_toc_paragraphs = ParagraphsFile(self.cbor_path)\n",
    "        \n",
    "        self.keys = selfself.cbor_toc_paragraphs.keys()\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        # preprocess and find the top k unique wikipedia links\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __get_whole_text(paragraph):\n",
    "        # Tokenize by word. When a link is detected, \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx)\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        paragraphs = [self.cbor_toc_paragraphs.get(self.keys[i]) for i in idx]\n",
    "                \n",
    "        # TODO: can we parallelize this?\n",
    "        \n",
    "        \n",
    "    \n",
    "    return torch.tensor(paragraphs)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page(Anarchism)\n",
      "Page(Albedo)\n",
      "Page(Autism)\n",
      "Page(A)\n",
      "Page(Achilles)\n",
      "Page(Alabama)\n",
      "Page(Abraham Lincoln)\n",
      "Page(An American in Paris)\n",
      "Page(Aristotle)\n",
      "Page(Academy Award for Best Production Design)\n"
     ]
    }
   ],
   "source": [
    "with wrap_open(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"rb\") as toc:\n",
    "    #toc.seek(46102636493)\n",
    "    \n",
    "    for idx, page in enumerate(read_data.iter_pages(toc)):\n",
    "        if idx >= 10:\n",
    "            break\n",
    "        print(page)\n",
    "        page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In the paper, the authors explain they used a modified BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ner.csv.zip\n",
      "replace ner.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from tools.dumps import wrap_open\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "!cd data && unzip ner.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos', 'next-next-shape', 'next-next-word', 'next-pos', 'next-shape', 'next-word', 'pos', 'prev-iob', 'prev-lemma', 'prev-pos', 'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape', 'prev-prev-word', 'prev-shape', 'prev-word', 'sentence_idx', 'shape', 'word', 'tag', 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (21,25,26,27,28,29,30,31,32,33) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>next-lemma</th>\n",
       "      <th>next-next-lemma</th>\n",
       "      <th>next-next-pos</th>\n",
       "      <th>next-next-shape</th>\n",
       "      <th>next-next-word</th>\n",
       "      <th>next-pos</th>\n",
       "      <th>next-shape</th>\n",
       "      <th>next-word</th>\n",
       "      <th>pos</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1050787</th>\n",
       "      <td>1048565.0</td>\n",
       "      <td>impact</td>\n",
       "      <td>.</td>\n",
       "      <td>__end1__</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>.</td>\n",
       "      <td>punct</td>\n",
       "      <td>.</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050788</th>\n",
       "      <td>1048566.0</td>\n",
       "      <td>.</td>\n",
       "      <td>__end1__</td>\n",
       "      <td>__end2__</td>\n",
       "      <td>__END2__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__END2__</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>wildcard</td>\n",
       "      <td>__END1__</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050789</th>\n",
       "      <td>1048567.0</td>\n",
       "      <td>indian</td>\n",
       "      <td>forc</td>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>said</td>\n",
       "      <td>NNS</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>forces</td>\n",
       "      <td>...</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050790</th>\n",
       "      <td>1048568.0</td>\n",
       "      <td>forc</td>\n",
       "      <td>said</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>they</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>said</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050791</th>\n",
       "      <td>1048569.0</td>\n",
       "      <td>said</td>\n",
       "      <td>they</td>\n",
       "      <td>respond</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>responded</td>\n",
       "      <td>PRP</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>they</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050792</th>\n",
       "      <td>1048570.0</td>\n",
       "      <td>they</td>\n",
       "      <td>respond</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>VBD</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>responded</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050793</th>\n",
       "      <td>1048571.0</td>\n",
       "      <td>respond</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>TO</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>to</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050794</th>\n",
       "      <td>1048572.0</td>\n",
       "      <td>to</td>\n",
       "      <td>the</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>attack</td>\n",
       "      <td>DT</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050795</th>\n",
       "      <td>1048573.0</td>\n",
       "      <td>the</td>\n",
       "      <td>attack</td>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>with</td>\n",
       "      <td>NN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>attack</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050796</th>\n",
       "      <td>1048574.0</td>\n",
       "      <td>attack</td>\n",
       "      <td>with</td>\n",
       "      <td>machine-gun</td>\n",
       "      <td>JJ</td>\n",
       "      <td>contains-hyphen</td>\n",
       "      <td>machine-gun</td>\n",
       "      <td>IN</td>\n",
       "      <td>lowercase</td>\n",
       "      <td>with</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>prev-prev-pos</td>\n",
       "      <td>prev-prev-shape</td>\n",
       "      <td>prev-prev-word</td>\n",
       "      <td>prev-shape</td>\n",
       "      <td>prev-word</td>\n",
       "      <td>sentence_idx</td>\n",
       "      <td>shape</td>\n",
       "      <td>word</td>\n",
       "      <td>tag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             lemma next-lemma next-next-lemma next-next-pos next-next-shape  \\\n",
       "1050787  1048565.0     impact               .      __end1__        __END1__   \n",
       "1050788  1048566.0          .        __end1__      __end2__        __END2__   \n",
       "1050789  1048567.0     indian            forc          said             VBD   \n",
       "1050790  1048568.0       forc            said          they             PRP   \n",
       "1050791  1048569.0       said            they       respond             VBD   \n",
       "1050792  1048570.0       they         respond            to              TO   \n",
       "1050793  1048571.0    respond              to           the              DT   \n",
       "1050794  1048572.0         to             the        attack              NN   \n",
       "1050795  1048573.0        the          attack          with              IN   \n",
       "1050796  1048574.0     attack            with   machine-gun              JJ   \n",
       "\n",
       "          next-next-word     next-pos next-shape  next-word        pos  ...  \\\n",
       "1050787         wildcard     __END1__          .      punct          .  ...   \n",
       "1050788         wildcard     __END2__   __END1__   wildcard   __END1__  ...   \n",
       "1050789        lowercase         said        NNS  lowercase     forces  ...   \n",
       "1050790        lowercase         they        VBD  lowercase       said  ...   \n",
       "1050791        lowercase    responded        PRP  lowercase       they  ...   \n",
       "1050792        lowercase           to        VBD  lowercase  responded  ...   \n",
       "1050793        lowercase          the         TO  lowercase         to  ...   \n",
       "1050794        lowercase       attack         DT  lowercase        the  ...   \n",
       "1050795        lowercase         with         NN  lowercase     attack  ...   \n",
       "1050796  contains-hyphen  machine-gun         IN  lowercase       with  ...   \n",
       "\n",
       "             0              1                2               3           4  \\\n",
       "1050787      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050788      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050789  B-gpe  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050790      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050791      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050792      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050793      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050794      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050795      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "1050796      O  prev-prev-pos  prev-prev-shape  prev-prev-word  prev-shape   \n",
       "\n",
       "                 5             6      7     8    9  \n",
       "1050787  prev-word  sentence_idx  shape  word  tag  \n",
       "1050788  prev-word  sentence_idx  shape  word  tag  \n",
       "1050789  prev-word  sentence_idx  shape  word  tag  \n",
       "1050790  prev-word  sentence_idx  shape  word  tag  \n",
       "1050791  prev-word  sentence_idx  shape  word  tag  \n",
       "1050792  prev-word  sentence_idx  shape  word  tag  \n",
       "1050793  prev-word  sentence_idx  shape  word  tag  \n",
       "1050794  prev-word  sentence_idx  shape  word  tag  \n",
       "1050795  prev-word  sentence_idx  shape  word  tag  \n",
       "1050796  prev-word  sentence_idx  shape  word  tag  \n",
       "\n",
       "[10 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The columns are a bit irregular.\n",
    "names = []\n",
    "with wrap_open(\"ner.csv\", \"r\", encoding=\"latin1\") as f:\n",
    "    names = f.readline().strip().split(\",\")[1:]\n",
    "    names = names + list(range(34 - len(names)))\n",
    "\n",
    "print(names)\n",
    "\n",
    "with wrap_open(\"ner.csv\", \"rb\") as f:\n",
    "    data = pd.read_csv(f, encoding=\"latin1\", names=names).fillna(method=\"ffill\")\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([          'lemma',      'next-lemma', 'next-next-lemma',\n",
       "         'next-next-pos', 'next-next-shape',  'next-next-word',\n",
       "              'next-pos',      'next-shape',       'next-word',\n",
       "                   'pos',        'prev-iob',      'prev-lemma',\n",
       "              'prev-pos',   'prev-prev-iob', 'prev-prev-lemma',\n",
       "         'prev-prev-pos', 'prev-prev-shape',  'prev-prev-word',\n",
       "            'prev-shape',       'prev-word',    'sentence_idx',\n",
       "                 'shape',            'word',             'tag',\n",
       "                       0,                 1,                 2,\n",
       "                       3,                 4,                 5,\n",
       "                       6,                 7,                 8,\n",
       "                       9],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(s):\n",
    "    return [(w, p, t) for w, p, t in zip(s[\"word\"].values.tolist(),\n",
    "                                               s[\"pos\"].values.tolist(),\n",
    "                                               s[\"tag\"].values.tolist())]\n",
    "\n",
    "sentences = [s for s in data.groupby(\"sentence_idx\").apply(aggregate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('number', 'million', '89.7'),\n",
       " ('number', 'million', '94.8'),\n",
       " ('number', 'million', '118'),\n",
       " ('number', 'million', '130.6'),\n",
       " ('number', 'million', '18'),\n",
       " ('number', 'million', '34'),\n",
       " ('number', 'million', '128'),\n",
       " ('number', 'million', '133'),\n",
       " ('number', 'million', '130'),\n",
       " ('number', 'million', '135'),\n",
       " ('number', 'million', '722')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = [[w[0] for w in s] for s in sentences]\n",
    "tags = [[w[1] for w in s] for s in sentences]\n",
    "labels = [[w[2] for w in s] for s in sentences]\n",
    "\n",
    "tag_values = list(set(data[\"tag\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 75 ## can replace with 512 as per the original paper\n",
    "bs = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8685c3a1a4b439489fee8c15549e0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descriptiâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "def tokenize_preserve_labels(sentence, text_labels):\n",
    "    \"\"\"\n",
    "    Tokenize the given sentence. Extend the corresponding label\n",
    "    for all the tokens the word is made of.\n",
    "    \n",
    "    Assumption: len(sentence) == len(text_labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    \n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        \n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "    \n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_labels = [\n",
    "    tokenize_preserve_labels(sent, labs) for sent, labs in zip(utterances, labels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_labels]\n",
    "tokenized_labels = [token_label_pair[1] for token_label_pair in tokenized_texts_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                                maxlen=MAX_LEN, dtype=\"long\", value=0.0, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                         maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                         dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a mask to the padding only.\n",
    "# Normally, BERT masks are used for cloze-style questions, but this is a translation problem\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train and validation sets\n",
    "\n",
    "train_inputs, validation_inputs, train_tags, validation_tags = train_test_split(input_ids, tags, random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_tags = torch.tensor(train_tags)\n",
    "validation_tags = torch.tensor(validation_tags)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_tags)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c0e1d0eda14eed85ada48ab8c5373a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6437bda1efd4d6da0751d15d0641303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descriâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(10.3149, grad_fn=<NllLossBackward>), tensor([[[-1.8813e-01, -1.4686e-01,  1.0079e-01,  ...,  1.3702e-01,\n",
      "           6.3725e-01, -1.9620e-01],\n",
      "         [ 9.6525e-02,  6.8279e-04, -1.5830e-02,  ...,  2.0342e-01,\n",
      "           2.9020e-01, -7.9644e-03],\n",
      "         [ 3.0085e-02,  1.1656e-02,  4.1793e-02,  ...,  5.8692e-02,\n",
      "          -1.8981e-02,  4.2294e-01],\n",
      "         ...,\n",
      "         [ 1.1023e-01, -1.2311e-01,  7.3450e-02,  ...,  1.6149e-01,\n",
      "           1.2158e-01,  3.0233e-01],\n",
      "         [ 1.3686e-01,  1.6160e-01,  3.9345e-02,  ..., -8.0749e-02,\n",
      "          -9.8029e-02,  3.2206e-01],\n",
      "         [ 9.9725e-01,  6.3929e-02,  6.3957e-01,  ..., -5.8454e-01,\n",
      "          -2.2921e-02,  4.7836e-01]]], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.encode(\"This is madness. Madness? THIS IS SPARTA!\")\n",
    "#output = model.forward(torch.tensor(tokenized).unsqueeze(0), labels=torch.tensor([[0] * 16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'BertForTokenClassification' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-895c27fcae32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'BertForTokenClassification' object has no attribute 'loss'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    \n",
    "    \n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-fc9d30398348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         outputs = model(b_input_ids, token_type_ids=None,\n\u001b[0;32m---> 16\u001b[0;31m                            attention_mask=b_input_mask, labels=b_labels)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m                 active_labels = torch.where(\n\u001b[0;32m-> 1357\u001b[0;31m                     \u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m                 )\n\u001b[1;32m   1359\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask, labels=b_labels)\n",
    "    \n",
    "        loss = outputs[0]\n",
    "\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(),\n",
    "                                              max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters, and choose next learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    # Now evaluate on the validation set\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0.0, 0.0\n",
    "    number_eval_steps, number_eval_examples = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                               attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        # Move logits and labels to cpu\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "    \n",
    "    \n",
    "    eval_loss = eval_loss / len(validation_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                          for p_i, l_i in zip(p, l)]\n",
    "    true_tags = [tag_values[l_i] for l in true_labels\n",
    "                                      for l_i in l]\n",
    "    \n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, true_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, true_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'PAD', 'PAD', 'PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'PAD', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "pred_tags = [[tag_values[p_i] for p_i, l_i in zip(p, l)]\n",
    "                 for p, l in zip(predictions, true_labels)]\n",
    "    \n",
    "true_tags = [[tag_values[l_i] for l_i in l]\n",
    "                 for l in true_labels]\n",
    "\n",
    "print(pred_tags[0])\n",
    "print(true_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Embedding, Dropout, ModuleList, Linear\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "GELU = torch.nn.GELU\n",
    "LayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "l0 = 4\n",
    "l1 = 8\n",
    "\n",
    "    \n",
    "class EntityMemory(Module):\n",
    "    \"\"\"\n",
    "    Entity Memory, as described in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size: int, entity_size: int,\n",
    "                   entity_embedding_size: int):\n",
    "        \"\"\"\n",
    "        :param embedding_size the size of an embedding. In the EaE paper it is called d_emb, previously as d_k\n",
    "            (attention_heads * embedding_per_head)\n",
    "        :param entity_size also known as N in the EaE paper, the maximum number of entities we store\n",
    "        :param entity_embedding_size also known as d_ent in the EaE paper, the embedding of each entity\n",
    "        \n",
    "        \"\"\"\n",
    "        self.N = entity_size\n",
    "        self.d_ent = entity_embedding_size\n",
    "        self.w_f = Linear(d_ent, 2*embedding_size)\n",
    "        \n",
    "    def forward(self, x, entity_spans, num_entities, k=None):\n",
    "        \"\"\"\n",
    "        :param x the (raw) output of the first transformer block. It has a shape:\n",
    "                B x N x (embed_size)\n",
    "        :param entity_spans entities and spans of such entities.\n",
    "                Shape: B x C x 3. Each \"row\" contains a triple (e_k, s_mi, t_mi)\n",
    "                where e_k is an (encoded) entity id, s_mi and t_mi are indices.\n",
    "        :param num_entities the number of found entities for each batch.\n",
    "        :param k the number of nearest entities to consider when softmax-ing.\n",
    "                if k = None, all the entities are used.\n",
    "                In the paper, one should set k for when running the inference\n",
    "        \"\"\"\n",
    "        \n",
    "        mentions = [entity_spans[:, :mentions_per_batch] for mentions_per_batch in num_entities]\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
