{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entities as Experts\n",
    "\n",
    "This notebook is a code implementation of the paper \"Entities as Experts: Sparse Memory Access with Entity Supervision\" by FÃ©vry, Baldini Soares, FitzGerald, Choi, Kwiatowski."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition and high-level model description\n",
    "\n",
    "We want to perform question answering on typical one-shot questions that require external knowledge or context. For example, in order to answer the question \"Which country was Charles Darwin born in?\" one needs some text providing answers on typical structured scenarios.\n",
    "\n",
    "In this case, however, we want to rely on knowledge-graph extracted information. For example, in the question given here, we can prune out unrelated to the antropologist and evolution theorist Charles Darwins, e.g. Charles River, Darwin City etc. \n",
    "\n",
    "In the paper, the authors propose to augment BERT in the task of cloze-type question answering by leveraging an Entity Memory extracted from e.g. a Knoweldge Graph.\n",
    "\n",
    "![Entity as Experts description](images/eae_highlevel.png)\n",
    "\n",
    "The Entity Memory is a simple bunch of embeddings of entities extracted from a Knowledge Graph. Relationships are ignored (see the Facts as Experts paper and notebook to see how they could be used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "> We assume access to a corpus $D={(xi,mi)}$,where all entity mentions are detected but not necessarily  all  linked  to  entities.   We  use  English Wikipedia as our corpus, with a vocabulary of 1m entities. Entity links come from hyperlinks, leading to 32m 128 byte contexts containing 17m entity links.\n",
    "\n",
    "In the appendix B, it is explained that:\n",
    "\n",
    "> We build our training corpus of contexts paired with entity mention labels from the 2019-04-14 dump of English Wikipedia. We first divide each article into chunks of 500 bytes,resulting in a corpus of 32 million contexts withover 17 million entity mentions. We restrict our-selves  to  the  one  million  most  frequent  entities\n",
    "(86% of the linked mentions).\n",
    "\n",
    "Given that the dump 2019-04-14 is not available at the time of writing, we will adopt the revision 2020-11-01.\n",
    "\n",
    "Entities are thus partially extracted by link annotations (e.g. they associate with each token a mention if that token belongs to a wikipedia url).\n",
    "\n",
    "## Mention Detection\n",
    "\n",
    "> In addition to the Wikipedia links, we annotaten each sentence with unlinked mention spans using the mention detector from Section 2.2\n",
    "\n",
    "The mention detection head discussed in Section 2.2 is a simple BIO sequence: each token is annotated with a B (beginning), I (inside) or O (outside) if they are respectivelly beginning, inside or outside of a mention. The reason why we use both BIO and EL is to avoid inconsistencies.\n",
    "\n",
    "There is a catch. In the paper, they explain they used Google NLP APIs to perform entity detection and linking on large-scale Wikipedia entries, that is, to have a properly annotated Wikipedia dataset.\n",
    "\n",
    "Since we cannot technically afford this, we will use spacy's entity detection and linking capabilities as a baseline. Data quality \n",
    "\n",
    "## Chunking\n",
    "\n",
    "- Split articles by chunks of 500 bytes (assuming unicode encoding).\n",
    "- We will elide sentences till the last period to make sure they reach such limit without giving weird effects.\n",
    "\n",
    "## Tokenization:\n",
    "\n",
    "- BERT Tokenizer (e.g. Wordpiece) using lowercase vocabulary, limited to 128 distinct word-piece tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.providers import WikipediaProvider\n",
    "\n",
    "#WikipediaProvider.dump_full_dataset(revision=\"20201101\")\n",
    "from trec_car import read_data\n",
    "from tools.dumps import wrap_open\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "from trec_car.read_data import Page, Section, List, Para, ParaLink, ParaText\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", pipeline=[\"tokenizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_section(skel, toks, links):\n",
    "    for subskel in skel.children:\n",
    "        visit_section(subskel, toks, links)\n",
    "\n",
    "\n",
    "def handle_list(skel, toks, links):\n",
    "    visit_section(body, toks, links)\n",
    "\n",
    "def handle_para(skel: Para, toks, links):\n",
    "    paragraph = skel.paragraph\n",
    "    bodies = paragraph.bodies\n",
    "\n",
    "    for body in bodies:\n",
    "        visit_section(body, toks, links)\n",
    "\n",
    "def handle_paratext(body: ParaBody, toks, links):\n",
    "    #lemmas = [x.text for x in nlp(body.get_text())]\n",
    "    #toks.extend(lemmas)\n",
    "    #links.extend([\"PAD\"] * len(lemmas))\n",
    "    pass\n",
    "\n",
    "def handle_paralink(body: ParaLink, toks, links):\n",
    "    #lemmas = [x.text for x in nlp(body.anchor_text)]\n",
    "    #toks.extend(lemmas)\n",
    "    #links.append([body.page] + [\"PAD\"] * (len(lemmas) - 1))\n",
    "    pass\n",
    "\n",
    "def nothing():\n",
    "    return lambda body, toks, links: None\n",
    "\n",
    "handler = defaultdict(nothing, {Section: handle_section,\n",
    "                     Para: handle_para,\n",
    "                     List: handle_list,\n",
    "                     ParaLink: handle_paralink,\n",
    "                     ParaText: handle_paratext})\n",
    "\n",
    "\n",
    "def visit_section(skel, toks, links):\n",
    "    \n",
    "    # Recur on the sections\n",
    "    handler[type(skel)](skel, toks, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    with wrap_open(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"rb\") as toc:\n",
    "        for idx, page in enumerate(read_data.iter_annotations(toc)):\n",
    "            links = []\n",
    "            toks = []\n",
    "            for skel in page.skeleton:\n",
    "                visit_section(skel, toks, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, \n",
    "import tqdm\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from tools.dumps import get_filename_path\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import random\n",
    "\n",
    "from trec_car.read_data import AnnotationsFile, ParagraphsFile\n",
    "\n",
    "class WikipediaCBOR(Dataset):\n",
    "    \"\"\"\n",
    "    This is a simple CBOR loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, cbor_path):\n",
    "        \"\"\"\n",
    "        :param cbor_path the path of the wikipedia cbor export\n",
    "        \"\"\"\n",
    "        # Let trec deal with that in my place\n",
    "        \n",
    "        self.cbor_path = get_filename_path(cbor_path)\n",
    "        self.cbor_toc_annotations = AnnotationsFile(self.cbor_path)\n",
    "        self.cbor_toc_paragraphs = ParagraphsFile(self.cbor_path)\n",
    "        \n",
    "        self.keys = selfself.cbor_toc_paragraphs.keys()\n",
    "        \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        # preprocess and find the top k unique wikipedia links\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __get_whole_text(paragraph):\n",
    "        # Tokenize by word. When a link is detected, \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx)\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        paragraphs = [self.cbor_toc_paragraphs.get(self.keys[i]) for i in idx]\n",
    "                \n",
    "        # TODO: can we parallelize this?\n",
    "        \n",
    "        \n",
    "    \n",
    "    return torch.tensor(paragraphs)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page(Anarchism)\n",
      "Page(Albedo)\n",
      "Page(Autism)\n",
      "Page(A)\n",
      "Page(Achilles)\n",
      "Page(Alabama)\n",
      "Page(Abraham Lincoln)\n",
      "Page(An American in Paris)\n",
      "Page(Aristotle)\n",
      "Page(Academy Award for Best Production Design)\n"
     ]
    }
   ],
   "source": [
    "with wrap_open(\"wikipedia/car-wiki2020-01-01/enwiki2020.cbor\", \"rb\") as toc:\n",
    "    #toc.seek(46102636493)\n",
    "    \n",
    "    for idx, page in enumerate(read_data.iter_pages(toc)):\n",
    "        if idx >= 10:\n",
    "            break\n",
    "        print(page)\n",
    "        page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In the paper, the authors explain they used a modified BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Embedding, Dropout, ModuleList, Linear\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "GELU = torch.nn.GELU\n",
    "LayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "l0 = 4\n",
    "l1 = 8\n",
    "\n",
    "class SublayerConnection(Module):\n",
    "    \"\"\"A residual connection followed by layer norm and applied dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size: int, dropout: float):\n",
    "        \"\"\"\n",
    "        :param size the size of the layer norm\n",
    "        :param dropout the dropout rate\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(self.norm(sublayer(x)))\n",
    "\n",
    "class PositionwiseFeedForward(Module):\n",
    "    \"\"\"\n",
    "    Positionwise Feed Forward: a Feed-Forward for each token.\n",
    "    We also add dropout here and use GELU(why?)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_hidden, embedding_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_hidden the hidden depth of the FCN\n",
    "        :param embedding_size the size of a single embedding to convert\n",
    "        \"\"\"\n",
    "        self(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = Linear(embedding_size, d_hidden)\n",
    "        self.w_2 = Linear(d_hidden, embedding_size)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class TokenEmbedding(Embedding):\n",
    "    \"\"\"A token embedding is a mere synonim of the default torch embedder.\n",
    "    \n",
    "    It simply keeps a fixed vocabulary size.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "    \n",
    "\n",
    "class PositionalEmbedding(Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        \"\"\"\n",
    "        Setup positional embeddings\n",
    "        \n",
    "        :param d_model: the size of a single embedding\n",
    "        :param max_len: the maximum number of tokens to embed\n",
    "        \"\"\"\n",
    "        super().__init()\n",
    "        \n",
    "        # Compute the positional encodings once in log space\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "        \n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model)).exp()\n",
    "         \n",
    "        # I have no clue here\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "    \n",
    "class SegmentEmbedding(Embedding):\n",
    "    \"\"\"\n",
    "    Segment Embedding. It works in a very similar way to token embed but we constrain the size\n",
    "    of our embedding to 3 (before, current, next sentence?)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0)\n",
    "        \n",
    "\n",
    "class BERTEmbedding(Module):\n",
    "    \"\"\"\n",
    "    BERT Embeddings which is made of the following features:\n",
    "    \n",
    "    1. Token Embedding: normal embedding matrix\n",
    "    2. Positional Embedding: additional position information\n",
    "    3. SegmentEmbedding: Additional sentence segment info (eg. tok_1:1, tok_2:2)\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: the size of the vocabulary\n",
    "        :embed_size: the vector dimensionality of the embeddings\n",
    "        :dropout: dropout rate\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism in BERT.\n",
    "    \n",
    "    Compute \"Scaled Dot Product Attention.\"\n",
    "    See: https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "    \n",
    "    :return return p_attn * value, p_attn (possibly with applied dropout)\n",
    "    \"\"\"\n",
    "    def forward(self, query, key, value, mask=None, dropout: Dropout = None):\n",
    "        \"\"\"\n",
    "        :param query the query matrix\n",
    "        :param key the key matrix\n",
    "        :param value the value matrix\n",
    "        :param mask the mask\n",
    "        :param dropout an instance of pytor\n",
    "        \"\"\"\n",
    "        \n",
    "        # apply key to the query to get attention scores\n",
    "        # Divide by the embedding size to keep the score covariance low\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "            \n",
    "        \n",
    "        # Be sure to erase out masked values\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # get a probability distribution\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        \n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(Module):\n",
    "    \"\"\"\n",
    "    Combine multiple attention heads\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, heads, total_embedding_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param heads the number of attention heads. Equivalent to h in the paper\n",
    "        :param total_embedding_size the size of a token embedding. Must be equal to embedding_per_head * heads,\n",
    "               where embeddding_per_head. Equivalent to d_k * h in the paper.\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        assert total_embedding_size % heads == 0\n",
    "        \n",
    "        self.embedding_per_head = total_embedding_size // heads\n",
    "        self.heads = heads\n",
    "        \n",
    "        # query, key, value\n",
    "        # Note for myself: ModuleList registers all the modules in autograd\n",
    "        self.linear_layers = ModuleList([Linear(total_embedding_size, total_embedding_size) for _ in range(3)])\n",
    "        self.output_linear = Linear(total_embedding_size, total_embedding_size)\n",
    "        self.attention = Attention()\n",
    "        \n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Note, we only use ONE attention block even though we have different attention heads.\n",
    "        \n",
    "        # 1. Perform linear projection in batch from total_embedding_size =>\n",
    "        #    heads x embedding_per_head\n",
    "        #\n",
    "        # Example: query' = self.linear_layers[0](query).view(batch_size, -1, heads, embedding_head)\n",
    "        # Now, each linear layer has size total_embedding_size x total_embedding_size, while a query has shape\n",
    "        # batch_size * heads * total_embedding_size ==> (tiled) batch_size * heads * total_embedding_size * total_embedding_size\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.heads, self.embedding_per_head)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        \n",
    "        # 2. Apply attention\n",
    "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "    \n",
    "        # 3. Concat the various attention heads, then project with a FCN\n",
    "        # Size: batch_size * token_length * heads * self.embedding_per_head\n",
    "        #\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.heads * self.embedding_per_head)\n",
    "        \n",
    "        return self.output_linear(x)\n",
    "\n",
    "class TransformerBlock(Module):\n",
    "    \"\"\"\n",
    "    Bidirectional (actually a-directional) Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHeadedAttention + FeedForward with Sublayer connection\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size: int, attn_heads: int, feed_forward_hidden: int,\n",
    "                 dropout: float):\n",
    "        \"\"\"\n",
    "        :param embedding_size the hidden size of a multiheaded attention layer.\n",
    "        :param attn_heads the number of heads in the multiheaded layer. Must divide embedding_size.\n",
    "        :param feed_forward_hidden size of the hidden feed_forward_hidden layer.\n",
    "        :param dropout the dropout rate\n",
    "        \"\"\"\n",
    "    \n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(heads=attn_heads, embedding_size=embedding_size)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_hidden=feed_forward_hidden,\n",
    "                                                    embedding_size=embedding_size, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(embedding_size, dropout)\n",
    "        self.output_sublayer = SublayerConnection(embedding_size, dropout)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return x.dropout(x)\n",
    "\n",
    "    \n",
    "class EntityMemory(Module):\n",
    "    \"\"\"\n",
    "    Entity Memory, as described in the paper\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size: int, entity_size: int,\n",
    "                   entity_embedding_size: int):\n",
    "        \"\"\"\n",
    "        :param embedding_size the size of an embedding. In the EaE paper it is called d_emb, previously as d_k\n",
    "            (attention_heads * embedding_per_head)\n",
    "        :param entity_size also known as N in the EaE paper, the maximum number of entities we store\n",
    "        :param entity_embedding_size also known as d_ent in the EaE paper, the embedding of each entity\n",
    "        \n",
    "        \"\"\"\n",
    "        self.N = entity_size\n",
    "        self.d_ent = entity_embedding_size\n",
    "        self.w_f = Linear(d_ent, 2*embedding_size)\n",
    "        \n",
    "    def forward(self, x, entity_spans, num_entities, k=None):\n",
    "        \"\"\"\n",
    "        :param x the (raw) output of the first transformer block. It has a shape:\n",
    "                B x N x (embed_size)\n",
    "        :param entity_spans entities and spans of such entities.\n",
    "                Shape: B x C x 3. Each \"row\" contains a triple (e_k, s_mi, t_mi)\n",
    "                where e_k is an (encoded) entity id, s_mi and t_mi are indices.\n",
    "        :param num_entities the number of found entities for each batch.\n",
    "        :param k the number of nearest entities to consider when softmax-ing.\n",
    "                if k = None, all the entities are used.\n",
    "                In the paper, one should set k for when running the inference\n",
    "        \"\"\"\n",
    "        \n",
    "        mentions = [entity_spans[:, :mentions_per_batch] for mentions_per_batch in num_entities]\n",
    "        pass\n",
    "        \n",
    "\n",
    "class EaE(Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_size: int=768,\n",
    "                     attn_heads: int = 12, n_layers: int = 12, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size the size of the vocabulary for the token embedder\n",
    "        :param embedding_size the hidden \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "        \n",
    "        # in the original paper the used embedding size is 512, and the d_ff size is 2048 = 4*512\n",
    "        # Thus we keep a factor of 4\n",
    "        self.feed_forward_hidden = 4*embedding_size\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=embedding_size)\n",
    "        \n",
    "        self.transformer_blocks = ModuleList(\n",
    "            [TransformerBlock(embedding_size, attn_heads, self.feed_forward_hidden, dropout)\n",
    "            for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, x, segment_info):\n",
    "        # x's size: B x T\n",
    "        # attention masking for padded token (i.e. 0 tokens)\n",
    "        # x's size after unsqueeze(1): Bx1xT\n",
    "        # repeat: Bx1xT?\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        \n",
    "        # Get the embeddings for the tokens\n",
    "        x = self.embedding(x, segment_info)\n",
    "        \n",
    "        # Run over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "            \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
