{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiktionary RDF dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parquet_pos = os.path.join(os.getcwd(), \"data/wiktionary/parquet-index_2.11-0.4.1-SNAPSHOT.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                        .config(\"spark.jars\", parquet_pos) \\\n",
    "                        .getOrCreate()\n",
    "\n",
    "# Add the python modules within the jar\n",
    "spark.sparkContext.addPyFile(parquet_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightcopy.index import QueryContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = QueryContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.providers import WiktionaryProvider\n",
    "\n",
    "provider = WiktionaryProvider()\n",
    "sample_1000 = os.path.join(\"data\", provider.get_filename_path(\"sample_1000\", \"parquet\"))\n",
    "context.index.create.mode('overwrite').indexBy(\"word\").parquet(sample_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.index.exists.parquet(sample_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary_df_sample = context.index.parquet(sample_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiktionary_df_sample.filter('word == \"decolorate\"').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.extractors import extract_form, extract_df\n",
    "\n",
    "hijacker_df = extract_form(extract_df(wiktionary_df_sample, 'hijacker'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "hijacker_df.select(explode('translations')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hijacker_df.select(explode('senses').alias('sense_destructured')).select('sense_destructured.examples').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the available namespaces in a handy dict\n",
    "from rdflib import Namespace, namespace\n",
    "\n",
    "namespaces = {\n",
    "    \"dct\": \"http://purl.org/dc/terms/\",\n",
    "    \"ontolex\": \"http://www.w3.org/ns/lemon/ontolex#\",\n",
    "    \"wikibase\": \"http://wikiba.se/ontology#\",\n",
    "    \"wd\": \"http://www.wikidata.org/entity/\",\n",
    "    \"wdt\": \"http://www.wikidata.org/prop/direct/\",\n",
    "    \"kgl\": \"http://grill-lab.org/kg/entity/\",\n",
    "    \"kglprop\": \"http://grill-lab.org/kg/property/\"\n",
    "}\n",
    "\n",
    "namespaces = dict((key, Namespace(val)) for (key, val) in namespaces.items())\n",
    "\n",
    "for ns in dir(namespace):\n",
    "    imported = getattr(namespace, ns)\n",
    "    if isinstance(imported, Namespace) or isinstance(imported, namespace.ClosedNamespace):\n",
    "        namespaces[ns.lower()] = imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dct': Namespace('http://purl.org/dc/terms/'),\n",
       " 'ontolex': Namespace('http://www.w3.org/ns/lemon/ontolex#'),\n",
       " 'wikibase': Namespace('http://wikiba.se/ontology#'),\n",
       " 'wd': Namespace('http://www.wikidata.org/entity/'),\n",
       " 'wdt': Namespace('http://www.wikidata.org/prop/direct/'),\n",
       " 'kgl': Namespace('http://grill-lab.org/kg/entity/'),\n",
       " 'kglprop': Namespace('http://grill-lab.org/kg/property/'),\n",
       " 'csvw': Namespace('http://www.w3.org/ns/csvw#'),\n",
       " 'dc': Namespace('http://purl.org/dc/elements/1.1/'),\n",
       " 'dcat': Namespace('http://www.w3.org/ns/dcat#'),\n",
       " 'dcterms': Namespace('http://purl.org/dc/terms/'),\n",
       " 'doap': Namespace('http://usefulinc.com/ns/doap#'),\n",
       " 'foaf': rdf.namespace.ClosedNamespace('http://xmlns.com/foaf/0.1/'),\n",
       " 'odrl2': Namespace('http://www.w3.org/ns/odrl/2/'),\n",
       " 'org': Namespace('http://www.w3.org/ns/org#'),\n",
       " 'owl': Namespace('http://www.w3.org/2002/07/owl#'),\n",
       " 'prof': Namespace('http://www.w3.org/ns/dx/prof/'),\n",
       " 'prov': rdf.namespace.ClosedNamespace('http://www.w3.org/ns/prov#'),\n",
       " 'rdf': rdf.namespace.ClosedNamespace('http://www.w3.org/1999/02/22-rdf-syntax-ns#'),\n",
       " 'rdfs': rdf.namespace.ClosedNamespace('http://www.w3.org/2000/01/rdf-schema#'),\n",
       " 'sdo': Namespace('https://schema.org/'),\n",
       " 'sh': Namespace('http://www.w3.org/ns/shacl#'),\n",
       " 'skos': rdf.namespace.ClosedNamespace('http://www.w3.org/2004/02/skos/core#'),\n",
       " 'sosa': Namespace('http://www.w3.org/ns/ssn/'),\n",
       " 'ssn': Namespace('http://www.w3.org/ns/sosa/'),\n",
       " 'time': Namespace('http://www.w3.org/2006/time#'),\n",
       " 'void': Namespace('http://rdfs.org/ns/void#'),\n",
       " 'xsd': Namespace('http://www.w3.org/2001/XMLSchema#')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tools.dumps import wrap_open\n",
    "\n",
    "with wrap_open(\"wikidata/grammatical_categories.json\") as fp:\n",
    "    wikidata_grammatical_categories = pd.read_json(fp)\n",
    "\n",
    "with wrap_open(\"wikidata/pos_categories.json\") as fp:\n",
    "    pos_categories = pd.read_json(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity.value</th>\n",
       "      <th>entityLabel.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>http://www.wikidata.org/entity/Q27918551</td>\n",
       "      <td>masculine personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52943193</td>\n",
       "      <td>masculine animate non-personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>http://www.wikidata.org/entity/Q54152717</td>\n",
       "      <td>not masculine personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51929218</td>\n",
       "      <td>first-person singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51929290</td>\n",
       "      <td>first-person plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51929369</td>\n",
       "      <td>second-person singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51929403</td>\n",
       "      <td>second-person plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51929447</td>\n",
       "      <td>third-person singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>http://www.wikidata.org/entity/Q51929517</td>\n",
       "      <td>third-person plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52431955</td>\n",
       "      <td>third-person masculine singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52431970</td>\n",
       "      <td>third-person feminine singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52432983</td>\n",
       "      <td>third-person masculine plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52433019</td>\n",
       "      <td>third-person feminine plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52433289</td>\n",
       "      <td>third-person neuter singular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>http://www.wikidata.org/entity/Q55097773</td>\n",
       "      <td>second-person singular masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>http://www.wikidata.org/entity/Q55097909</td>\n",
       "      <td>third-person singular masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>http://www.wikidata.org/entity/Q55097931</td>\n",
       "      <td>second-person plural masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>http://www.wikidata.org/entity/Q55097982</td>\n",
       "      <td>second-person singular feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>http://www.wikidata.org/entity/Q55098010</td>\n",
       "      <td>second-person plural feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>http://www.wikidata.org/entity/Q55098025</td>\n",
       "      <td>third-person singular feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>http://www.wikidata.org/entity/Q56650485</td>\n",
       "      <td>second-person informal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>http://www.wikidata.org/entity/Q56650487</td>\n",
       "      <td>second-person familiar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>http://www.wikidata.org/entity/Q65091957</td>\n",
       "      <td>first-person singular masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>http://www.wikidata.org/entity/Q65091963</td>\n",
       "      <td>first-person singular feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>http://www.wikidata.org/entity/Q63302102</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 entity.value  \\\n",
       "123  http://www.wikidata.org/entity/Q27918551   \n",
       "128  http://www.wikidata.org/entity/Q52943193   \n",
       "131  http://www.wikidata.org/entity/Q54152717   \n",
       "345  http://www.wikidata.org/entity/Q51929218   \n",
       "346  http://www.wikidata.org/entity/Q51929290   \n",
       "347  http://www.wikidata.org/entity/Q51929369   \n",
       "348  http://www.wikidata.org/entity/Q51929403   \n",
       "349  http://www.wikidata.org/entity/Q51929447   \n",
       "350  http://www.wikidata.org/entity/Q51929517   \n",
       "351  http://www.wikidata.org/entity/Q52431955   \n",
       "352  http://www.wikidata.org/entity/Q52431970   \n",
       "353  http://www.wikidata.org/entity/Q52432983   \n",
       "354  http://www.wikidata.org/entity/Q52433019   \n",
       "355  http://www.wikidata.org/entity/Q52433289   \n",
       "356  http://www.wikidata.org/entity/Q55097773   \n",
       "357  http://www.wikidata.org/entity/Q55097909   \n",
       "358  http://www.wikidata.org/entity/Q55097931   \n",
       "359  http://www.wikidata.org/entity/Q55097982   \n",
       "360  http://www.wikidata.org/entity/Q55098010   \n",
       "361  http://www.wikidata.org/entity/Q55098025   \n",
       "362  http://www.wikidata.org/entity/Q56650485   \n",
       "363  http://www.wikidata.org/entity/Q56650487   \n",
       "365  http://www.wikidata.org/entity/Q65091957   \n",
       "366  http://www.wikidata.org/entity/Q65091963   \n",
       "387  http://www.wikidata.org/entity/Q63302102   \n",
       "\n",
       "                    entityLabel.value  \n",
       "123                masculine personal  \n",
       "128    masculine animate non-personal  \n",
       "131            not masculine personal  \n",
       "345             first-person singular  \n",
       "346               first-person plural  \n",
       "347            second-person singular  \n",
       "348              second-person plural  \n",
       "349             third-person singular  \n",
       "350               third-person plural  \n",
       "351   third-person masculine singular  \n",
       "352    third-person feminine singular  \n",
       "353     third-person masculine plural  \n",
       "354      third-person feminine plural  \n",
       "355      third-person neuter singular  \n",
       "356  second-person singular masculine  \n",
       "357   third-person singular masculine  \n",
       "358    second-person plural masculine  \n",
       "359   second-person singular feminine  \n",
       "360     second-person plural feminine  \n",
       "361    third-person singular feminine  \n",
       "362            second-person informal  \n",
       "363            second-person familiar  \n",
       "365   first-person singular masculine  \n",
       "366    first-person singular feminine  \n",
       "387                          personal  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for verbs\n",
    "\n",
    "wikidata_grammatical_categories[wikidata_grammatical_categories['entityLabel.value'].str.contains(\"person\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity.value</th>\n",
       "      <th>entityLabel.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>http://www.wikidata.org/entity/Q192613</td>\n",
       "      <td>present tense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1240211</td>\n",
       "      <td>present perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>http://www.wikidata.org/entity/Q3502553</td>\n",
       "      <td>present subjunctive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>http://www.wikidata.org/entity/Q3686414</td>\n",
       "      <td>conditional present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>http://www.wikidata.org/entity/Q3910936</td>\n",
       "      <td>simple present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>http://www.wikidata.org/entity/Q7240943</td>\n",
       "      <td>present continuous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>http://www.wikidata.org/entity/Q9062494</td>\n",
       "      <td>present perfect in English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>http://www.wikidata.org/entity/Q10345583</td>\n",
       "      <td>present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52434162</td>\n",
       "      <td>present imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52434245</td>\n",
       "      <td>present infinitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52434511</td>\n",
       "      <td>present gerund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>http://www.wikidata.org/entity/Q56682909</td>\n",
       "      <td>present indicative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>http://www.wikidata.org/entity/Q1763348</td>\n",
       "      <td>present participe in French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>http://www.wikidata.org/entity/Q3686414</td>\n",
       "      <td>conditional present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>http://www.wikidata.org/entity/Q10345583</td>\n",
       "      <td>present participle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>http://www.wikidata.org/entity/Q52434245</td>\n",
       "      <td>present infinitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>http://www.wikidata.org/entity/Q12738495</td>\n",
       "      <td>present perfect continuous in English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>http://www.wikidata.org/entity/Q56649265</td>\n",
       "      <td>present imperfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>http://www.wikidata.org/entity/Q12738495</td>\n",
       "      <td>present perfect continuous in English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>http://www.wikidata.org/entity/Q12738495</td>\n",
       "      <td>present perfect continuous in English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 entity.value  \\\n",
       "136    http://www.wikidata.org/entity/Q192613   \n",
       "144   http://www.wikidata.org/entity/Q1240211   \n",
       "152   http://www.wikidata.org/entity/Q3502553   \n",
       "153   http://www.wikidata.org/entity/Q3686414   \n",
       "154   http://www.wikidata.org/entity/Q3910936   \n",
       "156   http://www.wikidata.org/entity/Q7240943   \n",
       "157   http://www.wikidata.org/entity/Q9062494   \n",
       "158  http://www.wikidata.org/entity/Q10345583   \n",
       "174  http://www.wikidata.org/entity/Q52434162   \n",
       "175  http://www.wikidata.org/entity/Q52434245   \n",
       "177  http://www.wikidata.org/entity/Q52434511   \n",
       "180  http://www.wikidata.org/entity/Q56682909   \n",
       "203   http://www.wikidata.org/entity/Q1763348   \n",
       "209   http://www.wikidata.org/entity/Q3686414   \n",
       "218  http://www.wikidata.org/entity/Q10345583   \n",
       "224  http://www.wikidata.org/entity/Q52434245   \n",
       "232  http://www.wikidata.org/entity/Q12738495   \n",
       "233  http://www.wikidata.org/entity/Q56649265   \n",
       "340  http://www.wikidata.org/entity/Q12738495   \n",
       "400  http://www.wikidata.org/entity/Q12738495   \n",
       "\n",
       "                         entityLabel.value  \n",
       "136                          present tense  \n",
       "144                        present perfect  \n",
       "152                    present subjunctive  \n",
       "153                    conditional present  \n",
       "154                         simple present  \n",
       "156                     present continuous  \n",
       "157             present perfect in English  \n",
       "158                     present participle  \n",
       "174                     present imperative  \n",
       "175                     present infinitive  \n",
       "177                         present gerund  \n",
       "180                     present indicative  \n",
       "203            present participe in French  \n",
       "209                    conditional present  \n",
       "218                     present participle  \n",
       "224                     present infinitive  \n",
       "232  present perfect continuous in English  \n",
       "233                      present imperfect  \n",
       "340  present perfect continuous in English  \n",
       "400  present perfect continuous in English  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikidata_grammatical_categories[wikidata_grammatical_categories['entityLabel.value'].str.contains(\"present\")]\n",
    "\n",
    "# Interesting: present tense, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kgl = namespaces[\"kgl\"]\n",
    "kgl_prop = namespaces[\"kglprop\"]\n",
    "form_link = namespaces[\"ontolex\"].lexicalForm\n",
    "kgl_form_link = kgl_prop.form\n",
    "sense_link = namespaces[\"ontolex\"].sense\n",
    "kgl_sense_link = kgl_prop.sense\n",
    "form_label = namespaces[\"ontolex\"].representation\n",
    "rdfs_label = namespaces[\"rdfs\"].label\n",
    "rdf_type = namespaces[\"rdf\"].type\n",
    "pos_link = kgl_prop.pos\n",
    "\n",
    "sameAs = namespaces[\"owl\"].sameAs\n",
    "definition = namespaces[\"skos\"].definition\n",
    "kgl_definition = kgl_prop.definition\n",
    "grammaticalFeature = kgl_prop.grammaticalFeature\n",
    "kgl_label = kgl_prop.label\n",
    "example_link = kgl_prop.example\n",
    "\n",
    "from rdflib import URIRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_category(g, label):\n",
    "    cat_id = kgl[hash(label, \"grammatical_category\")]\n",
    "    g.add((cat_id, rdfs_label, Literal(label)))\n",
    "    g.add((cat_id, kgl_label, Literal(label)))\n",
    "    g.add((cat_id, rdf_type, kgl.GrammaticalCategory))\n",
    "    return cat_id\n",
    "\n",
    "def populate_categories(g: Graph):\n",
    "    categories = {}\n",
    "    \n",
    "    for row in wikidata_grammatical_categories.iterrows():\n",
    "        label = row[1]['entityLabel.value']\n",
    "        wikidata_identifier = row[1]['entity.value']\n",
    "        cat_id = add_category(g, label)\n",
    "        g.add((cat_id, sameAs, URIRef(wikidata_identifier)))\n",
    "        categories[label] = cat_id\n",
    "\n",
    "    # Wikidata is a horrible mess\n",
    "    # Apparently some of the most beefy categories are not (in)direct subclasses\n",
    "    # of \"grammatical categories\".\n",
    "    extra_noun_categories = [\"countable\", \"uncountable\", \"irregular\",\n",
    "                                  \"usually uncountable\", \"unattested plural\",\n",
    "                                  \"uncertain plural\"]\n",
    "    \n",
    "    extra_verb_categories = [\"defective\"]\n",
    "    \n",
    "    extra_adjective_categories = [\"positive\", \"comparative\", \"superlative\",\n",
    "                                        \"not comparable\", \"comparable-only\",\n",
    "                                        \"generally not comparable\"]\n",
    "\n",
    "    for cat in extra_noun_categories + extra_verb_categories + extra_adjective_categories:\n",
    "        cat_id = add_category(g, cat)\n",
    "        categories[cat] = cat_id\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mmh3\n",
    "\n",
    "def hash(word, pos):\n",
    "    mmhash = mmh3.hash64(word + pos, signed=False)[0]\n",
    "    mmhash = int.to_bytes(mmhash, 8, \"big\")\n",
    "    return bytes.decode(base64.b32encode(mmhash)).rstrip(\"=\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Literal\n",
    "from collections import defaultdict\n",
    "\n",
    "def is_in_graph(x):\n",
    "    try:\n",
    "        next(g.triples((x, None, None)))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "def add_form(g: Graph, word_id: str, lexeme_id: URIRef, label: str):\n",
    "    count = form_counter[word_id]\n",
    "    form_id = kgl[f\"{word_id}-F{count}\"]\n",
    "    form_counter[word_id] += 1\n",
    "    g.add((lexeme_id, form_link, form_id))\n",
    "    g.add((lexeme_id, kgl_form_link, form_id))\n",
    "    g.add((form_id, namespaces['rdf'].type, kgl.Form))\n",
    "    g.add((form_id, kgl_prop['label'], Literal(label, lang=\"en\")))\n",
    "    g.add((form_id, rdfs_label, Literal(label, lang=\"en\")))\n",
    "    g.add((form_id, form_label, Literal(label, lang=\"en\")))\n",
    "    return form_id\n",
    "\n",
    "def add_sense(g: Graph, word_id: str, lexeme_id: URIRef, sense_definition: str):\n",
    "    count = sense_counter[word_id]\n",
    "    sense_id = kgl[f\"{word_id}-S{count}\"]\n",
    "    sense_counter[word_id] += 1\n",
    "    g.add((lexeme_id, sense_link, sense_id))\n",
    "    g.add((lexeme_id, kgl_sense_link, sense_id))\n",
    "    g.add((sense_id, namespaces['rdf'].type, kgl.Sense))\n",
    "    g.add((sense_id, rdfs_label, Literal(sense_definition, lang=\"en\")))\n",
    "    g.add((sense_id, definition, Literal(sense_definition, lang=\"en\")))\n",
    "    g.add((sense_id, kgl_definition, Literal(sense_definition, lang=\"en\")))\n",
    "    return sense_id\n",
    "\n",
    "def add_grammatical_categories(g,  word_id, cats):\n",
    "    for cat in cats:\n",
    "        g.add((word_id, grammaticalFeature, categories[cat]))\n",
    "        \n",
    "def add_to_graph(g: Graph, row):\n",
    "    \n",
    "    word = row['word']\n",
    "    senses = row['senses']\n",
    "    pos = row['pos']\n",
    "    noun_forms = row['noun_forms']\n",
    "    adj_forms = row['adj_forms']\n",
    "    verb_forms = row['verb_forms']\n",
    "    \n",
    "    word_id = hash(word, pos)\n",
    "    lexeme_id = kgl[word_id]\n",
    "    if not is_in_graph(word_id):\n",
    "        g.add((lexeme_id, namespaces['rdf'].type, kgl.Lexeme))\n",
    "        g.add((lexeme_id, pos_link, kgl[pos]))\n",
    "        g.add((lexeme_id, kgl_label, Literal(word, lang=\"en\")))\n",
    "        g.add((lexeme_id, rdfs_label, Literal(word, lang=\"en\")))\n",
    "        # g.add((lexeme_id, namespace['dct'].language, something_for_english_language))\n",
    "        \n",
    "    \n",
    "    # Detect collision by just looking at the word label.\n",
    "    # In theory we should also check that different pos may cause a collision\n",
    "    # but it looks extremely unlikely\n",
    "    else:\n",
    "        label = g.label(word_id)\n",
    "        if label != word:\n",
    "            print(f\"Detected collision between {label} and {word}\")\n",
    "            word_id = hash(word + \"$42\", pos)\n",
    "            lexeme_id = kgl[word_id]\n",
    "            g.add((lexeme_id, pos_link, kgl[pos]))\n",
    "            g.add((lexeme_id, kgl_prop.label, Literal(word, lang=\"en\")))\n",
    "            g.add((lexeme_id, namespaces['rdfs'].label, Literal(word, lang=\"en\")))\n",
    "    \n",
    "    # Senses\n",
    "    for sense in senses:\n",
    "        glosses = sense['glosses']\n",
    "        examples = sense['examples']\n",
    "        if glosses:\n",
    "            for gloss in glosses:\n",
    "                sense = add_sense(g, word_id, lexeme_id, gloss)\n",
    "        if examples:\n",
    "            for example in examples:\n",
    "                if example:\n",
    "                    g.add((word_id, example_link, Literal(example, lang=\"en\")))\n",
    "            \n",
    "    # Nouns\n",
    "    if noun_forms:\n",
    "        if noun_forms.irregular:\n",
    "            g.add((lexeme_id, grammaticalFeature, categories[\"irregular\"]))\n",
    "        \n",
    "        # countable can be either no or yes or sometimes.\n",
    "        if not noun_forms.countable == \"no\":\n",
    "            g.add((lexeme_id, grammaticalFeature, categories[\"countable\"]))\n",
    "        if not noun_forms.countable == \"yes\":\n",
    "            if noun_forms.always:\n",
    "                g.add((lexeme_id, grammaticalFeature, categories[\"uncountable\"]))\n",
    "            else:\n",
    "                g.add((lexeme_id, grammaticalFeature, categories[\"usually uncountable\"]))\n",
    "            \n",
    "        if noun_forms[\"optional\"]:\n",
    "            add_grammatical_categories(g, lexeme_id,\n",
    "                                            [noun_forms[\"optional\"] + \" plural\"])\n",
    "\n",
    "        singular = add_form(g, word_id, lexeme_id, word)\n",
    "        g.add((singular, grammaticalFeature, categories['singular']))\n",
    "        \n",
    "        if noun_forms[\"plurals\"]:\n",
    "            for plural in noun_forms.plurals:\n",
    "                form_id = add_form(g, word_id, lexeme_id, plural)\n",
    "                add_grammatical_categories(g, form_id, ['plural'])\n",
    "      \n",
    "    # Adjectives\n",
    "    if adj_forms:\n",
    "        opt = adj_forms['optional']\n",
    "        if adj_forms['optional']:\n",
    "            add_grammatical_categories(g, lexeme_id, [opt])\n",
    "        if opt is None or opt != \"not comparable\":\n",
    "            positive_form = add_form(g, word_id, lexeme_id, word)\n",
    "            add_grammatical_categories(g, positive_form, [\"positive\"])\n",
    "        \n",
    "        if adj_forms['comparatives']:\n",
    "            for comp in adj_forms['comparatives']:\n",
    "                comp_form = add_form(g, word_id, lexeme_id, comp)\n",
    "                add_grammatical_categories(g, comp_form, [\"comparative\"])\n",
    "    \n",
    "        if adj_forms['superlatives']:\n",
    "            for sup in adj_forms['superlatives']:\n",
    "                sup_form = add_form(g, word_id, lexeme_id, sup)\n",
    "                add_grammatical_categories(g, sup_form, [\"superlative\"])\n",
    "        \n",
    "    # Verbs\n",
    "    if verb_forms:\n",
    "        infinitive = add_form(g, word_id, lexeme_id, word)\n",
    "        add_grammatical_categories(g, infinitive,\n",
    "                                        [\"present tense\", \"infinitive\",\n",
    "                                         \"first-person singular\",\n",
    "                                         \"second-person singular\",\n",
    "                                         \"first-person plural\",\n",
    "                                         \"second-person plural\", \n",
    "                                         \"third-person plural\"])\n",
    "    \n",
    "        if verb_forms[\"pres_3sg\"]:\n",
    "            pres_3sg = add_form(g, word_id, lexeme_id, verb_forms[\"pres_3sg\"])\n",
    "            add_grammatical_categories(g, pres_3sg,\n",
    "                                       [\"present tense\",\n",
    "                                        \"third-person singular\"])\n",
    "        \n",
    "        else:\n",
    "            add_grammatical_categories(g, lexeme_id, [\"defective\"])\n",
    "        \n",
    "        pres_ptc = add_form(g, word_id, lexeme_id, verb_forms[\"pres_ptc\"])\n",
    "        add_grammatical_categories(g, pres_ptc, [\"present participle\"])\n",
    "        \n",
    "        past = add_form(g, word_id, lexeme_id, verb_forms[\"past\"])\n",
    "        add_grammatical_categories(g, past, [\"past tense\", \"simple past\"])\n",
    "            \n",
    "\n",
    "        past_ptc = add_form(g, word_id, lexeme_id, verb_forms[\"past_ptc\"])\n",
    "        add_grammatical_categories(g, past_ptc, [\"past participle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib.extras.infixowl import Class\n",
    "def reset():\n",
    "    global g\n",
    "    global form_counter\n",
    "    global sense_counter\n",
    "    global lexemeClass\n",
    "    global formClass\n",
    "    \n",
    "    form_counter = defaultdict(int)\n",
    "    sense_counter = defaultdict(int)\n",
    "    \n",
    "    g = Graph()\n",
    "    for k,v in namespaces.items():\n",
    "        g.bind(k, v)\n",
    "    \n",
    "    lexemeClass = Class(kgl.Lexeme,\n",
    "                            nameAnnotation=Literal(\"Lexeme\"),\n",
    "                            graph=g)\n",
    "    lexemeClass.comment = Literal(\"A lexeme is the main entry of the dictionary\")\n",
    "    formClass = Class(kgl.Form, nameAnnotation=Literal(\"Form\"), graph=g)\n",
    "    formClass.comment = Literal(\"A form is a morphological form that appears when the lexeme is a declinable or conjugable noun\")\n",
    "    senseClass = Class(kgl.Sense, nameAnnotation=Literal(\"Sense\"), graph=g)\n",
    "    senseClass.comment = Literal(\"A sense, or synset, is a unit of meaning of a lexeme\")\n",
    "    \n",
    "    grammaticalCategory = Class(kgl.GrammaticalCategory, graph=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import NullType\n",
    "\n",
    "reset()\n",
    "categories = populate_categories(g)\n",
    "first = True\n",
    "# This is dumb, but until I get SANSA or anything more decent to work...\n",
    "for row in extract_form(extract_df(wiktionary_df_sample)).rdd.toLocalIterator():\n",
    "    # Exclude automatically-generated single forms from being retrieved\n",
    "    if(row['head']['template_name'] != 'head'):\n",
    "        add_to_graph(g, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize(\"data/wiktionary/sample_1000.ttl\", \"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wiktionary(url):\n",
    "    r = get(url)\n",
    "    parsed = BeautifulSoup(r.content, \"html.parser\")\n",
    "    tables = parsed.find_all(\"table\")\n",
    "    table = tables[0]\n",
    "    rows = table.find_all(\"tr\")[1:]\n",
    "    cols = [row.find_all(\"td\")[1].find(\"a\").text for row in rows]\n",
    "    \n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpg_list = scrape_wiktionary(\"https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-10000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdpg_df = spark.createDataFrame([[word] for word in wdpg_list], ['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary_full_df = spark.read.parquet(\"data/wiktionary/sample_full.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary_full_df_json = spark.read.json(\"data/wiktionary/wikt.words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Can not create the managed table('`wiktionary_wdpg`'). The associated location('file:/nfs/knowledge-glue/notebooks/spark-warehouse/wiktionary_wdpg') already exists.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/share/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o340.saveAsTable.\n: org.apache.spark.sql.AnalysisException: Can not create the managed table('`wiktionary_wdpg`'). The associated location('file:/nfs/knowledge-glue/notebooks/spark-warehouse/wiktionary_wdpg') already exists.;\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:336)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:170)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8c30b2c2ab54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwiktionary_full_df_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwdpg_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wiktionary_wdpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/share/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/share/spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Can not create the managed table('`wiktionary_wdpg`'). The associated location('file:/nfs/knowledge-glue/notebooks/spark-warehouse/wiktionary_wdpg') already exists.;\""
     ]
    }
   ],
   "source": [
    "wiktionary_full_df_json.join(wdpg_df, \"word\", \"inner\").write.saveAsTable(\"wiktionary_wdpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary_wdpg_df = spark.read.parquet(\"spark-warehouse/wiktionary_wdpg\")\n",
    "\n",
    "reset()\n",
    "categories = populate_categories(g)\n",
    "# This is dumb, but until I get SANSA or anything more decent to work...\n",
    "for row in extract_form(extract_df(wiktionary_wdpg_df)).rdd.toLocalIterator():\n",
    "    # Exclude automatically-generated single forms from being retrieved\n",
    "    if(row['head']['template_name'] != 'head'):\n",
    "        add_to_graph(g, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize(\"data/wiktionary/sample_10000.ttl\", \"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Literal\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "\n",
    "def is_in_graph(x):\n",
    "    try:\n",
    "        next(g.triples((x, None, None)))\n",
    "        return True\n",
    "    except StopIteration:\n",
    "        return False\n",
    "\n",
    "def add_form(g: Graph, word_id: str, lexeme_id: URIRef, label: str):\n",
    "    count = form_counter[word_id]\n",
    "    form_id = kgl[f\"{word_id}-F{count}\"]\n",
    "    form_counter[word_id] += 1\n",
    "    g.add((lexeme_id, form_link, form_id))\n",
    "    g.add((lexeme_id, kgl_form_link, form_id))\n",
    "    g.add((form_id, namespaces['rdf'].type, kgl.Form))\n",
    "    g.add((form_id, kgl_prop['label'], Literal(label, lang=\"en\")))\n",
    "    g.add((form_id, rdfs_label, Literal(label, lang=\"en\")))\n",
    "    g.add((form_id, form_label, Literal(label, lang=\"en\")))\n",
    "    return form_id\n",
    "\n",
    "\n",
    "class SenseType(Enum):\n",
    "    SENSE = 1,\n",
    "    SUBSENSE = 2,\n",
    "    USAGE = 3\n",
    "\n",
    "    \n",
    "def add_sense(g: Graph, word_id: str, lexeme_id: URIRef, sense_definition: str, parent_sense_id=None, sense_type=SenseType.SENSE):\n",
    "    count = sense_counter[word_id]\n",
    "    sense_id = kgl[f\"{word_id}-S{count}\"]\n",
    "    sense_counter[word_id] += 1\n",
    "    \n",
    "    if sense_type == SenseType.SENSE:\n",
    "        g.add((lexeme_id, sense_link, sense_id))\n",
    "        g.add((lexeme_id, kgl_sense_link, sense_id))\n",
    "    elif sense_type == SenseType.SUBSENSE:\n",
    "        g.add((parent_sense_id, kgl_prop['subsense'], sense_id))\n",
    "    else:\n",
    "        g.add((parent_sense_id, kgl_prop['usage'], sense_id))\n",
    "    \n",
    "    g.add((sense_id, namespaces['rdf'].type, kgl.Sense))\n",
    "    g.add((sense_id, rdfs_label, Literal(sense_definition, lang=\"en\")))\n",
    "    g.add((sense_id, definition, Literal(sense_definition, lang=\"en\")))\n",
    "    g.add((sense_id, kgl_definition, Literal(sense_definition, lang=\"en\")))\n",
    "    \n",
    "    return sense_id\n",
    "\n",
    "def add_sense_rec(g: Graph, senses, word_id, lexeme_id, depth=0, parent_sense=None):    \n",
    "    for sense in senses:\n",
    "        # TODO: now that senses are hierarchically structured, glosses should become a single string\n",
    "        gloss = sense['glosses'][0] if sense['glosses'] else \"\"\n",
    "        examples = sense['examples']\n",
    "        if gloss:\n",
    "            senseType = {0: SenseType.SENSE, 1: SenseType.SUBSENSE, 2: SenseType.USAGE}\n",
    "            sense_id = add_sense(g, word_id, lexeme_id, gloss, parent_sense, senseType[depth])\n",
    "                \n",
    "        if examples:\n",
    "            for example in examples:\n",
    "                if example:\n",
    "                    g.add((lexeme_id, example_link, Literal(example, lang=\"en\")))\n",
    "                    \n",
    "        if 'subsenses' in row and row['subsenses'] is not None:\n",
    "            print(row['word'] + \" has subsenses!\")\n",
    "            add_sense_rec(g, row['subsenses'], word_id, lexeme_id, depth+1, sense_id)\n",
    "        if 'usages' in row and row['usages'] is not None:\n",
    "            print(row['word'] + \" has usages!\")\n",
    "            add_sense_rec(g, row['usages'], word_id, lexeme_id, depth+1, sense_id)\n",
    "                    \n",
    "       \n",
    "    \n",
    "def add_grammatical_categories(g,  word_id, cats):\n",
    "    for cat in cats:\n",
    "        g.add((word_id, grammaticalFeature, categories[cat]))\n",
    "\n",
    "def add_noun_forms(g: Graph, word, word_id, lexeme_id, noun_forms):\n",
    "    if noun_forms.irregular:\n",
    "        g.add((lexeme_id, grammaticalFeature, categories[\"irregular\"]))\n",
    "\n",
    "    # countable can be either no or yes or sometimes.\n",
    "    if not noun_forms.countable == \"no\":\n",
    "        g.add((lexeme_id, grammaticalFeature, categories[\"countable\"]))\n",
    "    if not noun_forms.countable == \"yes\":\n",
    "        if noun_forms.always:\n",
    "            g.add((lexeme_id, grammaticalFeature, categories[\"uncountable\"]))\n",
    "        else:\n",
    "            g.add((lexeme_id, grammaticalFeature, categories[\"usually uncountable\"]))\n",
    "\n",
    "    if noun_forms[\"optional\"]:\n",
    "        add_grammatical_categories(g, lexeme_id,\n",
    "                                        [noun_forms[\"optional\"] + \" plural\"])\n",
    "\n",
    "    singular = add_form(g, word_id, lexeme_id, word)\n",
    "    g.add((singular, grammaticalFeature, categories['singular']))\n",
    "\n",
    "    if noun_forms[\"plurals\"]:\n",
    "        for plural in noun_forms.plurals:\n",
    "            form_id = add_form(g, word_id, lexeme_id, plural)\n",
    "            add_grammatical_categories(g, form_id, ['plural'])\n",
    "\n",
    "\n",
    "def add_adj_forms(g: Graph, word,  word_id, lexeme_id, adj_forms):\n",
    "    opt = adj_forms['optional']\n",
    "    if adj_forms['optional']:\n",
    "        add_grammatical_categories(g, lexeme_id, [opt])\n",
    "    if opt is None or opt != \"not comparable\":\n",
    "        positive_form = add_form(g, word_id, lexeme_id, word)\n",
    "        add_grammatical_categories(g, positive_form, [\"positive\"])\n",
    "\n",
    "    if adj_forms['comparatives']:\n",
    "        for comp in adj_forms['comparatives']:\n",
    "            comp_form = add_form(g, word_id, lexeme_id, comp)\n",
    "            add_grammatical_categories(g, comp_form, [\"comparative\"])\n",
    "\n",
    "    if adj_forms['superlatives']:\n",
    "        for sup in adj_forms['superlatives']:\n",
    "            sup_form = add_form(g, word_id, lexeme_id, sup)\n",
    "            add_grammatical_categories(g, sup_form, [\"superlative\"])\n",
    "\n",
    "            \n",
    "def add_verb_forms(g: Graph, word: str, word_id, lexeme_id, verb_forms):\n",
    "    infinitive = add_form(g, word_id, lexeme_id, word)\n",
    "    add_grammatical_categories(g, infinitive,\n",
    "                                    [\"present tense\", \"infinitive\",\n",
    "                                     \"first-person singular\",\n",
    "                                     \"second-person singular\",\n",
    "                                     \"first-person plural\",\n",
    "                                     \"second-person plural\", \n",
    "                                     \"third-person plural\"])\n",
    "\n",
    "    if verb_forms[\"pres_3sg\"]:\n",
    "        pres_3sg = add_form(g, word_id, lexeme_id, verb_forms[\"pres_3sg\"])\n",
    "        add_grammatical_categories(g, pres_3sg,\n",
    "                                   [\"present tense\",\n",
    "                                    \"third-person singular\"])\n",
    "\n",
    "    else:\n",
    "        add_grammatical_categories(g, lexeme_id, [\"defective\"])\n",
    "\n",
    "    pres_ptc = add_form(g, word_id, lexeme_id, verb_forms[\"pres_ptc\"])\n",
    "    add_grammatical_categories(g, pres_ptc, [\"present participle\"])\n",
    "\n",
    "    past = add_form(g, word_id, lexeme_id, verb_forms[\"past\"])\n",
    "    add_grammatical_categories(g, past, [\"past tense\", \"simple past\"])\n",
    "\n",
    "\n",
    "    past_ptc = add_form(g, word_id, lexeme_id, verb_forms[\"past_ptc\"])\n",
    "    add_grammatical_categories(g, past_ptc, [\"past participle\"])\n",
    "        \n",
    "def add_to_graph(g: Graph, row):\n",
    "    \n",
    "    word = row['word']\n",
    "    senses = row['senses']\n",
    "    pos = row['pos']\n",
    "    noun_forms = row['noun_forms']\n",
    "    adj_forms = row['adj_forms']\n",
    "    verb_forms = row['verb_forms']\n",
    "    \n",
    "    word_id = hash(word, pos)\n",
    "    lexeme_id = kgl[word_id]\n",
    "    if not is_in_graph(word_id):\n",
    "        g.add((lexeme_id, namespaces['rdf'].type, kgl.Lexeme))\n",
    "        g.add((lexeme_id, pos_link, kgl[pos]))\n",
    "        g.add((lexeme_id, kgl_label, Literal(word, lang=\"en\")))\n",
    "        g.add((lexeme_id, rdfs_label, Literal(word, lang=\"en\")))\n",
    "        # g.add((lexeme_id, namespace['dct'].language, something_for_english_language))\n",
    "        \n",
    "    \n",
    "    # Detect collision by just looking at the word label.\n",
    "    # In theory we should also check that different pos may cause a collision\n",
    "    # but it looks extremely unlikely\n",
    "    else:\n",
    "        label = g.label(word_id)\n",
    "        if label != word:\n",
    "            print(f\"Detected collision between {label} and {word}\")\n",
    "            word_id = hash(word + \"$42\", pos)\n",
    "            lexeme_id = kgl[word_id]\n",
    "            g.add((lexeme_id, pos_link, kgl[pos]))\n",
    "            g.add((lexeme_id, kgl_prop.label, Literal(word, lang=\"en\")))\n",
    "            g.add((lexeme_id, namespaces['rdfs'].label, Literal(word, lang=\"en\")))\n",
    "    \n",
    "    if row['senses']:\n",
    "        add_sense_rec(g, row['senses'], word_id, lexeme_id)\n",
    "            \n",
    "    # Nouns\n",
    "    if noun_forms:\n",
    "        add_noun_forms(g, word, word_id, lexeme_id, noun_forms)\n",
    "      \n",
    "    # Adjectives\n",
    "    if adj_forms:\n",
    "        add_adj_forms(g, word, word_id, lexeme_id, adj_forms)\n",
    "        \n",
    "    # Verbs\n",
    "    if verb_forms:\n",
    "        add_verb_forms(g, word, word_id, lexeme_id, verb_forms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary_v2_df = spark.read.parquet(\"data/wiktionary/senses_examples_quotations_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()\n",
    "categories = populate_categories(g)\n",
    "# This is dumb, but until I get SANSA or anything more decent to work...\n",
    "for row in wiktionary_v2_df.rdd.toLocalIterator():\n",
    "    # Exclude automatically-generated single forms from being retrieved\n",
    "    #print(row['head']['template_name'])\n",
    "    if(row['head']['template_name'] != 'head'):\n",
    "        add_to_graph(g, row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize(\"data/wiktionary/full_v1.ttl\", \"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
