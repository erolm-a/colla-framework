{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sippycup semantic parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('3rdparty/sippycup')\n",
    "from annotator import *\n",
    "from parsing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rule(grammar, rule):\n",
    "    if contains_optionals(rule):\n",
    "        add_rule_containing_optional(grammar, rule)\n",
    "    elif is_lexical(rule):\n",
    "        grammar.lexical_rules[rule.rhs].append(rule)\n",
    "    elif is_unary(rule):\n",
    "        grammar.unary_rules[rule.rhs].append(rule)\n",
    "    elif is_binary(rule):\n",
    "        grammar.binary_rules[rule.rhs].append(rule)\n",
    "    elif all([is_cat(rhsi) for rhsi in rule.rhs]):\n",
    "        add_n_ary_rule(grammar, rule)\n",
    "    else:\n",
    "        make_cat(grammar, rule)\n",
    "        # raise Exception('RHS mixes terminals and non-terminals: %s' % rule\n",
    "\n",
    "def add_rule_containing_optional(grammar, rule):\n",
    "    # Find index of the first optional element on the RHS.\n",
    "    first = next((idx for idx, elt in enumerate(rule.rhs) if is_optional(elt)), -1)\n",
    "    assert first >= 0\n",
    "    assert len(rule.rhs) > 1, 'Entire RHS is optional: %s' % rule\n",
    "    prefix = rule.rhs[:first]\n",
    "    suffix = rule.rhs[(first + 1):]\n",
    "    # First variant: the first optional element gets deoptionalized.\n",
    "    deoptionalized = (rule.rhs[first][1:],)\n",
    "    add_rule(grammar, Rule(rule.lhs, prefix + deoptionalized + suffix, rule.sem))\n",
    "    # Second variant: the first optional element gets removed.\n",
    "    # If the semantics is a value, just keep it as is.\n",
    "    sem = rule.sem\n",
    "    # But if it's a function, we need to supply a dummy argument for the removed element.\n",
    "    if isinstance(rule.sem, FunctionType):\n",
    "        sem = lambda sems: rule.sem(sems[:first] + [None] + sems[first:])\n",
    "    add_rule(grammar, Rule(rule.lhs, prefix + suffix, sem))\n",
    "\n",
    "def make_cat(grammar, rule):\n",
    "    \"\"\"\n",
    "    Convert a terminal in the RHS into a non-terminal.\n",
    "    \n",
    "    Conversion works by creating a nonterminal from each terminal if\n",
    "    it does not exist already in the grammar, otherwise it just replaces it.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_rhs = []\n",
    "    for rhsi in rule.rhs:\n",
    "        if is_cat(rhsi):\n",
    "            cat_name = rhsi\n",
    "        else:\n",
    "            cat_name = \"$\" + rhsi + \"__nonterminal\"\n",
    "            if cat_name not in grammar.categories:\n",
    "                grammar.categories.add(cat_name)\n",
    "                # print(f\"Adding rule: {cat_name} := {str(rhsi)}\")\n",
    "                add_rule(grammar, Rule(cat_name, rhsi))\n",
    "        new_rhs.append(cat_name)\n",
    "        # print(f\"Adding rule: {rule.lhs} := {str(new_rhs)}\")\n",
    "    add_rule(grammar, Rule(rule.lhs, tuple(new_rhs), rule.sem))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input(grammar, input):\n",
    "    \"\"\"Returns a list of all parses for input using grammar.\"\"\"\n",
    "    tokens_spacy = nlp(input) # New\n",
    "    tokens = [token.text for token in tokens_spacy]\n",
    "    print(tokens)\n",
    "    chart = defaultdict(list)\n",
    "    for j in range(1, len(tokens) + 1):\n",
    "        for i in range(j - 1, -1, -1):\n",
    "            apply_annotators(grammar, chart, tokens, i, j)\n",
    "            apply_lexical_rules(grammar, chart, tokens, i, j)\n",
    "            apply_binary_rules(grammar, chart, i, j)\n",
    "            apply_unary_rules(grammar, chart, i, j)\n",
    "    parses = chart[(0, len(tokens))]\n",
    "    if hasattr(grammar, 'start_symbol') and grammar.start_symbol:\n",
    "        parses = [parse for parse in parses if parse.rule.lhs == grammar.start_symbol]\n",
    "    return parses\n",
    "\n",
    "class Grammar:\n",
    "    def __init__(self, rules=[], annotators=[], start_symbol='$ROOT'):\n",
    "        self.categories = set()\n",
    "        self.lexical_rules = defaultdict(list)\n",
    "        self.unary_rules = defaultdict(list)\n",
    "        self.binary_rules = defaultdict(list)\n",
    "        self.annotators = annotators\n",
    "        self.start_symbol = start_symbol\n",
    "        for rule in rules:\n",
    "            add_rule(self, rule)\n",
    "        print('Created grammar with %d rules.' % len(rules))\n",
    "\n",
    "    def parse_input(self, input):\n",
    "        \"\"\"Returns a list of parses for the given input.\"\"\"\n",
    "        return parse_input(self, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$Number', 16)]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NumberAnnotator().annotate(['16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$Token', 'foo')]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenAnnotator().annotate(['foo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordAnnotator(Annotator):\n",
    "    \"\"\"Let spacy detect stop words for us\"\"\"\n",
    "    def annotate(self, tokens):\n",
    "        if len(tokens) == 1:\n",
    "            if nlp(tokens[0])[0].is_stop:\n",
    "                return [('$StopWord', tokens[0])]\n",
    "        return []\n",
    "\n",
    "class ShowVerbAnnotator(Annotator):\n",
    "    def __init__(self, threshold = 0.7):\n",
    "        self.show_verbs = [(\"define\", \"\"), (\"tell\", \"me\"), (\"show\", \"\")]\n",
    "        self.spacy_show_toks = nlp(\" \".join([verb for verb, _ in self.show_verbs]))\n",
    "        self.threshold = 0.7\n",
    "\n",
    "    def annotate(self, tokens):\n",
    "        if len(tokens) <= 2:\n",
    "            spacy_tokens = nlp(\" \".join(tokens))\n",
    "            spacy_token = spacy_tokens[0]\n",
    "            if spacy_token.pos_ != 'VERB':\n",
    "                return []\n",
    "            \n",
    "            # If the verb matches in meaning and, in case it requires a\n",
    "            # follow-up word, that this matches as well, then it's a match.\n",
    "            for idx, (verb, acc) in enumerate(self.show_verbs):\n",
    "                spacy_verb = self.spacy_show_toks[idx]\n",
    "                if spacy_token.similarity(spacy_verb) >= self.threshold:\n",
    "                    if verb == tokens[0] and acc != \"\" and (len(tokens) == 1 or tokens[1] != acc):\n",
    "                        return []\n",
    "                    return [('$ShowVerb', tokens)]\n",
    "        return []\n",
    "\n",
    "    \n",
    "class TokenAnnotatorBuilder(Annotator):\n",
    "    def __init__(self, category_name, excluded):\n",
    "        Annotator.__init__(self)\n",
    "        self.category_name = category_name\n",
    "        self.excluded = excluded\n",
    "    \n",
    "    def annotate(self, tokens):\n",
    "        if len(tokens) == 1:\n",
    "            token = tokens[0]\n",
    "            if token not in self.excluded:\n",
    "                return [(self.category_name, token)]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$ShowVerb', ['say'])]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowVerbAnnotator().annotate(['say'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$ShowVerb', ['define'])]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowVerbAnnotator().annotate(['define'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('$ShowVerb', ['tell', 'me'])]"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ShowVerbAnnotator().annotate(['tell', 'me'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TokenWithoutQuotes', 'Jeff')]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenAnnotatorBuilder('TokenWithoutQuotes', ['\"', '\"']).annotate(['Jeff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CELL_CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar definition\n",
    "\n",
    "We will model the queries after a few intents:\n",
    "\n",
    "- Definition: asking for a definition of a noun phrase\n",
    "- Comparison: compare two noun phrases\n",
    "- Filtering/Details on a given sense: ask for further details on a previously mentioned sense\n",
    "- Usage of form\n",
    "- General grammar knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def sems_0(sems):\n",
    "    return sems[0]\n",
    "\n",
    "def sems_1(sems):\n",
    "    return sems[1]\n",
    "\n",
    "def merge_dicts(d1, d2):\n",
    "    if not d2:\n",
    "        return d1\n",
    "    if not d1:\n",
    "        return {}\n",
    "    return {**d1, **d2}\n",
    "\n",
    "def strip_none(sems):\n",
    "    return [sem for sem in sems if sem]\n",
    "\n",
    "def merge_dicts_singleparam(sems):\n",
    "    if all([sem is None for sem in sems]):\n",
    "        return {}\n",
    "    return reduce(merge_dicts, strip_none(sems))\n",
    "\n",
    "def to_np(sems):\n",
    "    print(\"Semantics: \", sems)\n",
    "    return {'np': strip_none(sems)[0]}\n",
    "\n",
    "def concatenate(sems):\n",
    "    #print(\"concatenating \", sems)\n",
    "    return \" \".join(strip_none(sems))\n",
    "\n",
    "\n",
    "rules_definition = [\n",
    "    Rule('$ROOT', '$DefinitionQuery', sems_0),\n",
    "    Rule('$DefinitionQuery', '$DefinitionQueryElements',\n",
    "         lambda sems: merge_dicts({'intent': 'definition'}, sems[0])),\n",
    "    Rule('$DefinitionQueryElements', '$DefinitionQuestion $NounPhrase ?$EndOfSentence',\n",
    "         merge_dicts_singleparam),\n",
    "    \n",
    "    Rule('$DefinitionQuestion', '$ShowVerb ?me'),\n",
    "    Rule('$DefinitionQuestion', '$WhatDefinition'),\n",
    "    Rule('$WhatDefinition', 'what is ?$Determiner ?$DefinitionFor'),\n",
    "    Rule('$WhatDefinition', 'how do you $ShowVerb'),\n",
    "    Rule('$DefinitionFor', 'meaning $StopWord'),\n",
    "    Rule('$DefinitionFor', 'sense $StopWord'),\n",
    "    Rule('$DefinitionFor', 'definition $StopWord'),\n",
    "    Rule('$NounPhrase', \"$Tokens\", to_np),\n",
    "    Rule('$NounPhrase', \"' $Tokens '\", to_np),\n",
    "    Rule('$NounPhrase', '\" $Tokens \"', to_np),\n",
    "    Rule('$Tokens', '$UnquotedToken ?$Tokens', concatenate)\n",
    "]\n",
    "\n",
    "rules_end_of_sentence = [\n",
    "    Rule('$EndOfSentence', '?'),\n",
    "    Rule('$EndOfSentence', '.'),\n",
    "    Rule('$EndOfSentence', '!')\n",
    "]\n",
    "\n",
    "rules_determiner = [\n",
    "    Rule('$Determiner', 'a'),\n",
    "    Rule('$Determiner', 'an'),\n",
    "    Rule('$Determiner', 'the'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 20 rules.\n",
      "['define', 'pi']\n",
      "Semantics:  ['define']\n",
      "Semantics:  ['pi']\n",
      "Semantics:  ['define pi']\n"
     ]
    }
   ],
   "source": [
    "annotators = [StopWordAnnotator(), ShowVerbAnnotator(), TokenAnnotatorBuilder(\"$UnquotedToken\", [\"'\", '\"', \"?\"])]\n",
    "rules = rules_definition + rules_end_of_sentence + rules_determiner\n",
    "grammar = Grammar(rules=rules, annotators=annotators)\n",
    "parses = grammar.parse_input('define pi')\n",
    "parse = parses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'definition', 'np': 'pi'}"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['define', '\"', 'pi', '\"']\n",
      "Semantics:  ['define']\n",
      "Semantics:  ['pi']\n",
      "Semantics:  [None, 'pi', None]\n"
     ]
    }
   ],
   "source": [
    "parses = grammar.parse_input('define \"pi\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'definition', 'np': 'pi'}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parses[0].semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " $ROOT ('$DefinitionQuery',)\n",
      "|\n",
      "-- $DefinitionQuery ('$DefinitionQueryElements',)\n",
      "|\n",
      "---- $DefinitionQueryElements ('$DefinitionQuestion', '$NounPhrase')\n",
      "|\n",
      "------ $DefinitionQuestion ('$ShowVerb',)\n",
      "|\n",
      "-------- $ShowVerb ('define',)\n",
      "|\n",
      "------ $NounPhrase ('$\"__nonterminal', '$NounPhrase_$\"__nonterminal')\n",
      "|\n",
      "-------- $\"__nonterminal ('\"',)\n",
      "|\n",
      "-------- $NounPhrase_$\"__nonterminal ('$Tokens', '$\"__nonterminal')\n",
      "|\n",
      "---------- $Tokens ('$UnquotedToken',)\n",
      "|\n",
      "------------ $UnquotedToken ('pi',)\n",
      "|\n",
      "---------- $\"__nonterminal ('\"',)\n"
     ]
    }
   ],
   "source": [
    "def pretty_print(parse, depth=0):\n",
    "    if not isinstance(parse, str):\n",
    "        if depth > 0:\n",
    "            for _ in range(1):\n",
    "                print(\"|\")\n",
    "        print(\"-\" * depth * 2, parse.rule.lhs, parse.rule.rhs)\n",
    "        for child in parse.children:\n",
    "            pretty_print(child, depth+1)\n",
    "\n",
    "pretty_print(parses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['define', \"'\", 'pi', \"'\"]\n",
      "Semantics:  ['define']\n",
      "Semantics:  ['pi']\n",
      "Semantics:  [None, 'pi', None]\n"
     ]
    }
   ],
   "source": [
    "parses = grammar.parse_input(\"define 'pi'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tell', 'me', 'the', 'life']\n",
      "Semantics:  ['tell']\n",
      "Semantics:  ['me']\n",
      "Semantics:  ['tell me']\n",
      "Semantics:  ['the']\n",
      "Semantics:  ['me the']\n",
      "Semantics:  ['tell me the']\n",
      "Semantics:  ['life']\n",
      "Semantics:  ['the life']\n",
      "Semantics:  ['me the life']\n",
      "Semantics:  ['tell me the life']\n"
     ]
    }
   ],
   "source": [
    "parses = grammar.parse_input(\"tell me the life\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'definition', 'np': 'the life'}"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parses[0].semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'an', \"'\", 'apple', \"'\", '?']\n",
      "Semantics:  ['what']\n",
      "Semantics:  ['is']\n",
      "Semantics:  ['what is']\n",
      "Semantics:  ['an']\n",
      "Semantics:  ['is an']\n",
      "Semantics:  ['what is an']\n",
      "Semantics:  ['apple']\n",
      "Semantics:  [None, 'apple', None]\n"
     ]
    }
   ],
   "source": [
    "parses = grammar.parse_input(\"what is an 'apple'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intent': 'definition', 'np': 'apple'}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parses[0].semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'definition', 'of', 'botanics', '?']\n",
      "Semantics:  ['what']\n",
      "Semantics:  ['is']\n",
      "Semantics:  ['what is']\n",
      "Semantics:  ['the']\n",
      "Semantics:  ['is the']\n",
      "Semantics:  ['what is the']\n",
      "Semantics:  ['definition']\n",
      "Semantics:  ['the definition']\n",
      "Semantics:  ['is the definition']\n",
      "Semantics:  ['what is the definition']\n",
      "Semantics:  ['of']\n",
      "Semantics:  ['definition of']\n",
      "Semantics:  ['the definition of']\n",
      "Semantics:  ['is the definition of']\n",
      "Semantics:  ['what is the definition of']\n",
      "Semantics:  ['botanics']\n",
      "Semantics:  ['of botanics']\n",
      "Semantics:  ['definition of botanics']\n",
      "Semantics:  ['the definition of botanics']\n",
      "Semantics:  ['is the definition of botanics']\n",
      "Semantics:  ['what is the definition of botanics']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'intent': 'definition', 'np': 'the definition of botanics'}"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parses = grammar.parse_input(\"what is the definition of botanics?\")\n",
    "parses[0].semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intent': 'definition', 'np': 'the definition of botanics'}\n",
      "{'intent': 'definition', 'np': 'definition of botanics'}\n",
      "{'intent': 'definition', 'np': 'botanics'}\n"
     ]
    }
   ],
   "source": [
    "for parse in parses:\n",
    "    print(parse.semantics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
