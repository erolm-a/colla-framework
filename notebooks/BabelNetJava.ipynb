{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/nfs/knowledge-glue/notebooks/3rdparty/lucene-index/babelnet-api-4.0.1.jar', '/nfs/knowledge-glue/notebooks/3rdparty/lucene-index/lib/*', '/usr/local/lib/python3.7/dist-packages/jnius/src']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "prefix =  \"/nfs/knowledge-glue/notebooks/3rdparty/lucene-index\"\n",
    "scala_path = \"/usr/share/scala-2.11/lib/scala-library.jar\"\n",
    "import jnius_config\n",
    "local_tools = os.getcwd()\n",
    "jnius_config.set_classpath(prefix + \"/babelnet-api-4.0.1.jar\", prefix + \"/lib/*\", local_tools, scala_path )\n",
    "print(jnius_config.get_classpath())\n",
    "import jnius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeff, forgive me for I hath sinned\n",
    "def jimport(path):\n",
    "    \"\"\"\n",
    "    Wrap a jnius import. This adds a global variable\n",
    "    This adds a global variable with the name of the class.\n",
    "    \n",
    "    >>> jimport(\"java.lang.Math\")\n",
    "    >>> Math.sqrt(100)\n",
    "    10.0\n",
    "    \n",
    "    As pyjnius does not seem to import static fields or subclasses\n",
    "    automatically via an 'autoclass' invocation, one needs to\n",
    "    import it manually:\n",
    "    \n",
    "    >>> jimport(\"org.mypackage.myclass\")\n",
    "    >>> jimport(\"org.mypackage.myclass$nestedclass\")\n",
    "    \n",
    "    At the moment it is not allowed to import a nested class alone.\n",
    "    \"\"\"\n",
    "    \n",
    "    classname = path.split(\".\")[-1]\n",
    "    if \"$\" in classname:\n",
    "        parent_class, subclass = classname.split(\"$\")\n",
    "        exec(\"\"\"\n",
    "global ?parent_class\n",
    "?parent_class.?subclass = jnius.autoclass(path)\n",
    "\"\"\".replace(\"?parent_class\", parent_class).replace(\"?subclass\", subclass),\n",
    "             globals(), {'path': path})\n",
    "    else:\n",
    "        exec(\"\"\"\n",
    "global ?classname\n",
    "?classname = jnius.autoclass(path)\n",
    "    \"\"\".replace(\"?classname\", classname), globals(), {'path': path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jimport(\"it.uniroma1.lcl.babelnet.BabelNet\")\n",
    "jimport(\"it.uniroma1.lcl.babelnet.BabelNetQuery\")\n",
    "jimport(\"it.uniroma1.lcl.babelnet.BabelNetQuery$Builder\")\n",
    "jimport(\"it.uniroma1.lcl.babelnet.BabelSynset\")\n",
    "jimport(\"it.uniroma1.lcl.jlt.util.Language\")\n",
    "jimport(\"com.babelscape.util.UniversalPOS\")\n",
    "jimport(\"tools.BabelNetBridge\")\n",
    "jimport(\"tools.BabelNetLexeme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "parquet_pos = os.path.join(os.getcwd(), \"data/wiktionary/parquet-index_2.11-0.4.1-SNAPSHOT.jar\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "                        .config(\"spark.jars\", parquet_pos) \\\n",
    "                        .getOrCreate()\n",
    "\n",
    "spark.sparkContext.addPyFile(parquet_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiktionary_2_wdpg = spark.read.parquet(\"spark-warehouse/wiktionary_2_wdpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.rdf_dumping import ExtractorGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ExtractorGraph()\n",
    "\n",
    "for row in wiktionary_2_wdpg.rdd.toLocalIterator():\n",
    "    if(row['head']['template_name'] != 'head'):\n",
    "        graph.add_wiktionary(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Namespace\n",
    "# TODO: check that the IRI is actually this one\n",
    "bn = Namespace('https://babelnet.org/synset?word=bn:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.g.bind('bn', 'https://babelnet.org/synset?word=bn:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.rdf_dumping import rdf_type, kgl, kgl_prop\n",
    "from tools.strings import strip_prefix\n",
    "import re\n",
    "# Match a sense, in the form of kgl:entity-Sx\n",
    "matcher = re.compile(\"^(.*)-S\\d+\")\n",
    "# for simplicity, a sense only has one gloss\n",
    "sense_names = []\n",
    "sense_glosses = []\n",
    "\n",
    "for (sense, _, sense_description) in graph.g.triples((None, kgl_prop['definition'], None)):\n",
    "    if not matcher.match(sense):\n",
    "        continue\n",
    "    if sense_description.language == \"en\":\n",
    "        sense_names.append(sense.toPython())\n",
    "        sense_glosses.append(sense_description.toPython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is potentially slow, beware!\n",
    "import numpy as np\n",
    "\n",
    "sense_embeddings_path = \"data/wiktionary/senses_encoded.npy\"\n",
    "if os.path.exists(sense_embeddings_path):\n",
    "    sense_embeddings = np.load(sense_embeddings_path)\n",
    "else:\n",
    "    sense_embeddings = model.encode(sense_glosses)\n",
    "    np.save(sense_embeddings_path, sense_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense_gloss_dict = dict(zip(sense_names, zip(sense_glosses, sense_embeddings)))\n",
    "seen = set()\n",
    "\n",
    "from tools.rdf_dumping import rdf_type, kgl, kgl_prop\n",
    "from tools.strings import strip_prefix\n",
    "#import re\n",
    "#matcher = re.compile(\"^(.*)-S\\d+\")\n",
    "related_to = {}\n",
    "\n",
    "\n",
    "def handle_pointer(lexeme, pointer):\n",
    "    pass\n",
    "\n",
    "def query_babelnet(lexeme, word: str, pos: str):\n",
    "    \n",
    "    if (word, pos) in seen:\n",
    "        return\n",
    "    \n",
    "    seen.add((word, pos))\n",
    "    \n",
    "    pos = strip_prefix(\"http://grill-lab.org/kg/entity/\", pos)\n",
    "    synsets = BabelNetBridge.getSynsetsForLexeme(word, pos)\n",
    "    \n",
    "    senses = [triple[2] for triple in graph.g.triples((lexeme, kgl_prop['sense'], None))]\n",
    "    \n",
    "    print(\"For word \", word, pos)\n",
    "\n",
    "    pertaining_senses = [sense_gloss_dict[sense.toPython()] for sense in senses]\n",
    "    print([gloss for gloss, embed in pertaining_senses])\n",
    "    wiktionary_senses_embed = np.array([embed for gloss, embed in pertaining_senses])\n",
    "    \n",
    "    for synset in synsets:\n",
    "        \n",
    "        babelnet_id = synset.id().getID()\n",
    "        glosses = synset.synsets()[0].getGloss()\n",
    "        related = synset.relatedWords()\n",
    "        \n",
    "        # sources = [source.toString() for source in BabelNet.getInstance().getSynset(synset.id()).getSenseSources()]\n",
    "        # print(sources)\n",
    "\n",
    "        print(f\"Determining embeddings for {babelnet_id} ({word})\")\n",
    "        glosses_embedded = np.array(model.encode([glosses]))\n",
    "        glosses_embedded /= np.linalg.norm(glosses_embedded)\n",
    "        \n",
    "        # print(wiktionary_senses_embed.shape)\n",
    "        # print(np.linalg.norm(wiktionary_senses_embed, axis=1).shape)\n",
    "        wiktionary_senses_embed /= np.linalg.norm(wiktionary_senses_embed, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        #print(glosses_embedded.shape)\n",
    "        #print(wiktionary_senses_embed.T.shape)\n",
    "        cosine_similarity = glosses_embedded.flatten() @ wiktionary_senses_embed.T\n",
    "        \n",
    "        # print(cosine_similarity)\n",
    "        \n",
    "        best_match = np.argmax(cosine_similarity)\n",
    "        print(f\"Given this BabelNet entry: {glosses}\")\n",
    "        print(f\"And this Wiktionary gloss: {pertaining_senses[best_match][0]}\")\n",
    "        print(f\"Best match score: {cosine_similarity[best_match]}\")\n",
    "        # TODO estimate ROC curve here\n",
    "        if cosine_similarity[best_match] < 0.53:\n",
    "            print(\"Not related to any. Creating a new synset.\")\n",
    "            sense_id = graph.add_sense(graph.hash(word, pos), lexeme, glosses)\n",
    "        else:\n",
    "            print(\"Related\")\n",
    "            sense_id = senses[best_match]\n",
    "        graph.g.add((sense_id, kgl_prop['synset'], bn[babelnet_id]))\n",
    "        graph.g.add((lexeme, kgl_prop['synset'], bn[babelnet_id]))\n",
    "        \n",
    "        handle_pointer(lexeme, related)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "    \n",
    "for idx, (lexeme, _, word) in enumerate(graph.g.triples((None, kgl_prop[\"label\"], None))):\n",
    "    all_pos = [t[2] for t in graph.g.triples((lexeme, kgl_prop[\"pos\"], None))]\n",
    "    for pos in all_pos:\n",
    "        query_babelnet(lexeme, word.toPython(), pos)\n",
    "    if idx >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
