{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f925bd605b4c24b686388ea0cd88d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1995.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef0db42a84d4912b75ba3221bd287dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=986.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.75 MiB, post-processed: Unknown size, total: 119.27 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875b6fccffce4b8ca69cc93448053834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=8116577.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62e35189f0f40c6ac5228671f1d7dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1054280.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
       " {'answer_start': [188], 'text': ['a copper statue of Christ']},\n",
       " {'answer_start': [279], 'text': ['the Main Building']},\n",
       " {'answer_start': [381], 'text': ['a Marian place of prayer and reflection']},\n",
       " {'answer_start': [92], 'text': ['a golden statue of the Virgin Mary']},\n",
       " {'answer_start': [248], 'text': ['September 1876']},\n",
       " {'answer_start': [441], 'text': ['twice']},\n",
       " {'answer_start': [598], 'text': ['The Observer']},\n",
       " {'answer_start': [126], 'text': ['three']},\n",
       " {'answer_start': [908], 'text': ['1987']},\n",
       " {'answer_start': [119], 'text': ['Rome']},\n",
       " {'answer_start': [145], 'text': ['Moreau Seminary']},\n",
       " {'answer_start': [234], 'text': ['Old College']},\n",
       " {'answer_start': [356], 'text': ['Retired priests and brothers']},\n",
       " {'answer_start': [675], 'text': ['Buechner Prize for Preaching']},\n",
       " {'answer_start': [487], 'text': ['eight']},\n",
       " {'answer_start': [46], 'text': ['1920']},\n",
       " {'answer_start': [126], 'text': ['the College of Science']},\n",
       " {'answer_start': [271], 'text': ['five']},\n",
       " {'answer_start': [155], 'text': ['the 1870s']},\n",
       " {'answer_start': [496], 'text': ['Learning Resource Center']},\n",
       " {'answer_start': [68], 'text': ['five']},\n",
       " {'answer_start': [155], 'text': ['The First Year of Studies program']},\n",
       " {'answer_start': [647], 'text': ['U.S. News & World Report']},\n",
       " {'answer_start': [358], 'text': ['1924']},\n",
       " {'answer_start': [624], 'text': ['Master of Divinity']},\n",
       " {'answer_start': [1163], 'text': ['Alliance for Catholic Education']},\n",
       " {'answer_start': [92], 'text': ['1854']},\n",
       " {'answer_start': [757], 'text': ['Department of Pre-Professional Studies']},\n",
       " {'answer_start': [4],\n",
       "  'text': ['Joan B. Kroc Institute for International Peace Studies']},\n",
       " {'answer_start': [466],\n",
       "  'text': ['President Emeritus of the University of Notre Dame']},\n",
       " {'answer_start': [303], 'text': ['1986']},\n",
       " {'answer_start': [377], 'text': ['Ray Kroc']},\n",
       " {'answer_start': [360], 'text': [\"McDonald's\"]},\n",
       " {'answer_start': [136], 'text': ['14']},\n",
       " {'answer_start': [145], 'text': ['Theodore M. Hesburgh Library']},\n",
       " {'answer_start': [188], 'text': ['1963']},\n",
       " {'answer_start': [344], 'text': ['Millard Sheets']},\n",
       " {'answer_start': [394], 'text': ['Touchdown Jesus']},\n",
       " {'answer_start': [109], 'text': ['3,577']},\n",
       " {'answer_start': [138], 'text': ['19.7%']},\n",
       " {'answer_start': [213], 'text': ['the top 10 to 15 in the nation']},\n",
       " {'answer_start': [488], 'text': ['39.1%']},\n",
       " {'answer_start': [618], 'text': ['more than 750 miles']},\n",
       " {'answer_start': [32], 'text': ['18th overall']},\n",
       " {'answer_start': [362], 'text': ['8th']},\n",
       " {'answer_start': [565], 'text': ['1st overall']},\n",
       " {'answer_start': [155], 'text': ['USA Today']},\n",
       " {'answer_start': [918], 'text': ['57.6%']},\n",
       " {'answer_start': [0], 'text': ['Father Joseph Carrier, C.S.C.']},\n",
       " {'answer_start': [353], 'text': ['1851–1921']},\n",
       " {'answer_start': [406], 'text': ['the Science Department']},\n",
       " {'answer_start': [638], 'text': ['Evolution and Dogma']},\n",
       " {'answer_start': [85], 'text': ['Professor of Chemistry and Physics']},\n",
       " {'answer_start': [3], 'text': ['1882']},\n",
       " {'answer_start': [136], 'text': ['Professor Jerome Green']},\n",
       " {'answer_start': [123], 'text': ['Around 1899']},\n",
       " {'answer_start': [222], 'text': ['Father Julius Nieuwland']},\n",
       " {'answer_start': [49], 'text': ['an early wind tunnel']},\n",
       " {'answer_start': [0], 'text': ['The Lobund Institute']},\n",
       " {'answer_start': [963], 'text': ['the 1940s']},\n",
       " {'answer_start': [1049], 'text': ['1950']},\n",
       " {'answer_start': [1099], 'text': ['1958']},\n",
       " {'answer_start': [86], 'text': ['1928']},\n",
       " {'answer_start': [0], 'text': ['The Review of Politics']},\n",
       " {'answer_start': [68], 'text': ['German Catholic journals']},\n",
       " {'answer_start': [233], 'text': ['44']},\n",
       " {'answer_start': [4], 'text': ['Review of Politics']},\n",
       " {'answer_start': [80], 'text': ['John Jenkins']},\n",
       " {'answer_start': [118], 'text': ['Notre Dame']},\n",
       " {'answer_start': [427], 'text': ['International Peace studies']},\n",
       " {'answer_start': [753], 'text': ['2013']},\n",
       " {'answer_start': [891], 'text': ['climate change']},\n",
       " {'answer_start': [71], 'text': ['8,448']},\n",
       " {'answer_start': [196], 'text': ['21–24%']},\n",
       " {'answer_start': [1446], 'text': ['over 700']},\n",
       " {'answer_start': [1588], 'text': ['the Holy Cross Missions in Bangladesh']},\n",
       " {'answer_start': [49], 'text': ['12,179']},\n",
       " {'answer_start': [6], 'text': ['80%']},\n",
       " {'answer_start': [136], 'text': ['four']},\n",
       " {'answer_start': [350], 'text': ['15']},\n",
       " {'answer_start': [32], 'text': ['20%']},\n",
       " {'answer_start': [368], 'text': ['14']},\n",
       " {'answer_start': [73], 'text': ['Congregatio a Sancta Cruce']},\n",
       " {'answer_start': [197], 'text': ['more than 93%']},\n",
       " {'answer_start': [331], 'text': ['over 100 times']},\n",
       " {'answer_start': [1237], 'text': ['Fifty-seven']},\n",
       " {'answer_start': [251], 'text': ['over 80%']},\n",
       " {'answer_start': [702], 'text': ['Washington Hall']},\n",
       " {'answer_start': [90], 'text': ['1879']},\n",
       " {'answer_start': [228], 'text': ['Rev. William Corby']},\n",
       " {'answer_start': [385], 'text': ['17th of May']},\n",
       " {'answer_start': [862], 'text': ['LaFortune Student Center']},\n",
       " {'answer_start': [244], 'text': ['scholastic and classical']},\n",
       " {'answer_start': [595], 'text': ['College of Commerce']},\n",
       " {'answer_start': [8], 'text': ['Father James Burns']},\n",
       " {'answer_start': [66], 'text': ['three years']},\n",
       " {'answer_start': [430], 'text': ['Harvard Law School']},\n",
       " {'answer_start': [117], 'text': ['Knute Rockne']},\n",
       " {'answer_start': [204], 'text': ['105']},\n",
       " {'answer_start': [354], 'text': ['1925']},\n",
       " {'answer_start': [251], 'text': ['13']},\n",
       " {'answer_start': [274], 'text': ['three']},\n",
       " {'answer_start': [297], 'text': ['the Protestant establishment']},\n",
       " {'answer_start': [571], 'text': ['the Ku Klux Klan']},\n",
       " {'answer_start': [1193], 'text': ['Fr. Matthew Walsh']},\n",
       " {'answer_start': [819], 'text': ['a week-long Klavern']},\n",
       " {'answer_start': [842], 'text': ['South Bend']},\n",
       " {'answer_start': [11], 'text': [\"Father John Francis O'Hara\"]},\n",
       " {'answer_start': [11], 'text': [\"Father John Francis O'Hara\"]},\n",
       " {'answer_start': [292], 'text': ['Laetare Medal']},\n",
       " {'answer_start': [321], 'text': ['1883']},\n",
       " {'answer_start': [587], 'text': ['God']},\n",
       " {'answer_start': [428], 'text': ['more than half']},\n",
       " {'answer_start': [522], 'text': ['Lobund Institute for Animal Studies']},\n",
       " {'answer_start': [720], 'text': ['Hall of Liberal Arts']},\n",
       " {'answer_start': [4], 'text': ['Rev. John J. Cavanaugh, C.S.C.']},\n",
       " {'answer_start': [575], 'text': ['Medieval Institute']},\n",
       " {'answer_start': [37], 'text': ['1917–2015']},\n",
       " {'answer_start': [181], 'text': ['18']},\n",
       " {'answer_start': [262], 'text': ['$9 million']},\n",
       " {'answer_start': [82], 'text': ['1952–87']},\n",
       " {'answer_start': [439], 'text': ['950']},\n",
       " {'answer_start': [82], 'text': ['coeducational']},\n",
       " {'answer_start': [625], 'text': ['Dean of Arts and Letters']},\n",
       " {'answer_start': [921], 'text': ['Vice President of Student Affairs']},\n",
       " {'answer_start': [1199], 'text': ['1971']},\n",
       " {'answer_start': [141], 'text': [\"Saint Mary's College\"]},\n",
       " {'answer_start': [64], 'text': ['1987–2005']},\n",
       " {'answer_start': [314], 'text': ['1240']},\n",
       " {'answer_start': [403], 'text': ['$350 million']},\n",
       " {'answer_start': [576], 'text': ['more than $70 million']},\n",
       " {'answer_start': [191], 'text': ['500']},\n",
       " {'answer_start': [6], 'text': ['2005']},\n",
       " {'answer_start': [68], 'text': ['17th']},\n",
       " {'answer_start': [138], 'text': ['Malloy']},\n",
       " {'answer_start': [488], 'text': ['Compton Family Ice Arena']},\n",
       " {'answer_start': [596], 'text': ['$400m']},\n",
       " {'answer_start': [162], 'text': ['Congregation of Holy Cross']},\n",
       " {'answer_start': [202], 'text': ['Basilica of the Sacred Heart']},\n",
       " {'answer_start': [349], 'text': ['French Revival']},\n",
       " {'answer_start': [474], 'text': ['Luigi Gregori']},\n",
       " {'answer_start': [730], 'text': ['1896']},\n",
       " {'answer_start': [56], 'text': ['Fr. Zahm']},\n",
       " {'answer_start': [73], 'text': ['1950']},\n",
       " {'answer_start': [157], 'text': ['Joseph LaFortune']},\n",
       " {'answer_start': [284], 'text': ['83,000 square feet']},\n",
       " {'answer_start': [535], 'text': ['$1.2 million']},\n",
       " {'answer_start': [120], 'text': ['29']},\n",
       " {'answer_start': [336], 'text': ['Theodore Hesburgh Library']},\n",
       " {'answer_start': [398], 'text': ['almost 4 million']},\n",
       " {'answer_start': [613], 'text': ['Duncan Hall']},\n",
       " {'answer_start': [1755], 'text': ['Frank Eck Stadium']},\n",
       " {'answer_start': [142], 'text': ['2008']},\n",
       " {'answer_start': [471], 'text': ['40%']},\n",
       " {'answer_start': [596], 'text': ['Sustainable Endowments Institute']},\n",
       " {'answer_start': [750],\n",
       "  'text': ['Kroc Institute for International Peace Studies']},\n",
       " {'answer_start': [198], 'text': ['1968']},\n",
       " {'answer_start': [289], 'text': ['1 Suffolk Street in Trafalgar Square']},\n",
       " {'answer_start': [535], 'text': ['Global Gateways']},\n",
       " {'answer_start': [210], 'text': ['1998']},\n",
       " {'answer_start': [0], 'text': ['The College of Arts and Letters']},\n",
       " {'answer_start': [85], 'text': ['1842']},\n",
       " {'answer_start': [122], 'text': ['1849']},\n",
       " {'answer_start': [221], 'text': ['Saint Louis University']},\n",
       " {'answer_start': [424], 'text': ['33']},\n",
       " {'answer_start': [78], 'text': ['Father Patrick Dillon']},\n",
       " {'answer_start': [60], 'text': ['1865']},\n",
       " {'answer_start': [134], 'text': ['six years']},\n",
       " {'answer_start': [242], 'text': ['Jordan Hall of Science']},\n",
       " {'answer_start': [275], 'text': ['over 1,200']},\n",
       " {'answer_start': [4], 'text': ['School of Architecture']},\n",
       " {'answer_start': [159], 'text': ['Bond Hall']},\n",
       " {'answer_start': [179], 'text': ['five-year']},\n",
       " {'answer_start': [325], 'text': ['Rome']},\n",
       " {'answer_start': [624], 'text': ['Driehaus Architecture Prize']},\n",
       " {'answer_start': [388], 'text': ['2015']},\n",
       " {'answer_start': [405], 'text': ['the first floor of Stanford Hall']},\n",
       " {'answer_start': [538], 'text': ['over three million volumes']},\n",
       " {'answer_start': [654], 'text': ['one of the 100 largest']},\n",
       " {'answer_start': [0], 'text': ['The rise of Hitler and other dictators']},\n",
       " {'answer_start': [162], 'text': ['Germany']},\n",
       " {'answer_start': [212], 'text': ['classics and law']},\n",
       " {'answer_start': [478], 'text': ['Max Scheler']},\n",
       " {'answer_start': [519], 'text': ['a renowned sculptor']},\n",
       " {'answer_start': [4], 'text': ['University of Notre Dame du']},\n",
       " {'answer_start': [92], 'text': ['Catholic research university']},\n",
       " {'answer_start': [220], 'text': ['Our Lady of the Lake']},\n",
       " {'answer_start': [287], 'text': ['the Virgin Mary']},\n",
       " {'answer_start': [327], 'text': ['1,250']},\n",
       " {'answer_start': [62], 'text': ['its Fighting Irish football team']},\n",
       " {'answer_start': [149], 'text': ['Knute Rockne']},\n",
       " {'answer_start': [214], 'text': ['NCAA Division I']},\n",
       " {'answer_start': [372], 'text': ['seven']},\n",
       " {'answer_start': [454], 'text': ['13']},\n",
       " {'answer_start': [140], 'text': ['among the top twenty']},\n",
       " {'answer_start': [294], 'text': ['four']},\n",
       " {'answer_start': [494], 'text': ['Driehaus Architecture Prize']},\n",
       " {'answer_start': [557], 'text': ['more than 50']},\n",
       " {'answer_start': [887], 'text': ['Snite Museum of Art']},\n",
       " {'answer_start': [3], 'text': ['1842']},\n",
       " {'answer_start': [34], 'text': ['Célestine Guynemer de la Hailandière']},\n",
       " {'answer_start': [111], 'text': ['the Congregation of the Holy Cross']},\n",
       " {'answer_start': [290], 'text': ['November 26, 1842']},\n",
       " {'answer_start': [336], 'text': [\"Father Stephen Badin's old log chapel\"]},\n",
       " {'answer_start': [51], 'text': ['1849']},\n",
       " {'answer_start': [359], 'text': ['1865']},\n",
       " {'answer_start': [495], 'text': ['Father Lemonnier']},\n",
       " {'answer_start': [516], 'text': ['1879']},\n",
       " {'answer_start': [453], 'text': ['1873']},\n",
       " {'answer_start': [24], 'text': ['NDtv']},\n",
       " {'answer_start': [40], 'text': ['one show']},\n",
       " {'answer_start': [128], 'text': ['WSND-FM']},\n",
       " {'answer_start': [440], 'text': ['WVFI']},\n",
       " {'answer_start': [42], 'text': ['$215 million']},\n",
       " {'answer_start': [169], 'text': ['June 3, 2008']},\n",
       " {'answer_start': [401], 'text': ['Kite Realty']},\n",
       " {'answer_start': [249], 'text': ['the City of South Bend']},\n",
       " {'answer_start': [367], 'text': ['non-union workers']},\n",
       " {'answer_start': [82], 'text': ['National Collegiate Athletic Association']},\n",
       " {'answer_start': [293], 'text': ['Horizon League']},\n",
       " {'answer_start': [889], 'text': ['Midwest Fencing Conference']},\n",
       " {'answer_start': [959], 'text': ['Hockey East']},\n",
       " {'answer_start': [384], 'text': ['Big East Conference']},\n",
       " {'answer_start': [239], 'text': ['the ACC']},\n",
       " {'answer_start': [382], 'text': ['five']},\n",
       " {'answer_start': [674], 'text': ['Central Collegiate Hockey Association']},\n",
       " {'answer_start': [1223], 'text': ['Navy Blue and Gold Rush']},\n",
       " {'answer_start': [1398], 'text': ['Leprechaun']},\n",
       " {'answer_start': [50], 'text': ['Under Armour']},\n",
       " {'answer_start': [223], 'text': ['almost $100 million']},\n",
       " {'answer_start': [392], 'text': ['1846']},\n",
       " {'answer_start': [420],\n",
       "  'text': ['oldest university band in continuous existence in the United States']},\n",
       " {'answer_start': [657], 'text': ['Notre Dame Victory March']},\n",
       " {'answer_start': [74], 'text': ['Michigan Wolverines football team']},\n",
       " {'answer_start': [142], 'text': ['1887']},\n",
       " {'answer_start': [509], 'text': ['Ohio State University']},\n",
       " {'answer_start': [715], 'text': ['USC']},\n",
       " {'answer_start': [441], 'text': ['the most']},\n",
       " {'answer_start': [0], 'text': ['George Gipp']},\n",
       " {'answer_start': [362], 'text': ['the Army team']},\n",
       " {'answer_start': [457], 'text': [\"Pat O'Brien\"]},\n",
       " {'answer_start': [506], 'text': ['Gipp']},\n",
       " {'answer_start': [562], 'text': ['80,795']},\n",
       " {'answer_start': [166], 'text': ['two-story banner']},\n",
       " {'answer_start': [245], 'text': [\"the Drummers' Circle\"]},\n",
       " {'answer_start': [594], 'text': ['the steps of Bond Hall']},\n",
       " {'answer_start': [480],\n",
       "  'text': ['the Notre Dame Victory March and the Notre Dame Alma Mater']},\n",
       " {'answer_start': [424], 'text': ['Saturday']},\n",
       " {'answer_start': [30], 'text': ['over 1,600']},\n",
       " {'answer_start': [59], 'text': ['12']},\n",
       " {'answer_start': [119], 'text': ['28']},\n",
       " {'answer_start': [154], 'text': ['Austin Carr']},\n",
       " {'answer_start': [850], 'text': ['Mike Brey']},\n",
       " {'answer_start': [222], 'text': ['John F. Shea']},\n",
       " {'answer_start': [173], 'text': ['1904']},\n",
       " {'answer_start': [149], 'text': ['Rev. Michael J. Shea']},\n",
       " {'answer_start': [411], 'text': ['1928']},\n",
       " {'answer_start': [677], 'text': ['onward to victory']},\n",
       " {'answer_start': [267], 'text': ['The Gipper']},\n",
       " {'answer_start': [344], 'text': ['Airplane!']},\n",
       " {'answer_start': [513], 'text': ['Sean Astin']},\n",
       " {'answer_start': [410], 'text': ['George Zipp']},\n",
       " {'answer_start': [40], 'text': ['Knute Rockne']},\n",
       " {'answer_start': [185], 'text': ['Condoleezza Rice']},\n",
       " {'answer_start': [278], 'text': ['Eric F. Wieschaus']},\n",
       " {'answer_start': [384], 'text': ['Rev. John Jenkins']},\n",
       " {'answer_start': [937], 'text': ['Olympic gold']},\n",
       " {'answer_start': [1232], 'text': ['Jim Wetherbee']},\n",
       " {'answer_start': [269], 'text': ['in the late 1990s']},\n",
       " {'answer_start': [207], 'text': ['singing and dancing']},\n",
       " {'answer_start': [526], 'text': ['2003']},\n",
       " {'answer_start': [166], 'text': ['Houston, Texas']},\n",
       " {'answer_start': [276], 'text': ['late 1990s']},\n",
       " {'answer_start': [320], 'text': [\"Destiny's Child\"]},\n",
       " {'answer_start': [505], 'text': ['Dangerously in Love']},\n",
       " {'answer_start': [360], 'text': ['Mathew Knowles']},\n",
       " {'answer_start': [166], 'text': ['Houston']},\n",
       " {'answer_start': [505], 'text': ['Dangerously in Love']},\n",
       " {'answer_start': [64], 'text': ['September 4, 1981']},\n",
       " {'answer_start': [0], 'text': ['Beyoncé Giselle Knowles-Carter']},\n",
       " {'answer_start': [276], 'text': ['late 1990s']},\n",
       " {'answer_start': [290], 'text': ['lead singer']},\n",
       " {'answer_start': [505], 'text': ['Dangerously in Love']},\n",
       " {'answer_start': [526], 'text': ['2003']},\n",
       " {'answer_start': [590], 'text': ['five']},\n",
       " {'answer_start': [290], 'text': ['lead singer']},\n",
       " {'answer_start': [505], 'text': ['Dangerously in Love']},\n",
       " {'answer_start': [526], 'text': ['2003']},\n",
       " {'answer_start': [207], 'text': ['acting']},\n",
       " {'answer_start': [369], 'text': ['Jay Z']},\n",
       " {'answer_start': [565], 'text': ['six']},\n",
       " {'answer_start': [260], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [586], 'text': ['2010']},\n",
       " {'answer_start': [180], 'text': ['Beyoncé']},\n",
       " {'answer_start': [406], 'text': ['Cadillac Records']},\n",
       " {'answer_start': [48], 'text': ['June 2005']},\n",
       " {'answer_start': [95], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [260], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [369], 'text': ['Jay Z']},\n",
       " {'answer_start': [466], 'text': ['Sasha Fierce']},\n",
       " {'answer_start': [104], 'text': ['love, relationships, and monogamy']},\n",
       " {'answer_start': [935], 'text': ['influential']},\n",
       " {'answer_start': [985], 'text': ['Forbes']},\n",
       " {'answer_start': [736], 'text': ['2000s']},\n",
       " {'answer_start': [985], 'text': ['Forbes']},\n",
       " {'answer_start': [18], 'text': ['modern-day feminist']},\n",
       " {'answer_start': [970], 'text': ['2013 and 2014']},\n",
       " {'answer_start': [393], 'text': ['118 million']},\n",
       " {'answer_start': [445], 'text': ['60 million']},\n",
       " {'answer_start': [393], 'text': ['118 million']},\n",
       " {'answer_start': [552], 'text': ['20']},\n",
       " {'answer_start': [985], 'text': ['Forbes']},\n",
       " {'answer_start': [303], 'text': [\"Destiny's Child\"]},\n",
       " {'answer_start': [204], 'text': [\"her mother's maiden name\"]},\n",
       " {'answer_start': [330], 'text': ['African-American']},\n",
       " {'answer_start': [578], 'text': ['Methodist']},\n",
       " {'answer_start': [152], 'text': ['Xerox']},\n",
       " {'answer_start': [101], 'text': ['hairdresser and salon owner']},\n",
       " {'answer_start': [255], 'text': ['Solange']},\n",
       " {'answer_start': [540], 'text': ['Joseph Broussard']},\n",
       " {'answer_start': [152], 'text': ['Xerox']},\n",
       " {'answer_start': [117], 'text': ['salon']},\n",
       " {'answer_start': [255], 'text': ['Solange']},\n",
       " {'answer_start': [540], 'text': ['Joseph Broussard.']},\n",
       " {'answer_start': [578], 'text': ['Methodist']},\n",
       " {'answer_start': [49], 'text': ['Fredericksburg']},\n",
       " {'answer_start': [165], 'text': ['Darlette Johnson']},\n",
       " {'answer_start': [507], 'text': ['Houston']},\n",
       " {'answer_start': [148], 'text': ['dance instructor Darlette Johnson']},\n",
       " {'answer_start': [711], 'text': [\"St. John's United Methodist Church\"]},\n",
       " {'answer_start': [484], 'text': ['music magnet school']},\n",
       " {'answer_start': [385], 'text': ['Imagine']},\n",
       " {'answer_start': [49], 'text': ['Fredericksburg']},\n",
       " {'answer_start': [165], 'text': ['Darlette Johnson']},\n",
       " {'answer_start': [355], 'text': ['seven']},\n",
       " {'answer_start': [711], 'text': [\"St. John's United Methodist Church\"]},\n",
       " {'answer_start': [303], 'text': ['Arne Frager']},\n",
       " {'answer_start': [542], 'text': [\"Beyoncé's father\"]},\n",
       " {'answer_start': [918], 'text': ['Elektra Records']},\n",
       " {'answer_start': [303], 'text': ['Arne Frager']},\n",
       " {'answer_start': [537], 'text': ['1995']},\n",
       " {'answer_start': [1264], 'text': ['Sony Music']},\n",
       " {'answer_start': [918], 'text': ['Elektra Records']},\n",
       " {'answer_start': [3], 'text': ['age eight']},\n",
       " {'answer_start': [7], 'text': ['eight']},\n",
       " {'answer_start': [192], 'text': [\"Girl's Tyme\"]},\n",
       " {'answer_start': [303], 'text': ['Arne Frager']},\n",
       " {'answer_start': [537], 'text': ['1995']},\n",
       " {'answer_start': [1126],\n",
       "  'text': [\"Dwayne Wiggins's Grass Roots Entertainment\"]},\n",
       " {'answer_start': [215], 'text': ['Men in Black']},\n",
       " {'answer_start': [848], 'text': ['\"Say My Name\"']},\n",
       " {'answer_start': [1212], 'text': ['Marc Nelson']},\n",
       " {'answer_start': [51], 'text': ['1996']},\n",
       " {'answer_start': [85], 'text': ['Book of Isaiah']},\n",
       " {'answer_start': [215], 'text': ['Men in Black']},\n",
       " {'answer_start': [849], 'text': ['Say My Name']},\n",
       " {'answer_start': [1212], 'text': ['Marc Nelson']},\n",
       " {'answer_start': [85], 'text': ['Book of Isaiah.']},\n",
       " {'answer_start': [215], 'text': ['Men in Black.']},\n",
       " {'answer_start': [330], 'text': ['No, No, No']},\n",
       " {'answer_start': [688], 'text': ['1999']},\n",
       " {'answer_start': [1212], 'text': ['Marc Nelson']},\n",
       " {'answer_start': [169], 'text': ['depression']},\n",
       " {'answer_start': [320], 'text': ['boyfriend left her']},\n",
       " {'answer_start': [714], 'text': ['her mother']},\n",
       " {'answer_start': [194], 'text': ['split with Luckett and Rober']},\n",
       " {'answer_start': [396], 'text': ['a couple of years']},\n",
       " {'answer_start': [714], 'text': ['her mother']},\n",
       " {'answer_start': [110], 'text': ['Farrah Franklin and Michelle Williams.']},\n",
       " {'answer_start': [149], 'text': ['Beyoncé']},\n",
       " {'answer_start': [714], 'text': ['her mother']},\n",
       " {'answer_start': [110], 'text': ['Farrah Franklin']},\n",
       " {'answer_start': [37], 'text': ['Independent Women Part I']},\n",
       " {'answer_start': [216], 'text': ['eleven']},\n",
       " {'answer_start': [348], 'text': ['MTV']},\n",
       " {'answer_start': [793], 'text': ['663,000 copies']},\n",
       " {'answer_start': [557], 'text': ['Georges Bizet']},\n",
       " {'answer_start': [593], 'text': ['Survivor']},\n",
       " {'answer_start': [115], 'text': [\"Charlie's Angels.\"]},\n",
       " {'answer_start': [378], 'text': ['Carmen: A Hip Hopera']},\n",
       " {'answer_start': [593], 'text': ['Survivor']},\n",
       " {'answer_start': [628], 'text': ['Luckett and Roberson']},\n",
       " {'answer_start': [1070], 'text': ['October 2001']},\n",
       " {'answer_start': [84], 'text': ['Mike Myers']},\n",
       " {'answer_start': [331], 'text': ['UK, Norway, and Belgium']},\n",
       " {'answer_start': [431], 'text': ['The Fighting Temptations']},\n",
       " {'answer_start': [705], 'text': ['Missy Elliott']},\n",
       " {'answer_start': [834], 'text': ['Summertime']},\n",
       " {'answer_start': [115], 'text': ['Austin Powers in Goldmember']},\n",
       " {'answer_start': [210], 'text': ['73 million']},\n",
       " {'answer_start': [416], 'text': ['musical comedy']},\n",
       " {'answer_start': [435], 'text': ['Fighting Temptations']},\n",
       " {'answer_start': [545], 'text': ['mixed reviews']},\n",
       " {'answer_start': [115], 'text': ['Austin Powers in Goldmember']},\n",
       " {'answer_start': [58], 'text': ['Foxxy Cleopatra']},\n",
       " {'answer_start': [240], 'text': ['Work It Out']},\n",
       " {'answer_start': [431], 'text': ['The Fighting Temptations']},\n",
       " {'answer_start': [435], 'text': ['Fighting Temptations']},\n",
       " {'answer_start': [123], 'text': ['number four']},\n",
       " {'answer_start': [193], 'text': ['Dangerously in Love']},\n",
       " {'answer_start': [419], 'text': ['11 million']},\n",
       " {'answer_start': [474], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [130], 'text': ['four']},\n",
       " {'answer_start': [48], 'text': ['Jay Z']},\n",
       " {'answer_start': [193], 'text': ['Dangerously in Love']},\n",
       " {'answer_start': [123], 'text': ['number four']},\n",
       " {'answer_start': [1042], 'text': ['Luther Vandross']},\n",
       " {'answer_start': [48], 'text': ['Jay Z']},\n",
       " {'answer_start': [229], 'text': ['June 24, 2003']},\n",
       " {'answer_start': [474], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [1042], 'text': ['Luther Vandross.']},\n",
       " {'answer_start': [696], 'text': ['five.']},\n",
       " {'answer_start': [513], 'text': ['Destiny Fulfilled']},\n",
       " {'answer_start': [1212], 'text': ['2006']},\n",
       " {'answer_start': [3], 'text': ['November 2003']},\n",
       " {'answer_start': [664], 'text': ['Destiny Fulfilled']},\n",
       " {'answer_start': [935], 'text': ['Barcelona']},\n",
       " {'answer_start': [1206], 'text': ['March 2006']},\n",
       " {'answer_start': [38], 'text': ['Dangerously in Love Tour']},\n",
       " {'answer_start': [100], 'text': ['Missy Elliott and Alicia Keys']},\n",
       " {'answer_start': [253], 'text': ['Super Bowl XXXVIII']},\n",
       " {'answer_start': [848], 'text': ['Destiny Fulfilled.']},\n",
       " {'answer_start': [132], 'text': ['541,000']},\n",
       " {'answer_start': [303], 'text': ['Déjà Vu']},\n",
       " {'answer_start': [346], 'text': ['five']},\n",
       " {'answer_start': [346], 'text': ['five']},\n",
       " {'answer_start': [101], 'text': ['twenty-fifth birthday']},\n",
       " {'answer_start': [323], 'text': ['Jay Z']},\n",
       " {'answer_start': [342], 'text': ['top five']},\n",
       " {'answer_start': [28], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [132], 'text': ['541,000']},\n",
       " {'answer_start': [323], 'text': ['Jay Z']},\n",
       " {'answer_start': [635], 'text': ['Green Light']},\n",
       " {'answer_start': [53], 'text': ['The Pink Panther']},\n",
       " {'answer_start': [171], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [171], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [550], 'text': ['2007']},\n",
       " {'answer_start': [671], 'text': ['24 million']},\n",
       " {'answer_start': [112], 'text': ['158.8 million']},\n",
       " {'answer_start': [576], 'text': ['The Beyoncé Experience']},\n",
       " {'answer_start': [932], 'text': ['Shakira']},\n",
       " {'answer_start': [53], 'text': ['The Pink Panther']},\n",
       " {'answer_start': [436], 'text': ['Diana Ross.']},\n",
       " {'answer_start': [487], 'text': ['Listen']},\n",
       " {'answer_start': [576], 'text': ['The Beyoncé Experience']},\n",
       " {'answer_start': [932], 'text': ['Shakira']},\n",
       " {'answer_start': [34], 'text': ['Jay Z']},\n",
       " {'answer_start': [253], 'text': ['November 18, 2008']},\n",
       " {'answer_start': [897], 'text': ['2000s']},\n",
       " {'answer_start': [1567], 'text': ['Taylor Swift']},\n",
       " {'answer_start': [1881], 'text': ['119.5 million']},\n",
       " {'answer_start': [78], 'text': ['in a video montage']},\n",
       " {'answer_start': [1744], 'text': ['March 2009']},\n",
       " {'answer_start': [1567], 'text': ['Taylor Swift']},\n",
       " {'answer_start': [1881], 'text': ['119.5 million']},\n",
       " {'answer_start': [3], 'text': ['April 4, 2008']},\n",
       " {'answer_start': [34], 'text': ['Jay Z.']},\n",
       " {'answer_start': [156], 'text': ['Sasha Fierce']},\n",
       " {'answer_start': [605], 'text': ['Single Ladies']},\n",
       " {'answer_start': [1611], 'text': ['Kanye West']},\n",
       " {'answer_start': [69], 'text': ['Etta James']},\n",
       " {'answer_start': [439], 'text': ['Phoenix House']},\n",
       " {'answer_start': [582], 'text': ['At Last']},\n",
       " {'answer_start': [693], 'text': ['thriller']},\n",
       " {'answer_start': [1101], 'text': ['MTV Movie Award for Best Fight']},\n",
       " {'answer_start': [439], 'text': ['Phoenix House']},\n",
       " {'answer_start': [703], 'text': ['Obsessed']},\n",
       " {'answer_start': [724], 'text': ['Sharon Charles']},\n",
       " {'answer_start': [940], 'text': ['60 million']},\n",
       " {'answer_start': [69], 'text': ['Etta James']},\n",
       " {'answer_start': [439], 'text': ['Phoenix House']},\n",
       " {'answer_start': [594], 'text': [\"the First Couple's first inaugural ball.\"]},\n",
       " {'answer_start': [703], 'text': ['Obsessed.']},\n",
       " {'answer_start': [51], 'text': ['ten']},\n",
       " {'answer_start': [242], 'text': ['Lauryn Hill']},\n",
       " {'answer_start': [352], 'text': ['Lady Gaga']},\n",
       " {'answer_start': [457], 'text': ['six']},\n",
       " {'answer_start': [517], 'text': ['Mariah Carey']},\n",
       " {'answer_start': [51], 'text': ['ten nominations']},\n",
       " {'answer_start': [372], 'text': ['Telephone']},\n",
       " {'answer_start': [352], 'text': ['Lady Gaga']},\n",
       " {'answer_start': [517], 'text': ['Mariah Carey']},\n",
       " {'answer_start': [242], 'text': ['Lauryn Hill']},\n",
       " {'answer_start': [51], 'text': ['ten']},\n",
       " {'answer_start': [242], 'text': ['Lauryn Hill']},\n",
       " {'answer_start': [352], 'text': ['Lady Gaga']},\n",
       " {'answer_start': [517], 'text': ['Mariah Carey']},\n",
       " {'answer_start': [60], 'text': ['2010']},\n",
       " {'answer_start': [60], 'text': ['2010']},\n",
       " {'answer_start': [300], 'text': ['the Great Wall of China']},\n",
       " {'answer_start': [60], 'text': ['2010']},\n",
       " {'answer_start': [74], 'text': ['her mother']},\n",
       " {'answer_start': [143], 'text': ['During the break']},\n",
       " {'answer_start': [244], 'text': ['nine months']},\n",
       " {'answer_start': [18], 'text': ['a hiatus']},\n",
       " {'answer_start': [74], 'text': ['her mother']},\n",
       " {'answer_start': [168], 'text': ['her father']},\n",
       " {'answer_start': [244], 'text': ['nine months']},\n",
       " {'answer_start': [3], 'text': ['2011']},\n",
       " {'answer_start': [367], 'text': ['Clinton Bush Haiti Fund']},\n",
       " {'answer_start': [486], 'text': ['the 2011 Glastonbury Festival']},\n",
       " {'answer_start': [313], 'text': ['The Huffington Post']},\n",
       " {'answer_start': [596], 'text': ['minute']},\n",
       " {'answer_start': [9], 'text': ['documents obtained by WikiLeaks']},\n",
       " {'answer_start': [3], 'text': ['2011']},\n",
       " {'answer_start': [313], 'text': ['The Huffington Post']},\n",
       " {'answer_start': [495], 'text': ['Glastonbury Festival']},\n",
       " {'answer_start': [137], 'text': ['Muammar Gaddafi.']},\n",
       " {'answer_start': [31], 'text': ['WikiLeaks']},\n",
       " {'answer_start': [367], 'text': ['Clinton Bush Haiti Fund.']},\n",
       " {'answer_start': [469], 'text': ['Pyramid stage']},\n",
       " {'answer_start': [51], 'text': ['2011']},\n",
       " {'answer_start': [371], 'text': ['Love on Top']},\n",
       " {'answer_start': [617], 'text': ['writing']},\n",
       " {'answer_start': [719], 'text': [\"New York's Roseland Ballroom\"]},\n",
       " {'answer_start': [42], 'text': ['June 28, 2011']},\n",
       " {'answer_start': [74], 'text': ['310,000 copies']},\n",
       " {'answer_start': [640],\n",
       "  'text': ['New York Association of Black Journalists']},\n",
       " {'answer_start': [51], 'text': ['2011']},\n",
       " {'answer_start': [24], 'text': ['4']},\n",
       " {'answer_start': [42], 'text': ['June 28, 2011']},\n",
       " {'answer_start': [74], 'text': ['310,000']},\n",
       " {'answer_start': [562], 'text': ['Essence']},\n",
       " {'answer_start': [719], 'text': [\"New York's Roseland Ballroom\"]},\n",
       " {'answer_start': [3], 'text': ['January 7, 2012']},\n",
       " {'answer_start': [91], 'text': ['Lenox Hill Hospital']},\n",
       " {'answer_start': [71], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [176], 'text': [\"Revel Atlantic City's Ovation Hall\"]},\n",
       " {'answer_start': [3], 'text': ['January 7, 2012']},\n",
       " {'answer_start': [71], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [124], 'text': ['Five months']},\n",
       " {'answer_start': [161], 'text': ['four nights']},\n",
       " {'answer_start': [3], 'text': ['January 7, 2012']},\n",
       " {'answer_start': [71], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [91], 'text': ['Lenox Hill Hospital in New York.']},\n",
       " {'answer_start': [176], 'text': [\"Revel Atlantic City's Ovation Hall\"]},\n",
       " {'answer_start': [161], 'text': ['four']},\n",
       " {'answer_start': [81], 'text': ['romance']},\n",
       " {'answer_start': [689], 'text': ['Life Is But a Dream']},\n",
       " {'answer_start': [1163], 'text': ['global publishing agreement']},\n",
       " {'answer_start': [3], 'text': ['January 2013']},\n",
       " {'answer_start': [158], 'text': ['Nuclear']},\n",
       " {'answer_start': [258], 'text': ['President Obama']},\n",
       " {'answer_start': [523], 'text': ['268,000 tweets per minute']},\n",
       " {'answer_start': [3], 'text': ['January 2013']},\n",
       " {'answer_start': [158], 'text': ['Nuclear']},\n",
       " {'answer_start': [186], 'text': ['the American national anthem']},\n",
       " {'answer_start': [362], 'text': ['Super Bowl XLVII halftime show']},\n",
       " {'answer_start': [689], 'text': ['Life Is But a Dream']},\n",
       " {'answer_start': [103], 'text': ['132']},\n",
       " {'answer_start': [20], 'text': ['The Mrs. Carter Show']},\n",
       " {'answer_start': [560], 'text': ['Rise Up']},\n",
       " {'answer_start': [469], 'text': ['Epic']},\n",
       " {'answer_start': [55], 'text': ['April 15']},\n",
       " {'answer_start': [399], 'text': ['2013 Met Gala']},\n",
       " {'answer_start': [429], 'text': ['Queen Tara']},\n",
       " {'answer_start': [560], 'text': ['Rise Up']},\n",
       " {'answer_start': [20], 'text': ['The Mrs. Carter Show World Tour']},\n",
       " {'answer_start': [103], 'text': ['132']},\n",
       " {'answer_start': [288], 'text': ['Back to Black']},\n",
       " {'answer_start': [404], 'text': ['Met Gala.']},\n",
       " {'answer_start': [429], 'text': ['Queen Tara']},\n",
       " {'answer_start': [88], 'text': ['the iTunes Store']},\n",
       " {'answer_start': [3], 'text': ['December 13, 2013']},\n",
       " {'answer_start': [88], 'text': ['the iTunes Store']},\n",
       " {'answer_start': [810], 'text': ['Jay Z']},\n",
       " {'answer_start': [1344], 'text': ['Forbes']},\n",
       " {'answer_start': [1471], 'text': ['more than double her earnings']},\n",
       " {'answer_start': [3], 'text': ['December 13, 2013']},\n",
       " {'answer_start': [440], 'text': ['one million']},\n",
       " {'answer_start': [784], 'text': ['Drunk in Love']},\n",
       " {'answer_start': [974], 'text': ['On the Run Tour.']},\n",
       " {'answer_start': [108], 'text': ['three']},\n",
       " {'answer_start': [283], 'text': ['Beck']},\n",
       " {'answer_start': [364], 'text': ['Vogue']},\n",
       " {'answer_start': [770], 'text': ['Coldplay']},\n",
       " {'answer_start': [108], 'text': ['three']},\n",
       " {'answer_start': [283], 'text': ['Beck']},\n",
       " {'answer_start': [770], 'text': ['Coldplay']},\n",
       " {'answer_start': [77], 'text': ['six awards']},\n",
       " {'answer_start': [364], 'text': ['Vogue']},\n",
       " {'answer_start': [770], 'text': ['Coldplay']},\n",
       " {'answer_start': [770], 'text': ['Coldplay']},\n",
       " {'answer_start': [77], 'text': ['six']},\n",
       " {'answer_start': [108], 'text': ['three']},\n",
       " {'answer_start': [283], 'text': ['Beck']},\n",
       " {'answer_start': [364], 'text': ['Vogue']},\n",
       " {'answer_start': [770], 'text': ['Coldplay']},\n",
       " {'answer_start': [140], 'text': ['Tidal']},\n",
       " {'answer_start': [154], 'text': ['Formation']},\n",
       " {'answer_start': [3], 'text': ['February 6, 2016']},\n",
       " {'answer_start': [101], 'text': ['exclusively']},\n",
       " {'answer_start': [140], 'text': ['Tidal']},\n",
       " {'answer_start': [116], 'text': ['music streaming']},\n",
       " {'answer_start': [3], 'text': ['February 6, 2016']},\n",
       " {'answer_start': [140], 'text': ['Tidal']},\n",
       " {'answer_start': [447], 'text': ['300 million']},\n",
       " {'answer_start': [825], 'text': ['Paris']},\n",
       " {'answer_start': [617], 'text': ['miscarriage']},\n",
       " {'answer_start': [62], 'text': ['Jay Z']},\n",
       " {'answer_start': [332], 'text': ['April 4, 2008']},\n",
       " {'answer_start': [447], 'text': ['300 million']},\n",
       " {'answer_start': [736], 'text': ['wrote music']},\n",
       " {'answer_start': [825], 'text': ['Paris']},\n",
       " {'answer_start': [94], 'text': [\"'03 Bonnie & Clyde\"]},\n",
       " {'answer_start': [332], 'text': ['April 4, 2008']},\n",
       " {'answer_start': [447], 'text': ['300 million']},\n",
       " {'answer_start': [617], 'text': ['miscarriage']},\n",
       " {'answer_start': [912], 'text': ['Paris.']},\n",
       " {'answer_start': [40], 'text': ['MTV Video Music Awards']},\n",
       " {'answer_start': [360], 'text': ['her pregnancy']},\n",
       " {'answer_start': [535], 'text': ['12.4 million']},\n",
       " {'answer_start': [35], 'text': ['2011 MTV Video Music Awards']},\n",
       " {'answer_start': [417], 'text': ['Her appearance']},\n",
       " {'answer_start': [616], 'text': ['most tweets per second']},\n",
       " {'answer_start': [719], 'text': ['Beyonce pregnant']},\n",
       " {'answer_start': [92], 'text': ['Love on Top']},\n",
       " {'answer_start': [35], 'text': ['2011 MTV Video Music Awards']},\n",
       " {'answer_start': [92], 'text': ['Love on Top']},\n",
       " {'answer_start': [535], 'text': ['12.4 million']},\n",
       " {'answer_start': [719], 'text': ['Beyonce pregnant']},\n",
       " {'answer_start': [216], 'text': ['Lifeandtimes.com']},\n",
       " {'answer_start': [160], 'text': ['Glory']},\n",
       " {'answer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [74], 'text': ['Lenox Hill Hospital']},\n",
       " {'answer_start': [160], 'text': ['Glory']},\n",
       " {'answer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [457], 'text': ['B.I.C.']},\n",
       " {'answer_start': [3], 'text': ['January 7, 2012']},\n",
       " {'answer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [160], 'text': ['Glory']},\n",
       " {'answer_start': [367], 'text': [\"Blue Ivy's cries\"]},\n",
       " {'answer_start': [457], 'text': ['B.I.C.']},\n",
       " {'answer_start': [880], 'text': ['George Zimmerman']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [398], 'text': ['4 million']},\n",
       " {'answer_start': [700], 'text': ['same sex marriage']},\n",
       " {'answer_start': [840], 'text': ['a rally']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [186], 'text': ['At Last']},\n",
       " {'answer_start': [458], 'text': ['Tumblr']},\n",
       " {'answer_start': [700], 'text': ['same sex marriage']},\n",
       " {'answer_start': [29], 'text': ['Vogue']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy campaign']},\n",
       " {'answer_start': [29], 'text': ['Vogue']},\n",
       " {'answer_start': [38], 'text': ['April 2013']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy']},\n",
       " {'answer_start': [445], 'text': ['Flawless']},\n",
       " {'answer_start': [586], 'text': ['leadership in girls']},\n",
       " {'answer_start': [365], 'text': ['Chimamanda Ngozi Adichie']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy']},\n",
       " {'answer_start': [44], 'text': ['the ONE Campaign']},\n",
       " {'answer_start': [374], 'text': ['September 2015']},\n",
       " {'answer_start': [191], 'text': ['women']},\n",
       " {'answer_start': [313], 'text': ['priorities']},\n",
       " {'answer_start': [3], 'text': ['2015']},\n",
       " {'answer_start': [125], 'text': ['Angela Merkel and Nkosazana Dlamini-Zuma']},\n",
       " {'answer_start': [218], 'text': ['head of the G7 in Germany']},\n",
       " {'answer_start': [374], 'text': ['September 2015']},\n",
       " {'answer_start': [44], 'text': ['the ONE Campaign']},\n",
       " {'answer_start': [125], 'text': ['Angela Merkel and Nkosazana Dlamini-Zuma']},\n",
       " {'answer_start': [214], 'text': ['the head of the G7 in Germany']},\n",
       " {'answer_start': [191], 'text': ['women']},\n",
       " {'answer_start': [23], 'text': ['Freddie Gray']},\n",
       " {'answer_start': [132], 'text': ['protesters']},\n",
       " {'answer_start': [23], 'text': ['Freddie Gray']},\n",
       " {'answer_start': [186], 'text': ['thousands of dollars']},\n",
       " {'answer_start': [248], 'text': ['Madonna and Celine Dion']},\n",
       " {'answer_start': [1013], 'text': ['highest-earning power couple']},\n",
       " {'answer_start': [1460], 'text': ['2014']},\n",
       " {'answer_start': [1880], 'text': ['250 million']},\n",
       " {'answer_start': [248], 'text': ['Madonna and Celine Dion']},\n",
       " {'answer_start': [0], 'text': ['Forbes']},\n",
       " {'answer_start': [1112], 'text': ['2011']},\n",
       " {'answer_start': [1660], 'text': ['115 million']},\n",
       " {'answer_start': [1880], 'text': ['250 million']},\n",
       " {'answer_start': [0], 'text': ['Forbes']},\n",
       " {'answer_start': [1557], 'text': ['April 2014.']},\n",
       " {'answer_start': [1427], 'text': ['MTV']},\n",
       " {'answer_start': [1204], 'text': ['2013']},\n",
       " {'answer_start': [28], 'text': ['four']},\n",
       " {'answer_start': [42], 'text': ['Jody Rosen']},\n",
       " {'answer_start': [415], 'text': ['The Daily Mail']},\n",
       " {'answer_start': [546], 'text': ['hip hop']},\n",
       " {'answer_start': [28], 'text': ['four octaves']},\n",
       " {'answer_start': [453], 'text': ['versatile']},\n",
       " {'answer_start': [714], 'text': ['hip hop']},\n",
       " {'answer_start': [883], 'text': ['praise her range and power']},\n",
       " {'answer_start': [28], 'text': ['four']},\n",
       " {'answer_start': [333], 'text': ['Her vocal abilities']},\n",
       " {'answer_start': [630], 'text': ['tart']},\n",
       " {'answer_start': [710], 'text': ['the hip hop era']},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [60], 'text': ['pop, soul and funk']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [417], 'text': [\"re-release of B'Day\"]},\n",
       " {'answer_start': [516], 'text': ['Rudy Perez']},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [267], 'text': ['English']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [369], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [516], 'text': ['Rudy Perez.']},\n",
       " {'answer_start': [431], 'text': [\"B'Day.\"]},\n",
       " {'answer_start': [521], 'text': ['beats']},\n",
       " {'answer_start': [338], 'text': ['Cater 2 U']},\n",
       " {'answer_start': [153], 'text': ['female-empowerment']},\n",
       " {'answer_start': [309], 'text': ['man-tending anthems']},\n",
       " {'answer_start': [376], 'text': ['co-producing credits']},\n",
       " {'answer_start': [564], 'text': ['melodies']},\n",
       " {'answer_start': [210], 'text': ['Women']},\n",
       " {'answer_start': [376], 'text': ['co-producing']},\n",
       " {'answer_start': [564], 'text': ['melodies and ideas']},\n",
       " {'answer_start': [205], 'text': ['Beyoncé']},\n",
       " {'answer_start': [132],\n",
       "  'text': ['American Society of Composers, Authors, and Publishers Pop Music Awards']},\n",
       " {'answer_start': [436], 'text': ['Diane Warren']},\n",
       " {'answer_start': [656], 'text': ['Top 20 Hot 100 Songwriters']},\n",
       " {'answer_start': [3], 'text': ['2001']},\n",
       " {'answer_start': [221], 'text': ['third']},\n",
       " {'answer_start': [587], 'text': ['Billboard magazine']},\n",
       " {'answer_start': [221], 'text': ['third woman']},\n",
       " {'answer_start': [92], 'text': ['Pop Songwriter of the Year award']},\n",
       " {'answer_start': [128],\n",
       "  'text': ['the American Society of Composers, Authors, and Publishers Pop Music Awards.']},\n",
       " {'answer_start': [260], 'text': ['three']},\n",
       " {'answer_start': [631], 'text': ['17']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [67], 'text': ['five']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [589], 'text': ['vocal runs']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [534], 'text': ['Vision of Love']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [358], 'text': ['Diana Ross']},\n",
       " {'answer_start': [404], 'text': ['Whitney Houston']},\n",
       " {'answer_start': [534], 'text': ['Vision of Love']},\n",
       " {'answer_start': [4], 'text': ['feminism and female empowerment']},\n",
       " {'answer_start': [134], 'text': ['Josephine Baker']},\n",
       " {'answer_start': [399], 'text': ['Etta James']},\n",
       " {'answer_start': [109], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [418], 'text': ['boldness']},\n",
       " {'answer_start': [211], 'text': ['2006 Fashion Rocks concert']},\n",
       " {'answer_start': [134], 'text': ['Josephine Baker.']},\n",
       " {'answer_start': [195], 'text': ['Déjà Vu']},\n",
       " {'answer_start': [68], 'text': ['Michelle Obama']},\n",
       " {'answer_start': [588], 'text': ['February 2013']},\n",
       " {'answer_start': [144], 'text': ['Oprah Winfrey']},\n",
       " {'answer_start': [68], 'text': ['Michelle Obama']},\n",
       " {'answer_start': [196], 'text': ['a strong woman']},\n",
       " {'answer_start': [567], 'text': ['lyrical and raw']},\n",
       " {'answer_start': [642], 'text': ['to take control of her own career']},\n",
       " {'answer_start': [251], 'text': ['continuing inspiration']},\n",
       " {'answer_start': [57], 'text': ['First Lady Michelle Obama']},\n",
       " {'answer_start': [144], 'text': ['Oprah Winfrey']},\n",
       " {'answer_start': [431], 'text': ['Jean-Michel Basquiat']},\n",
       " {'answer_start': [621], 'text': ['Madonna']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [216], 'text': ['The Mamas']},\n",
       " {'answer_start': [238],\n",
       "  'text': ['Montina Cooper-Donnell, Crystal Collins and Tiffany Moniqué Riddick']},\n",
       " {'answer_start': [3], 'text': ['2006']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [347], 'text': ['2006 BET Awards']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [91], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [216], 'text': ['The Mamas']},\n",
       " {'answer_start': [343], 'text': ['the 2006 BET Awards']},\n",
       " {'answer_start': [36], 'text': ['stage presence and voice']},\n",
       " {'answer_start': [445], 'text': ['L.A. Reid']},\n",
       " {'answer_start': [36], 'text': ['stage presence']},\n",
       " {'answer_start': [87], 'text': ['Jarett Wieselman']},\n",
       " {'answer_start': [484], 'text': ['greatest entertainer alive']},\n",
       " {'answer_start': [393], 'text': [\"she's almost too good\"]},\n",
       " {'answer_start': [87], 'text': ['Jarett Wieselman']},\n",
       " {'answer_start': [445], 'text': ['L.A. Reid']},\n",
       " {'answer_start': [139], 'text': ['Sasha Fierce']},\n",
       " {'answer_start': [378], 'text': ['making of \"Crazy in Love\"']},\n",
       " {'answer_start': [501], 'text': ['2010']},\n",
       " {'answer_start': [712], 'text': ['Revel Presents: Beyoncé Live']},\n",
       " {'answer_start': [243], 'text': ['too aggressive, too strong']},\n",
       " {'answer_start': [679], 'text': ['she would bring her back']},\n",
       " {'answer_start': [475], 'text': ['Sasha Fierce.']},\n",
       " {'answer_start': [456], 'text': ['2008']},\n",
       " {'answer_start': [389], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [542], 'text': ['Allure magazine']},\n",
       " {'answer_start': [41], 'text': ['wide-ranging']},\n",
       " {'answer_start': [88], 'text': ['Touré']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [497], 'text': [\"Destiny's Child\"]},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [543], 'text': ['2006']},\n",
       " {'answer_start': [88], 'text': ['Touré']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [543], 'text': ['2006']},\n",
       " {'answer_start': [242], 'text': ['sexily']},\n",
       " {'answer_start': [43], 'text': ['modelling']},\n",
       " {'answer_start': [62],\n",
       "  'text': [\"Tom Ford's Spring/Summer 2011 fashion show\"]},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [228], 'text': ['January 2013']},\n",
       " {'answer_start': [339], 'text': ['VH1']},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [208], 'text': ['Complex']},\n",
       " {'answer_start': [236], 'text': ['2013']},\n",
       " {'answer_start': [357], 'text': ['number 1']},\n",
       " {'answer_start': [13], 'text': ['2010']},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [170], 'text': ['Hottest Female Singer of All Time']},\n",
       " {'answer_start': [443], 'text': ['Madame Tussauds Wax Museums']},\n",
       " {'answer_start': [134], 'text': ['Her mother']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [188], 'text': [\"Destiny's Style\"]},\n",
       " {'answer_start': [404], 'text': ['2007']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [551], 'text': ['People']},\n",
       " {'answer_start': [0], 'text': ['The Bey Hive']},\n",
       " {'answer_start': [83], 'text': ['The Beyontourage']},\n",
       " {'answer_start': [321], 'text': ['Twitter']},\n",
       " {'answer_start': [4], 'text': ['Bey Hive']},\n",
       " {'answer_start': [87], 'text': ['Beyontourage']},\n",
       " {'answer_start': [4], 'text': ['Bey Hive']},\n",
       " {'answer_start': [87], 'text': ['Beyontourage']},\n",
       " {'answer_start': [184], 'text': ['beehive']},\n",
       " {'answer_start': [158], 'text': ['House of Deréon']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [252], 'text': ['blackface and tribal makeup']},\n",
       " {'answer_start': [3], 'text': ['2006']},\n",
       " {'answer_start': [111], 'text': ['for wearing and using fur']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [249], 'text': ['in blackface and tribal makeup']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [158], 'text': ['House of Deréon.']},\n",
       " {'answer_start': [213], 'text': ['French fashion magazine']},\n",
       " {'answer_start': [80], 'text': ['African-American']},\n",
       " {'answer_start': [108], 'text': ['Emmett Price']},\n",
       " {'answer_start': [335], 'text': [\"L'Oréal\"]},\n",
       " {'answer_start': [615], 'text': ['natural pictures be used']},\n",
       " {'answer_start': [436], 'text': ['it is categorically untrue']},\n",
       " {'answer_start': [33], 'text': ['costuming']},\n",
       " {'answer_start': [108], 'text': ['Emmett Price']},\n",
       " {'answer_start': [335], 'text': [\"L'Oréal\"]},\n",
       " {'answer_start': [386], 'text': ['Feria hair color advertisements']},\n",
       " {'answer_start': [505], 'text': ['H&M']},\n",
       " {'answer_start': [215], 'text': ['The Guardian']},\n",
       " {'answer_start': [700], 'text': ['2013']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [238], 'text': ['Artist of the Decade']},\n",
       " {'answer_start': [700], 'text': ['2013']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [738], 'text': ['Baz Luhrmann']},\n",
       " {'answer_start': [31], 'text': ['Jody Rosen']},\n",
       " {'answer_start': [215], 'text': ['The Guardian']},\n",
       " {'answer_start': [723], 'text': ['Time 100 list']},\n",
       " {'answer_start': [738], 'text': ['Baz Luhrmann']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [385], 'text': ['Gwyneth Paltrow']},\n",
       " {'answer_start': [562], 'text': ['Pepsi']},\n",
       " {'answer_start': [552], 'text': [\"Beyoncé's Pepsi commercial\"]},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [10], 'text': ['work']},\n",
       " {'answer_start': [501], 'text': ['Country Strong']},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [358], 'text': ['Milk Famous']},\n",
       " {'answer_start': [385], 'text': ['Gwyneth Paltrow']},\n",
       " {'answer_start': [501], 'text': ['Country Strong.']},\n",
       " {'answer_start': [517], 'text': ['Nicki Minaj']},\n",
       " {'answer_start': [19], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [225], 'text': ['two']},\n",
       " {'answer_start': [304], 'text': ['8 million']},\n",
       " {'answer_start': [959], 'text': ['fly']},\n",
       " {'answer_start': [1073], 'text': ['July 2014']},\n",
       " {'answer_start': [19], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [218], 'text': ['earned two Grammy Awards']},\n",
       " {'answer_start': [297], 'text': ['around 8 million copies']},\n",
       " {'answer_start': [702], 'text': ['Drake']},\n",
       " {'answer_start': [155], 'text': ['Rolling Stone']},\n",
       " {'answer_start': [702], 'text': ['Drake']},\n",
       " {'answer_start': [940], 'text': ['a species of horse fly']},\n",
       " {'answer_start': [73], 'text': ['15 million']},\n",
       " {'answer_start': [111], 'text': ['118 million']},\n",
       " {'answer_start': [387], 'text': ['64']},\n",
       " {'answer_start': [152], 'text': ['60 million']},\n",
       " {'answer_start': [0], 'text': ['Beyoncé']},\n",
       " {'answer_start': [73], 'text': ['15 million']},\n",
       " {'answer_start': [111], 'text': ['118 million']},\n",
       " {'answer_start': [152], 'text': ['60 million']},\n",
       " {'answer_start': [1052], 'text': ['2008 World Music Awards']},\n",
       " {'answer_start': [387], 'text': ['64 certifications']},\n",
       " {'answer_start': [68], 'text': ['over 15 million']},\n",
       " {'answer_start': [106], 'text': ['over 118 million']},\n",
       " {'answer_start': [261],\n",
       "  'text': ['The Recording Industry Association of America']},\n",
       " {'answer_start': [387], 'text': ['64']},\n",
       " {'answer_start': [1048], 'text': ['the 2008 World Music Awards']},\n",
       " {'answer_start': [16], 'text': ['20']},\n",
       " {'answer_start': [159], 'text': ['Alison Krauss']},\n",
       " {'answer_start': [231], 'text': ['52']},\n",
       " {'answer_start': [586], 'text': ['six']},\n",
       " {'answer_start': [949], 'text': ['two']},\n",
       " {'answer_start': [16], 'text': ['20 Grammy Awards']},\n",
       " {'answer_start': [231], 'text': ['52 nominations']},\n",
       " {'answer_start': [306], 'text': ['2010']},\n",
       " {'answer_start': [705], 'text': ['Adele']},\n",
       " {'answer_start': [16], 'text': ['20']},\n",
       " {'answer_start': [231], 'text': ['52']},\n",
       " {'answer_start': [247], 'text': ['\"Single Ladies (Put a Ring on It)\"']},\n",
       " {'answer_start': [756], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [24], 'text': ['Pepsi']},\n",
       " {'answer_start': [172], 'text': ['50 million']},\n",
       " {'answer_start': [206],\n",
       "  'text': ['The Center for Science in the Public Interest (CSPINET)']},\n",
       " {'answer_start': [535], 'text': ['70']},\n",
       " {'answer_start': [36], 'text': ['2002']},\n",
       " {'answer_start': [101],\n",
       "  'text': ['Britney Spears, Pink, and Enrique Iglesias']},\n",
       " {'answer_start': [191], 'text': ['endorse Pepsi']},\n",
       " {'answer_start': [210],\n",
       "  'text': ['Center for Science in the Public Interest']},\n",
       " {'answer_start': [24], 'text': ['Pepsi']},\n",
       " {'answer_start': [171], 'text': ['$50 million']},\n",
       " {'answer_start': [206],\n",
       "  'text': ['The Center for Science in the Public Interest (CSPINET)']},\n",
       " {'answer_start': [437], 'text': ['NetBase']},\n",
       " {'answer_start': [24], 'text': ['Tommy Hilfiger']},\n",
       " {'answer_start': [0], 'text': ['Beyoncé']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [577], 'text': ['2013']},\n",
       " {'answer_start': [750], 'text': ['400 million']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [452], 'text': ['2011']},\n",
       " {'answer_start': [535], 'text': ['Pulse']},\n",
       " {'answer_start': [654], 'text': ['six editions']},\n",
       " {'answer_start': [172], 'text': ['Diamonds']},\n",
       " {'answer_start': [255], 'text': ['2010.']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [654], 'text': ['six']},\n",
       " {'answer_start': [450], 'text': ['18']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [433], 'text': ['since the age of 18']},\n",
       " {'answer_start': [168], 'text': ['70 staff']},\n",
       " {'answer_start': [236], 'text': ['out of court']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [109], 'text': ['GateFive']},\n",
       " {'answer_start': [168], 'text': ['70']},\n",
       " {'answer_start': [267], 'text': ['June 2013']},\n",
       " {'answer_start': [136], 'text': ['fashion retailer Topshop']},\n",
       " {'answer_start': [209], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [299], 'text': ['activewear']},\n",
       " {'answer_start': [698], 'text': ['fall of 2015']},\n",
       " {'answer_start': [153], 'text': ['Topshop']},\n",
       " {'answer_start': [706], 'text': ['2015']},\n",
       " {'answer_start': [75], 'text': ['Parkwood Entertainment']},\n",
       " {'answer_start': [153], 'text': ['Topshop']},\n",
       " {'answer_start': [123], 'text': ['London']},\n",
       " {'answer_start': [299], 'text': ['activewear']},\n",
       " {'answer_start': [3], 'text': ['March 30, 2015']},\n",
       " {'answer_start': [230], 'text': ['Jay Z']},\n",
       " {'answer_start': [3], 'text': ['March 30, 2015']},\n",
       " {'answer_start': [105], 'text': ['music streaming service']},\n",
       " {'answer_start': [763], 'text': ['low payout of royalties']},\n",
       " {'answer_start': [129], 'text': ['Tidal.']},\n",
       " {'answer_start': [274], 'text': ['Aspiro']},\n",
       " {'answer_start': [230], 'text': ['Jay Z']},\n",
       " {'answer_start': [717], 'text': ['Spotify']},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [218], 'text': ['Agnèz Deréon']},\n",
       " {'answer_start': [408], 'text': ['Beyond Productions']},\n",
       " {'answer_start': [670],\n",
       "  'text': ['sportswear, denim offerings with fur, outerwear and accessories that include handbags and footwear']},\n",
       " {'answer_start': [834], 'text': ['US and Canada']},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [91], 'text': ['2005']},\n",
       " {'answer_start': [205], 'text': ['grandmother, Agnèz Deréon']},\n",
       " {'answer_start': [572], 'text': [\"in Destiny's Child's shows and tours\"]},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [526], 'text': ['Deréon.']},\n",
       " {'answer_start': [51], 'text': ['shoe']},\n",
       " {'answer_start': [741], 'text': ['Brazil']},\n",
       " {'answer_start': [294], 'text': ['2009']},\n",
       " {'answer_start': [258], 'text': ['House of Deréon collection']},\n",
       " {'answer_start': [360], 'text': ['Sasha Fierce for Deréon']},\n",
       " {'answer_start': [638], 'text': ['May 27, 2010']},\n",
       " {'answer_start': [32], 'text': ['House of Brands']},\n",
       " {'answer_start': [159], 'text': ['Beyoncé Fashion Diva']},\n",
       " {'answer_start': [360], 'text': ['Sasha Fierce for Deréon']},\n",
       " {'answer_start': [690], 'text': ['C&A']},\n",
       " {'answer_start': [570], 'text': [\"Dillard's\"]},\n",
       " {'answer_start': [110], 'text': ['Topshop']},\n",
       " {'answer_start': [250], 'text': ['autumn 2015']},\n",
       " {'answer_start': [147], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [126], 'text': ['50']},\n",
       " {'answer_start': [287], 'text': ['April 2016']},\n",
       " {'answer_start': [110], 'text': ['Topshop']},\n",
       " {'answer_start': [147], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [52], 'text': ['activewear']},\n",
       " {'answer_start': [6], 'text': ['Hurricane Katrina']},\n",
       " {'answer_start': [191], 'text': ['250,000']},\n",
       " {'answer_start': [321], 'text': ['Ike']},\n",
       " {'answer_start': [61], 'text': ['the Survivor Foundation']},\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset[\"train\"][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-ff73dcd1c8abea1f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-995cd76e77ca6b01.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'length', 'question', 'title'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'length', 'question', 'title'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wikipedia_cbor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4e536ddfdf9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwikipedia_cbor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wikipedia_cbor' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy', 'bertscore', 'bleu', 'bleurt', 'coval', 'f1', 'gleu', 'glue', 'indic_glue', 'meteor', 'precision', 'recall', 'rouge', 'sacrebleu', 'seqeval', 'squad', 'squad_v2', 'xnli']\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer\n",
    "#'tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "print(list_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
       " {'answer_start': [188], 'text': ['a copper statue of Christ']},\n",
       " {'answer_start': [279], 'text': ['the Main Building']},\n",
       " {'answer_start': [381], 'text': ['a Marian place of prayer and reflection']},\n",
       " {'answer_start': [92], 'text': ['a golden statue of the Virgin Mary']}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset[\"train\"][\"answers\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140712192193816 acquired on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6088e2ce3da494baf2b800febcac88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140712192193816 released on /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140710216695200 acquired on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0ba7902ff64d738e0064bb0aa39d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140710216695200 released on /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140710216695200 acquired on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba771c488da44f5dba55ddc2e7da0308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140710216695200 released on /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "# what happens with 2?\n",
    "\n",
    "tokenized_text[masked_index+2] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', '[MASK]', 'puppet', '##eer', '[SEP]']\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors, output_hidden_states=True)\n",
    "    \n",
    "    for hid in outputs.hidden_states:\n",
    "        print(hid.size())\n",
    "    predictions = outputs.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.8907,  -7.8076,  -7.8029,  ...,  -7.0465,  -6.7833,  -4.5639],\n",
       "         [-13.6117, -13.9999, -13.9970,  ..., -11.8237, -11.1671, -14.1446],\n",
       "         [-11.4372, -11.0038, -11.2873,  ..., -11.7792,  -8.5248,  -6.4698],\n",
       "         ...,\n",
       "         [ -4.5049,  -5.0288,  -4.7041,  ...,  -2.7198,  -2.8374,  -6.6997],\n",
       "         [-12.4046, -12.9720, -13.0765,  ...,  -9.6064,  -9.3818, -10.1639],\n",
       "         [-12.8552, -12.5336, -12.6702,  ..., -10.1807, -10.0876,  -9.0030]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['henson', 'a']\n"
     ]
    }
   ],
   "source": [
    "predicted_index_1 = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_index_2 = torch.argmax(predictions[0, masked_index+2]).item()\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens([predicted_index_1, predicted_index_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d920b3b2d3ac432dbb6cd454f60b5638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1616.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a07e6ffc4a7477d857fc31275e20302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1195.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f154d1aae0d340ac9434aec86723168e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2140.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005307a5b92c42d2ad7370b57fe899c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3486.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "squad_metric, squad_v2_metric = datasets.load_metric('squad'), datasets.load_metric('squad_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair as given in the references (see below)\n",
       "        - 'prediction_text': the text of the answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair (see above),\n",
       "        - 'answers': a Dict in the SQuAD dataset format\n",
       "            {\n",
       "                'text': list of possible texts for the answer, as a list of strings\n",
       "                'answer_start': list of start positions for the answer, as a list of ints\n",
       "            }\n",
       "            Note that answer_start values are not taken into account to compute the metric.\n",
       "Returns:\n",
       "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"squad_v2\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None), 'no_answer_probability': Value(dtype='float32', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD v2 scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of triple for question-answers to score with the following elements:\n",
       "        - the question-answer 'id' field as given in the references (see below)\n",
       "        - the text of the answer\n",
       "        - the probability that the question has no answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "            - 'id': id of the question-answer pair (see above),\n",
       "            - 'answers': a list of Dict {'text': text of the answer as a string}\n",
       "    no_answer_threshold: float\n",
       "        Probability threshold to decide that a question has no answer.\n",
       "Returns:\n",
       "    'exact': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "    'total': Number of score considered\n",
       "    'HasAns_exact': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'HasAns_f1': The F-score of predicted tokens versus the gold answer\n",
       "    'HasAns_total': Number of score considered\n",
       "    'NoAns_exact': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'NoAns_f1': The F-score of predicted tokens versus the gold answer\n",
       "    'NoAns_total': Number of score considered\n",
       "    'best_exact': Best exact match (with varying threshold)\n",
       "    'best_exact_thresh': No-answer probability threshold associated to the best exact match\n",
       "    'best_f1': Best F1 (with varying threshold)\n",
       "    'best_f1_thresh': No-answer probability threshold associated to the best F1\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_v2_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).\n",
      "\n",
      "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by\n",
      "crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,\n",
      "from the corresponding reading passage, or the question might be unanswerable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(squad_metric.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.vocabs import load_tokenizer\n",
    "\n",
    "tokenizer = load_tokenizer('bert-base-uncased')\n",
    "\n",
    "def get_prediction_as_str(identifider, input_ids, start_token, end_token):\n",
    "    return {\n",
    "        \"id\": identifier,\n",
    "        \"prediction_text\": \"\".join(tokenizer.id_to_token(x) \\\n",
    "                                   for x in input_ids[start_token:end_token+1])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-6f5b1d9507b91c14.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-94ee633b92a1c5d3.arrow\n"
     ]
    }
   ],
   "source": [
    "from tools.dataloaders import SQuADDataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "squad_dataset = SQuADDataloader()\n",
    "squad_train_dataset = squad_dataset.train_dataset\n",
    "squad_train_dataloader = DataLoader(squad_train_dataset, batch_size=8)\n",
    "squad_validation_dataset = squad_dataset.validation_dataset\n",
    "squad_validation_dataloader = DataLoader(squad_validation_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_end': tensor([ 35,  46,  83,  35,  98, 100,  66,  29]), 'answer_start': tensor([34, 45, 80, 34, 98, 97, 63, 27]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'input_ids': tensor([[ 101, 3565, 4605,  ...,    0,    0,    0],\n",
      "        [ 101, 3565, 4605,  ...,    0,    0,    0],\n",
      "        [ 101, 3565, 4605,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3565, 4605,  ...,    0,    0,    0],\n",
      "        [ 101, 3565, 4605,  ...,    0,    0,    0],\n",
      "        [ 101, 3565, 4605,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for b in squad_validation_dataloader:\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
