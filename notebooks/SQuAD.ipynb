{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset squad (/home/erolm_a/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "swer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [74], 'text': ['Lenox Hill Hospital']},\n",
       " {'answer_start': [160], 'text': ['Glory']},\n",
       " {'answer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [457], 'text': ['B.I.C.']},\n",
       " {'answer_start': [3], 'text': ['January 7, 2012']},\n",
       " {'answer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [160], 'text': ['Glory']},\n",
       " {'answer_start': [367], 'text': [\"Blue Ivy's cries\"]},\n",
       " {'answer_start': [457], 'text': ['B.I.C.']},\n",
       " {'answer_start': [880], 'text': ['George Zimmerman']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [398], 'text': ['4 million']},\n",
       " {'answer_start': [700], 'text': ['same sex marriage']},\n",
       " {'answer_start': [840], 'text': ['a rally']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [186], 'text': ['At Last']},\n",
       " {'answer_start': [458], 'text': ['Tumblr']},\n",
       " {'answer_start': [700], 'text': ['same sex marriage']},\n",
       " {'answer_start': [29], 'text': ['Vogue']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy campaign']},\n",
       " {'answer_start': [29], 'text': ['Vogue']},\n",
       " {'answer_start': [38], 'text': ['April 2013']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy']},\n",
       " {'answer_start': [445], 'text': ['Flawless']},\n",
       " {'answer_start': [586], 'text': ['leadership in girls']},\n",
       " {'answer_start': [365], 'text': ['Chimamanda Ngozi Adichie']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy']},\n",
       " {'answer_start': [44], 'text': ['the ONE Campaign']},\n",
       " {'answer_start': [374], 'text': ['September 2015']},\n",
       " {'answer_start': [191], 'text': ['women']},\n",
       " {'answer_start': [313], 'text': ['priorities']},\n",
       " {'answer_start': [3], 'text': ['2015']},\n",
       " {'answer_start': [125], 'text': ['Angela Merkel and Nkosazana Dlamini-Zuma']},\n",
       " {'answer_start': [218], 'text': ['head of the G7 in Germany']},\n",
       " {'answer_start': [374], 'text': ['September 2015']},\n",
       " {'answer_start': [44], 'text': ['the ONE Campaign']},\n",
       " {'answer_start': [125], 'text': ['Angela Merkel and Nkosazana Dlamini-Zuma']},\n",
       " {'answer_start': [214], 'text': ['the head of the G7 in Germany']},\n",
       " {'answer_start': [191], 'text': ['women']},\n",
       " {'answer_start': [23], 'text': ['Freddie Gray']},\n",
       " {'answer_start': [132], 'text': ['protesters']},\n",
       " {'answer_start': [23], 'text': ['Freddie Gray']},\n",
       " {'answer_start': [186], 'text': ['thousands of dollars']},\n",
       " {'answer_start': [248], 'text': ['Madonna and Celine Dion']},\n",
       " {'answer_start': [1013], 'text': ['highest-earning power couple']},\n",
       " {'answer_start': [1460], 'text': ['2014']},\n",
       " {'answer_start': [1880], 'text': ['250 million']},\n",
       " {'answer_start': [248], 'text': ['Madonna and Celine Dion']},\n",
       " {'answer_start': [0], 'text': ['Forbes']},\n",
       " {'answer_start': [1112], 'text': ['2011']},\n",
       " {'answer_start': [1660], 'text': ['115 million']},\n",
       " {'answer_start': [1880], 'text': ['250 million']},\n",
       " {'answer_start': [0], 'text': ['Forbes']},\n",
       " {'answer_start': [1557], 'text': ['April 2014.']},\n",
       " {'answer_start': [1427], 'text': ['MTV']},\n",
       " {'answer_start': [1204], 'text': ['2013']},\n",
       " {'answer_start': [28], 'text': ['four']},\n",
       " {'answer_start': [42], 'text': ['Jody Rosen']},\n",
       " {'answer_start': [415], 'text': ['The Daily Mail']},\n",
       " {'answer_start': [546], 'text': ['hip hop']},\n",
       " {'answer_start': [28], 'text': ['four octaves']},\n",
       " {'answer_start': [453], 'text': ['versatile']},\n",
       " {'answer_start': [714], 'text': ['hip hop']},\n",
       " {'answer_start': [883], 'text': ['praise her range and power']},\n",
       " {'answer_start': [28], 'text': ['four']},\n",
       " {'answer_start': [333], 'text': ['Her vocal abilities']},\n",
       " {'answer_start': [630], 'text': ['tart']},\n",
       " {'answer_start': [710], 'text': ['the hip hop era']},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [60], 'text': ['pop, soul and funk']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [417], 'text': [\"re-release of B'Day\"]},\n",
       " {'answer_start': [516], 'text': ['Rudy Perez']},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [267], 'text': ['English']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [369], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [516], 'text': ['Rudy Perez.']},\n",
       " {'answer_start': [431], 'text': [\"B'Day.\"]},\n",
       " {'answer_start': [521], 'text': ['beats']},\n",
       " {'answer_start': [338], 'text': ['Cater 2 U']},\n",
       " {'answer_start': [153], 'text': ['female-empowerment']},\n",
       " {'answer_start': [309], 'text': ['man-tending anthems']},\n",
       " {'answer_start': [376], 'text': ['co-producing credits']},\n",
       " {'answer_start': [564], 'text': ['melodies']},\n",
       " {'answer_start': [210], 'text': ['Women']},\n",
       " {'answer_start': [376], 'text': ['co-producing']},\n",
       " {'answer_start': [564], 'text': ['melodies and ideas']},\n",
       " {'answer_start': [205], 'text': ['Beyoncé']},\n",
       " {'answer_start': [132],\n",
       "  'text': ['American Society of Composers, Authors, and Publishers Pop Music Awards']},\n",
       " {'answer_start': [436], 'text': ['Diane Warren']},\n",
       " {'answer_start': [656], 'text': ['Top 20 Hot 100 Songwriters']},\n",
       " {'answer_start': [3], 'text': ['2001']},\n",
       " {'answer_start': [221], 'text': ['third']},\n",
       " {'answer_start': [587], 'text': ['Billboard magazine']},\n",
       " {'answer_start': [221], 'text': ['third woman']},\n",
       " {'answer_start': [92], 'text': ['Pop Songwriter of the Year award']},\n",
       " {'answer_start': [128],\n",
       "  'text': ['the American Society of Composers, Authors, and Publishers Pop Music Awards.']},\n",
       " {'answer_start': [260], 'text': ['three']},\n",
       " {'answer_start': [631], 'text': ['17']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [67], 'text': ['five']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [589], 'text': ['vocal runs']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [534], 'text': ['Vision of Love']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [358], 'text': ['Diana Ross']},\n",
       " {'answer_start': [404], 'text': ['Whitney Houston']},\n",
       " {'answer_start': [534], 'text': ['Vision of Love']},\n",
       " {'answer_start': [4], 'text': ['feminism and female empowerment']},\n",
       " {'answer_start': [134], 'text': ['Josephine Baker']},\n",
       " {'answer_start': [399], 'text': ['Etta James']},\n",
       " {'answer_start': [109], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [418], 'text': ['boldness']},\n",
       " {'answer_start': [211], 'text': ['2006 Fashion Rocks concert']},\n",
       " {'answer_start': [134], 'text': ['Josephine Baker.']},\n",
       " {'answer_start': [195], 'text': ['Déjà Vu']},\n",
       " {'answer_start': [68], 'text': ['Michelle Obama']},\n",
       " {'answer_start': [588], 'text': ['February 2013']},\n",
       " {'answer_start': [144], 'text': ['Oprah Winfrey']},\n",
       " {'answer_start': [68], 'text': ['Michelle Obama']},\n",
       " {'answer_start': [196], 'text': ['a strong woman']},\n",
       " {'answer_start': [567], 'text': ['lyrical and raw']},\n",
       " {'answer_start': [642], 'text': ['to take control of her own career']},\n",
       " {'answer_start': [251], 'text': ['continuing inspiration']},\n",
       " {'answer_start': [57], 'text': ['First Lady Michelle Obama']},\n",
       " {'answer_start': [144], 'text': ['Oprah Winfrey']},\n",
       " {'answer_start': [431], 'text': ['Jean-Michel Basquiat']},\n",
       " {'answer_start': [621], 'text': ['Madonna']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [216], 'text': ['The Mamas']},\n",
       " {'answer_start': [238],\n",
       "  'text': ['Montina Cooper-Donnell, Crystal Collins and Tiffany Moniqué Riddick']},\n",
       " {'answer_start': [3], 'text': ['2006']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [347], 'text': ['2006 BET Awards']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [91], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [216], 'text': ['The Mamas']},\n",
       " {'answer_start': [343], 'text': ['the 2006 BET Awards']},\n",
       " {'answer_start': [36], 'text': ['stage presence and voice']},\n",
       " {'answer_start': [445], 'text': ['L.A. Reid']},\n",
       " {'answer_start': [36], 'text': ['stage presence']},\n",
       " {'answer_start': [87], 'text': ['Jarett Wieselman']},\n",
       " {'answer_start': [484], 'text': ['greatest entertainer alive']},\n",
       " {'answer_start': [393], 'text': [\"she's almost too good\"]},\n",
       " {'answer_start': [87], 'text': ['Jarett Wieselman']},\n",
       " {'answer_start': [445], 'text': ['L.A. Reid']},\n",
       " {'answer_start': [139], 'text': ['Sasha Fierce']},\n",
       " {'answer_start': [378], 'text': ['making of \"Crazy in Love\"']},\n",
       " {'answer_start': [501], 'text': ['2010']},\n",
       " {'answer_start': [712], 'text': ['Revel Presents: Beyoncé Live']},\n",
       " {'answer_start': [243], 'text': ['too aggressive, too strong']},\n",
       " {'answer_start': [679], 'text': ['she would bring her back']},\n",
       " {'answer_start': [475], 'text': ['Sasha Fierce.']},\n",
       " {'answer_start': [456], 'text': ['2008']},\n",
       " {'answer_start': [389], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [542], 'text': ['Allure magazine']},\n",
       " {'answer_start': [41], 'text': ['wide-ranging']},\n",
       " {'answer_start': [88], 'text': ['Touré']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [497], 'text': [\"Destiny's Child\"]},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [543], 'text': ['2006']},\n",
       " {'answer_start': [88], 'text': ['Touré']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [543], 'text': ['2006']},\n",
       " {'answer_start': [242], 'text': ['sexily']},\n",
       " {'answer_start': [43], 'text': ['modelling']},\n",
       " {'answer_start': [62],\n",
       "  'text': [\"Tom Ford's Spring/Summer 2011 fashion show\"]},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [228], 'text': ['January 2013']},\n",
       " {'answer_start': [339], 'text': ['VH1']},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [208], 'text': ['Complex']},\n",
       " {'answer_start': [236], 'text': ['2013']},\n",
       " {'answer_start': [357], 'text': ['number 1']},\n",
       " {'answer_start': [13], 'text': ['2010']},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [170], 'text': ['Hottest Female Singer of All Time']},\n",
       " {'answer_start': [443], 'text': ['Madame Tussauds Wax Museums']},\n",
       " {'answer_start': [134], 'text': ['Her mother']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [188], 'text': [\"Destiny's Style\"]},\n",
       " {'answer_start': [404], 'text': ['2007']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [551], 'text': ['People']},\n",
       " {'answer_start': [0], 'text': ['The Bey Hive']},\n",
       " {'answer_start': [83], 'text': ['The Beyontourage']},\n",
       " {'answer_start': [321], 'text': ['Twitter']},\n",
       " {'answer_start': [4], 'text': ['Bey Hive']},\n",
       " {'answer_start': [87], 'text': ['Beyontourage']},\n",
       " {'answer_start': [4], 'text': ['Bey Hive']},\n",
       " {'answer_start': [87], 'text': ['Beyontourage']},\n",
       " {'answer_start': [184], 'text': ['beehive']},\n",
       " {'answer_start': [158], 'text': ['House of Deréon']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [252], 'text': ['blackface and tribal makeup']},\n",
       " {'answer_start': [3], 'text': ['2006']},\n",
       " {'answer_start': [111], 'text': ['for wearing and using fur']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [249], 'text': ['in blackface and tribal makeup']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [158], 'text': ['House of Deréon.']},\n",
       " {'answer_start': [213], 'text': ['French fashion magazine']},\n",
       " {'answer_start': [80], 'text': ['African-American']},\n",
       " {'answer_start': [108], 'text': ['Emmett Price']},\n",
       " {'answer_start': [335], 'text': [\"L'Oréal\"]},\n",
       " {'answer_start': [615], 'text': ['natural pictures be used']},\n",
       " {'answer_start': [436], 'text': ['it is categorically untrue']},\n",
       " {'answer_start': [33], 'text': ['costuming']},\n",
       " {'answer_start': [108], 'text': ['Emmett Price']},\n",
       " {'answer_start': [335], 'text': [\"L'Oréal\"]},\n",
       " {'answer_start': [386], 'text': ['Feria hair color advertisements']},\n",
       " {'answer_start': [505], 'text': ['H&M']},\n",
       " {'answer_start': [215], 'text': ['The Guardian']},\n",
       " {'answer_start': [700], 'text': ['2013']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [238], 'text': ['Artist of the Decade']},\n",
       " {'answer_start': [700], 'text': ['2013']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [738], 'text': ['Baz Luhrmann']},\n",
       " {'answer_start': [31], 'text': ['Jody Rosen']},\n",
       " {'answer_start': [215], 'text': ['The Guardian']},\n",
       " {'answer_start': [723], 'text': ['Time 100 list']},\n",
       " {'answer_start': [738], 'text': ['Baz Luhrmann']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [385], 'text': ['Gwyneth Paltrow']},\n",
       " {'answer_start': [562], 'text': ['Pepsi']},\n",
       " {'answer_start': [552], 'text': [\"Beyoncé's Pepsi commercial\"]},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [10], 'text': ['work']},\n",
       " {'answer_start': [501], 'text': ['Country Strong']},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [358], 'text': ['Milk Famous']},\n",
       " {'answer_start': [385], 'text': ['Gwyneth Paltrow']},\n",
       " {'answer_start': [501], 'text': ['Country Strong.']},\n",
       " {'answer_start': [517], 'text': ['Nicki Minaj']},\n",
       " {'answer_start': [19], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [225], 'text': ['two']},\n",
       " {'answer_start': [304], 'text': ['8 million']},\n",
       " {'answer_start': [959], 'text': ['fly']},\n",
       " {'answer_start': [1073], 'text': ['July 2014']},\n",
       " {'answer_start': [19], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [218], 'text': ['earned two Grammy Awards']},\n",
       " {'answer_start': [297], 'text': ['around 8 million copies']},\n",
       " {'answer_start': [702], 'text': ['Drake']},\n",
       " {'answer_start': [155], 'text': ['Rolling Stone']},\n",
       " {'answer_start': [702], 'text': ['Drake']},\n",
       " {'answer_start': [940], 'text': ['a species of horse fly']},\n",
       " {'answer_start': [73], 'text': ['15 million']},\n",
       " {'answer_start': [111], 'text': ['118 million']},\n",
       " {'answer_start': [387], 'text': ['64']},\n",
       " {'answer_start': [152], 'text': ['60 million']},\n",
       " {'answer_start': [0], 'text': ['Beyoncé']},\n",
       " {'answer_start': [73], 'text': ['15 million']},\n",
       " {'answer_start': [111], 'text': ['118 million']},\n",
       " {'answer_start': [152], 'text': ['60 million']},\n",
       " {'answer_start': [1052], 'text': ['2008 World Music Awards']},\n",
       " {'answer_start': [387], 'text': ['64 certifications']},\n",
       " {'answer_start': [68], 'text': ['over 15 million']},\n",
       " {'answer_start': [106], 'text': ['over 118 million']},\n",
       " {'answer_start': [261],\n",
       "  'text': ['The Recording Industry Association of America']},\n",
       " {'answer_start': [387], 'text': ['64']},\n",
       " {'answer_start': [1048], 'text': ['the 2008 World Music Awards']},\n",
       " {'answer_start': [16], 'text': ['20']},\n",
       " {'answer_start': [159], 'text': ['Alison Krauss']},\n",
       " {'answer_start': [231], 'text': ['52']},\n",
       " {'answer_start': [586], 'text': ['six']},\n",
       " {'answer_start': [949], 'text': ['two']},\n",
       " {'answer_start': [16], 'text': ['20 Grammy Awards']},\n",
       " {'answer_start': [231], 'text': ['52 nominations']},\n",
       " {'answer_start': [306], 'text': ['2010']},\n",
       " {'answer_start': [705], 'text': ['Adele']},\n",
       " {'answer_start': [16], 'text': ['20']},\n",
       " {'answer_start': [231], 'text': ['52']},\n",
       " {'answer_start': [247], 'text': ['\"Single Ladies (Put a Ring on It)\"']},\n",
       " {'answer_start': [756], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [24], 'text': ['Pepsi']},\n",
       " {'answer_start': [172], 'text': ['50 million']},\n",
       " {'answer_start': [206],\n",
       "  'text': ['The Center for Science in the Public Interest (CSPINET)']},\n",
       " {'answer_start': [535], 'text': ['70']},\n",
       " {'answer_start': [36], 'text': ['2002']},\n",
       " {'answer_start': [101],\n",
       "  'text': ['Britney Spears, Pink, and Enrique Iglesias']},\n",
       " {'answer_start': [191], 'text': ['endorse Pepsi']},\n",
       " {'answer_start': [210],\n",
       "  'text': ['Center for Science in the Public Interest']},\n",
       " {'answer_start': [24], 'text': ['Pepsi']},\n",
       " {'answer_start': [171], 'text': ['$50 million']},\n",
       " {'answer_start': [206],\n",
       "  'text': ['The Center for Science in the Public Interest (CSPINET)']},\n",
       " {'answer_start': [437], 'text': ['NetBase']},\n",
       " {'answer_start': [24], 'text': ['Tommy Hilfiger']},\n",
       " {'answer_start': [0], 'text': ['Beyoncé']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [577], 'text': ['2013']},\n",
       " {'answer_start': [750], 'text': ['400 million']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [452], 'text': ['2011']},\n",
       " {'answer_start': [535], 'text': ['Pulse']},\n",
       " {'answer_start': [654], 'text': ['six editions']},\n",
       " {'answer_start': [172], 'text': ['Diamonds']},\n",
       " {'answer_start': [255], 'text': ['2010.']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [654], 'text': ['six']},\n",
       " {'answer_start': [450], 'text': ['18']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [433], 'text': ['since the age of 18']},\n",
       " {'answer_start': [168], 'text': ['70 staff']},\n",
       " {'answer_start': [236], 'text': ['out of court']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [109], 'text': ['GateFive']},\n",
       " {'answer_start': [168], 'text': ['70']},\n",
       " {'answer_start': [267], 'text': ['June 2013']},\n",
       " {'answer_start': [136], 'text': ['fashion retailer Topshop']},\n",
       " {'answer_start': [209], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [299], 'text': ['activewear']},\n",
       " {'answer_start': [698], 'text': ['fall of 2015']},\n",
       " {'answer_start': [153], 'text': ['Topshop']},\n",
       " {'answer_start': [706], 'text': ['2015']},\n",
       " {'answer_start': [75], 'text': ['Parkwood Entertainment']},\n",
       " {'answer_start': [153], 'text': ['Topshop']},\n",
       " {'answer_start': [123], 'text': ['London']},\n",
       " {'answer_start': [299], 'text': ['activewear']},\n",
       " {'answer_start': [3], 'text': ['March 30, 2015']},\n",
       " {'answer_start': [230], 'text': ['Jay Z']},\n",
       " {'answer_start': [3], 'text': ['March 30, 2015']},\n",
       " {'answer_start': [105], 'text': ['music streaming service']},\n",
       " {'answer_start': [763], 'text': ['low payout of royalties']},\n",
       " {'answer_start': [129], 'text': ['Tidal.']},\n",
       " {'answer_start': [274], 'text': ['Aspiro']},\n",
       " {'answer_start': [230], 'text': ['Jay Z']},\n",
       " {'answer_start': [717], 'text': ['Spotify']},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [218], 'text': ['Agnèz Deréon']},\n",
       " {'answer_start': [408], 'text': ['Beyond Productions']},\n",
       " {'answer_start': [670],\n",
       "  'text': ['sportswear, denim offerings with fur, outerwear and accessories that include handbags and footwear']},\n",
       " {'answer_start': [834], 'text': ['US and Canada']},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [91], 'text': ['2005']},\n",
       " {'answer_start': [205], 'text': ['grandmother, Agnèz Deréon']},\n",
       " {'answer_start': [572], 'text': [\"in Destiny's Child's shows and tours\"]},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [526], 'text': ['Deréon.']},\n",
       " {'answer_start': [51], 'text': ['shoe']},\n",
       " {'answer_start': [741], 'text': ['Brazil']},\n",
       " {'answer_start': [294], 'text': ['2009']},\n",
       " {'answer_start': [258], 'text': ['House of Deréon collection']},\n",
       " {'answer_start': [360], 'text': ['Sasha Fierce for Deréon']},\n",
       " {'answer_start': [638], 'text': ['May 27, 2010']},\n",
       " {'answer_start': [32], 'text': ['House of Brands']},\n",
       " {'answer_start': [159], 'text': ['Beyoncé Fashion Diva']},\n",
       " {'answer_start': [360], 'text': ['Sasha Fierce for Deréon']},\n",
       " {'answer_start': [690], 'text': ['C&A']},\n",
       " {'answer_start': [570], 'text': [\"Dillard's\"]},\n",
       " {'answer_start': [110], 'text': ['Topshop']},\n",
       " {'answer_start': [250], 'text': ['autumn 2015']},\n",
       " {'answer_start': [147], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [126], 'text': ['50']},\n",
       " {'answer_start': [287], 'text': ['April 2016']},\n",
       " {'answer_start': [110], 'text': ['Topshop']},\n",
       " {'answer_start': [147], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [52], 'text': ['activewear']},\n",
       " {'answer_start': [6], 'text': ['Hurricane Katrina']},\n",
       " {'answer_start': [191], 'text': ['250,000']},\n",
       " {'answer_start': [321], 'text': ['Ike']},\n",
       " {'answer_start': [61], 'text': ['the Survivor Foundation']},\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "squad_dataset[\"train\"][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/erolm_a/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-e31338153c845b21.arrow\n",
      "Loading cached processed dataset at /home/erolm_a/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-3d1f03e4f978c026.arrow\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'length', 'question', 'title'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'length', 'question', 'title'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
       " {'answer_start': [188], 'text': ['a copper statue of Christ']},\n",
       " {'answer_start': [279], 'text': ['the Main Building']},\n",
       " {'answer_start': [381], 'text': ['a Marian place of prayer and reflection']},\n",
       " {'answer_start': [92], 'text': ['a golden statue of the Virgin Mary']}]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "squad_dataset[\"train\"][\"answers\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:filelock:Lock 140261613437904 acquired on /home/erolm_a/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 579kB/s]\n",
      "INFO:filelock:Lock 140261613437904 released on /home/erolm_a/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]''answer_end'])\n",
    "\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0f6705d6c8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# If you have a GPU, put everything on cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msegments_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens with 2?\n",
    "\n",
    "tokenized_text[masked_index+2] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', '[MASK]', 'puppet', '##eer', '[SEP]']\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors, output_hidden_states=True)\n",
    "    \n",
    "    for hid in outputs.hidden_states:\n",
    "        print(hid.size())\n",
    "    predictions = outputs.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['henson', 'a']\n"
     ]
    }
   ],
   "source": [
    "predicted_index_1 = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_index_2 = torch.argmax(predictions[0, masked_index+2]).item()\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens([predicted_index_1, predicted_index_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "squad_metric, squad_v2_metric = datasets.load_metric('squad'), datasets.load_metric('squad_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Metric(name: \"squad\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair as given in the references (see below)\n",
       "        - 'prediction_text': the text of the answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "        - 'id': id of the question-answer pair (see above),\n",
       "        - 'answers': a Dict in the SQuAD dataset format\n",
       "            {\n",
       "                'text': list of possible texts for the answer, as a list of strings\n",
       "                'answer_start': list of start positions for the answer, as a list of ints\n",
       "            }\n",
       "            Note that answer_start values are not taken into account to compute the metric.\n",
       "Returns:\n",
       "    'exact_match': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "squad_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Metric(name: \"squad_v2\", features: {'predictions': {'id': Value(dtype='string', id=None), 'prediction_text': Value(dtype='string', id=None), 'no_answer_probability': Value(dtype='float32', id=None)}, 'references': {'id': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)}}, usage: \"\"\"\n",
       "Computes SQuAD v2 scores (F1 and EM).\n",
       "Args:\n",
       "    predictions: List of triple for question-answers to score with the following elements:\n",
       "        - the question-answer 'id' field as given in the references (see below)\n",
       "        - the text of the answer\n",
       "        - the probability that the question has no answer\n",
       "    references: List of question-answers dictionaries with the following key-values:\n",
       "            - 'id': id of the question-answer pair (see above),\n",
       "            - 'answers': a list of Dict {'text': text of the answer as a string}\n",
       "    no_answer_threshold: float\n",
       "        Probability threshold to decide that a question has no answer.\n",
       "Returns:\n",
       "    'exact': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'f1': The F-score of predicted tokens versus the gold answer\n",
       "    'total': Number of score considered\n",
       "    'HasAns_exact': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'HasAns_f1': The F-score of predicted tokens versus the gold answer\n",
       "    'HasAns_total': Number of score considered\n",
       "    'NoAns_exact': Exact match (the normalized answer exactly match the gold answer)\n",
       "    'NoAns_f1': The F-score of predicted tokens versus the gold answer\n",
       "    'NoAns_total': Number of score considered\n",
       "    'best_exact': Best exact match (with varying threshold)\n",
       "    'best_exact_thresh': No-answer probability threshold associated to the best exact match\n",
       "    'best_f1': Best F1 (with varying threshold)\n",
       "    'best_f1_thresh': No-answer probability threshold associated to the best F1\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "squad_v2_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nThis metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).\n\nStanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by\ncrowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,\nfrom the corresponding reading passage, or the question might be unanswerable.\n\n"
     ]
    }
   ],
   "source": [
    "print(squad_metric.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset squad (/home/erolm_a/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n",
      "100%|██████████| 88/88 [01:17<00:00,  1.14ba/s]\n",
      "100%|██████████| 11/11 [00:09<00:00,  1.18ba/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from tools.dataloaders import SQuADDataloader\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "squad_dataset = SQuADDataloader()\n",
    "\n",
    "\n",
    "def squad_collate_fn(rows):\n",
    "    keys = rows[0].keys()\n",
    "    return {key: [row[key] for row in rows] for key in keys}\n",
    "\n",
    "squad_train_dataset = squad_dataset.train_dataset\n",
    "\n",
    "FULL_FINETUNING=False\n",
    "if not FULL_FINETUNING:\n",
    "    squad_dev_size = int(0.01*len(squad_dataset.train_dataset))\n",
    "    squad_dev_indices = np.random.choice(len(squad_dataset.train_dataset), size=squad_dev_size)\n",
    "    squad_train_sampler = SubsetRandomSampler(squad_dev_indices, generator=torch.Generator().manual_seed(42))\n",
    "    squad_train_dataloader = DataLoader(squad_train_dataset, sampler=squad_train_sampler, batch_size=8, collate_fn=squad_collate_fn)\n",
    "\n",
    "else:\n",
    "    squad_train_dataloader = DataLoader(squad_train_dataset, batch_size=8, collate_fn=squad_collate_fn)\n",
    "\n",
    "squad_validation_dataset = squad_dataset.validation_dataset\n",
    "squad_validation_dataloader = DataLoader(squad_validation_dataset, batch_size=8, collate_fn=squad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertModel\n",
    "from torch.nn import Module\n",
    "\n",
    "bert_normal = BertModel.from_pretrained('bert-base-uncased')\n",
    "config = bert_normal.config\n",
    "config.num_labels = 1024\n",
    "model_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "class BQAShim(Module):\n",
    "    def __init__(self, model_qa):\n",
    "        super().__init__()\n",
    "        self.model_qa = model_qa\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, start_positions=None, end_positions=None):\n",
    "        return self.model_qa(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "\n",
    "model_qa_shimmed = BQAShim(model_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10950 [00:00<?, ?it/s]8\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8])\n",
      "torch.Size([8])\n",
      "  0%|          | 0/10950 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-33f2019d1e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_qa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquad_train_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_qa_shimmed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquad_train_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquad_validation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documenti/Work/knowledge-glue/notebooks/models/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, validation_dataloader, load_from_dataloader, optimizer, scheduler, epochs, metric)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-28edacd9763f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, start_positions, end_positions)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel_qa_shimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBQAShim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_qa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1760\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1763\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         )\n\u001b[0;32m--> 958\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    959\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 )\n\u001b[1;32m    558\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    560\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     ):\n\u001b[0;32m--> 386\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/colla/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models.training import train_model, get_optimizer, get_schedule\n",
    "import torch\n",
    "\n",
    "def parse_batch(batch):\n",
    "    input_ids = torch.tensor(batch['input_ids'])\n",
    "    attention_mask = torch.FloatTensor(batch['attention_mask'])\n",
    "    token_type_ids = torch.tensor(batch['token_type_ids'])\n",
    "    start = torch.tensor(batch['answer_start'])\n",
    "    end = torch.tensor(batch['answer_end'])\n",
    "    \n",
    "    return (input_ids, attention_mask, token_type_ids, start, end), (batch,)\n",
    "\n",
    "\n",
    "def my_metric(inputs, outputs):\n",
    "    \"\"\"\n",
    "        predictions: List of question-answers dictionaries with the following key-values:\n",
    "        - 'id': id of the question-answer pair as given in the references (see below)\n",
    "        - 'prediction_text': the text of the answer\n",
    "    references: List of question-answers dictionaries with the following key-values:\n",
    "        - 'id': id of the question-answer pair (see above),\n",
    "        - 'answers': a Dict in the SQuAD dataset format\n",
    "            {\n",
    "                'text': list of possible texts for the answer, as a list of strings\n",
    "                'answer_start': list of start positions for the answer, as a list of ints\n",
    "            }\n",
    "            Note that answer_start values are not taken into account to compute the metric.\n",
    "    \"\"\"\n",
    "    batch_input = inputs[-1]\n",
    "    references = batch_input['answers']\n",
    "    # outputs = total_loss, answer_start_logits, answer_end_logits\n",
    "    answer_start_logits = outputs[1].detach().cpu()\n",
    "    answer_end_logits = outputs[1].detach().cpu()\n",
    "    \n",
    "    answer_starts = torch.argmax(answer_start_logits, 1)\n",
    "    answer_ends = torch.argmax(answer_end_logits, 1)\n",
    "\n",
    "    prediction_texts = squad_dataset.reconstruct_sentence(inputs[0], answer_starts, answer_ends)\n",
    "\n",
    "    predictions = [{\n",
    "        \"id\": id,\n",
    "        \"prediction_text\": prediction_text\n",
    "    } for id, prediction_text in zip(references[\"id\"], prediction_texts)]\n",
    "\n",
    "\n",
    "    squad_metric.add_batch(predictions, references)\n",
    "\n",
    "\n",
    "optimizer = get_optimizer(model_qa)\n",
    "scheduler = get_schedule(1, optimizer, squad_train_dataloader)\n",
    "train_model(model_qa_shimmed, squad_train_dataloader, squad_validation_dataloader, parse_batch, optimizer, scheduler, 1, my_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.6 64-bit ('colla')",
   "display_name": "Python 3.8.6 64-bit ('colla')",
   "metadata": {
    "interpreter": {
     "hash": "38521377e11c81ca32ec32885782ecea85e949d1ff2d3a27fc0b822bf5b348a6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}