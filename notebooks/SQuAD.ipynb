{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset, list_metrics, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/erolm_a/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "swer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [74], 'text': ['Lenox Hill Hospital']},\n",
       " {'answer_start': [160], 'text': ['Glory']},\n",
       " {'answer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [457], 'text': ['B.I.C.']},\n",
       " {'answer_start': [3], 'text': ['January 7, 2012']},\n",
       " {'answer_start': [54], 'text': ['Blue Ivy Carter']},\n",
       " {'answer_start': [160], 'text': ['Glory']},\n",
       " {'answer_start': [367], 'text': [\"Blue Ivy's cries\"]},\n",
       " {'answer_start': [457], 'text': ['B.I.C.']},\n",
       " {'answer_start': [880], 'text': ['George Zimmerman']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [398], 'text': ['4 million']},\n",
       " {'answer_start': [700], 'text': ['same sex marriage']},\n",
       " {'answer_start': [840], 'text': ['a rally']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [112], 'text': ['America the Beautiful']},\n",
       " {'answer_start': [186], 'text': ['At Last']},\n",
       " {'answer_start': [458], 'text': ['Tumblr']},\n",
       " {'answer_start': [700], 'text': ['same sex marriage']},\n",
       " {'answer_start': [29], 'text': ['Vogue']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy campaign']},\n",
       " {'answer_start': [29], 'text': ['Vogue']},\n",
       " {'answer_start': [38], 'text': ['April 2013']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy']},\n",
       " {'answer_start': [445], 'text': ['Flawless']},\n",
       " {'answer_start': [586], 'text': ['leadership in girls']},\n",
       " {'answer_start': [365], 'text': ['Chimamanda Ngozi Adichie']},\n",
       " {'answer_start': [514], 'text': ['Ban Bossy']},\n",
       " {'answer_start': [44], 'text': ['the ONE Campaign']},\n",
       " {'answer_start': [374], 'text': ['September 2015']},\n",
       " {'answer_start': [191], 'text': ['women']},\n",
       " {'answer_start': [313], 'text': ['priorities']},\n",
       " {'answer_start': [3], 'text': ['2015']},\n",
       " {'answer_start': [125], 'text': ['Angela Merkel and Nkosazana Dlamini-Zuma']},\n",
       " {'answer_start': [218], 'text': ['head of the G7 in Germany']},\n",
       " {'answer_start': [374], 'text': ['September 2015']},\n",
       " {'answer_start': [44], 'text': ['the ONE Campaign']},\n",
       " {'answer_start': [125], 'text': ['Angela Merkel and Nkosazana Dlamini-Zuma']},\n",
       " {'answer_start': [214], 'text': ['the head of the G7 in Germany']},\n",
       " {'answer_start': [191], 'text': ['women']},\n",
       " {'answer_start': [23], 'text': ['Freddie Gray']},\n",
       " {'answer_start': [132], 'text': ['protesters']},\n",
       " {'answer_start': [23], 'text': ['Freddie Gray']},\n",
       " {'answer_start': [186], 'text': ['thousands of dollars']},\n",
       " {'answer_start': [248], 'text': ['Madonna and Celine Dion']},\n",
       " {'answer_start': [1013], 'text': ['highest-earning power couple']},\n",
       " {'answer_start': [1460], 'text': ['2014']},\n",
       " {'answer_start': [1880], 'text': ['250 million']},\n",
       " {'answer_start': [248], 'text': ['Madonna and Celine Dion']},\n",
       " {'answer_start': [0], 'text': ['Forbes']},\n",
       " {'answer_start': [1112], 'text': ['2011']},\n",
       " {'answer_start': [1660], 'text': ['115 million']},\n",
       " {'answer_start': [1880], 'text': ['250 million']},\n",
       " {'answer_start': [0], 'text': ['Forbes']},\n",
       " {'answer_start': [1557], 'text': ['April 2014.']},\n",
       " {'answer_start': [1427], 'text': ['MTV']},\n",
       " {'answer_start': [1204], 'text': ['2013']},\n",
       " {'answer_start': [28], 'text': ['four']},\n",
       " {'answer_start': [42], 'text': ['Jody Rosen']},\n",
       " {'answer_start': [415], 'text': ['The Daily Mail']},\n",
       " {'answer_start': [546], 'text': ['hip hop']},\n",
       " {'answer_start': [28], 'text': ['four octaves']},\n",
       " {'answer_start': [453], 'text': ['versatile']},\n",
       " {'answer_start': [714], 'text': ['hip hop']},\n",
       " {'answer_start': [883], 'text': ['praise her range and power']},\n",
       " {'answer_start': [28], 'text': ['four']},\n",
       " {'answer_start': [333], 'text': ['Her vocal abilities']},\n",
       " {'answer_start': [630], 'text': ['tart']},\n",
       " {'answer_start': [710], 'text': ['the hip hop era']},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [60], 'text': ['pop, soul and funk']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [417], 'text': [\"re-release of B'Day\"]},\n",
       " {'answer_start': [516], 'text': ['Rudy Perez']},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [267], 'text': ['English']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [369], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [29], 'text': ['R&B']},\n",
       " {'answer_start': [307], 'text': ['Spanish']},\n",
       " {'answer_start': [516], 'text': ['Rudy Perez.']},\n",
       " {'answer_start': [431], 'text': [\"B'Day.\"]},\n",
       " {'answer_start': [521], 'text': ['beats']},\n",
       " {'answer_start': [338], 'text': ['Cater 2 U']},\n",
       " {'answer_start': [153], 'text': ['female-empowerment']},\n",
       " {'answer_start': [309], 'text': ['man-tending anthems']},\n",
       " {'answer_start': [376], 'text': ['co-producing credits']},\n",
       " {'answer_start': [564], 'text': ['melodies']},\n",
       " {'answer_start': [210], 'text': ['Women']},\n",
       " {'answer_start': [376], 'text': ['co-producing']},\n",
       " {'answer_start': [564], 'text': ['melodies and ideas']},\n",
       " {'answer_start': [205], 'text': ['Beyoncé']},\n",
       " {'answer_start': [132],\n",
       "  'text': ['American Society of Composers, Authors, and Publishers Pop Music Awards']},\n",
       " {'answer_start': [436], 'text': ['Diane Warren']},\n",
       " {'answer_start': [656], 'text': ['Top 20 Hot 100 Songwriters']},\n",
       " {'answer_start': [3], 'text': ['2001']},\n",
       " {'answer_start': [221], 'text': ['third']},\n",
       " {'answer_start': [587], 'text': ['Billboard magazine']},\n",
       " {'answer_start': [221], 'text': ['third woman']},\n",
       " {'answer_start': [92], 'text': ['Pop Songwriter of the Year award']},\n",
       " {'answer_start': [128],\n",
       "  'text': ['the American Society of Composers, Authors, and Publishers Pop Music Awards.']},\n",
       " {'answer_start': [260], 'text': ['three']},\n",
       " {'answer_start': [631], 'text': ['17']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [67], 'text': ['five']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [589], 'text': ['vocal runs']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [534], 'text': ['Vision of Love']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [14], 'text': ['Michael Jackson']},\n",
       " {'answer_start': [358], 'text': ['Diana Ross']},\n",
       " {'answer_start': [404], 'text': ['Whitney Houston']},\n",
       " {'answer_start': [534], 'text': ['Vision of Love']},\n",
       " {'answer_start': [4], 'text': ['feminism and female empowerment']},\n",
       " {'answer_start': [134], 'text': ['Josephine Baker']},\n",
       " {'answer_start': [399], 'text': ['Etta James']},\n",
       " {'answer_start': [109], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [418], 'text': ['boldness']},\n",
       " {'answer_start': [211], 'text': ['2006 Fashion Rocks concert']},\n",
       " {'answer_start': [134], 'text': ['Josephine Baker.']},\n",
       " {'answer_start': [195], 'text': ['Déjà Vu']},\n",
       " {'answer_start': [68], 'text': ['Michelle Obama']},\n",
       " {'answer_start': [588], 'text': ['February 2013']},\n",
       " {'answer_start': [144], 'text': ['Oprah Winfrey']},\n",
       " {'answer_start': [68], 'text': ['Michelle Obama']},\n",
       " {'answer_start': [196], 'text': ['a strong woman']},\n",
       " {'answer_start': [567], 'text': ['lyrical and raw']},\n",
       " {'answer_start': [642], 'text': ['to take control of her own career']},\n",
       " {'answer_start': [251], 'text': ['continuing inspiration']},\n",
       " {'answer_start': [57], 'text': ['First Lady Michelle Obama']},\n",
       " {'answer_start': [144], 'text': ['Oprah Winfrey']},\n",
       " {'answer_start': [431], 'text': ['Jean-Michel Basquiat']},\n",
       " {'answer_start': [621], 'text': ['Madonna']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [216], 'text': ['The Mamas']},\n",
       " {'answer_start': [238],\n",
       "  'text': ['Montina Cooper-Donnell, Crystal Collins and Tiffany Moniqué Riddick']},\n",
       " {'answer_start': [3], 'text': ['2006']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [347], 'text': ['2006 BET Awards']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [53], 'text': ['Suga Mama']},\n",
       " {'answer_start': [91], 'text': [\"B'Day\"]},\n",
       " {'answer_start': [216], 'text': ['The Mamas']},\n",
       " {'answer_start': [343], 'text': ['the 2006 BET Awards']},\n",
       " {'answer_start': [36], 'text': ['stage presence and voice']},\n",
       " {'answer_start': [445], 'text': ['L.A. Reid']},\n",
       " {'answer_start': [36], 'text': ['stage presence']},\n",
       " {'answer_start': [87], 'text': ['Jarett Wieselman']},\n",
       " {'answer_start': [484], 'text': ['greatest entertainer alive']},\n",
       " {'answer_start': [393], 'text': [\"she's almost too good\"]},\n",
       " {'answer_start': [87], 'text': ['Jarett Wieselman']},\n",
       " {'answer_start': [445], 'text': ['L.A. Reid']},\n",
       " {'answer_start': [139], 'text': ['Sasha Fierce']},\n",
       " {'answer_start': [378], 'text': ['making of \"Crazy in Love\"']},\n",
       " {'answer_start': [501], 'text': ['2010']},\n",
       " {'answer_start': [712], 'text': ['Revel Presents: Beyoncé Live']},\n",
       " {'answer_start': [243], 'text': ['too aggressive, too strong']},\n",
       " {'answer_start': [679], 'text': ['she would bring her back']},\n",
       " {'answer_start': [475], 'text': ['Sasha Fierce.']},\n",
       " {'answer_start': [456], 'text': ['2008']},\n",
       " {'answer_start': [389], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [542], 'text': ['Allure magazine']},\n",
       " {'answer_start': [41], 'text': ['wide-ranging']},\n",
       " {'answer_start': [88], 'text': ['Touré']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [497], 'text': [\"Destiny's Child\"]},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [543], 'text': ['2006']},\n",
       " {'answer_start': [88], 'text': ['Touré']},\n",
       " {'answer_start': [389], 'text': ['Bootylicious']},\n",
       " {'answer_start': [543], 'text': ['2006']},\n",
       " {'answer_start': [242], 'text': ['sexily']},\n",
       " {'answer_start': [43], 'text': ['modelling']},\n",
       " {'answer_start': [62],\n",
       "  'text': [\"Tom Ford's Spring/Summer 2011 fashion show\"]},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [228], 'text': ['January 2013']},\n",
       " {'answer_start': [339], 'text': ['VH1']},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [208], 'text': ['Complex']},\n",
       " {'answer_start': [236], 'text': ['2013']},\n",
       " {'answer_start': [357], 'text': ['number 1']},\n",
       " {'answer_start': [13], 'text': ['2010']},\n",
       " {'answer_start': [154], 'text': ['People']},\n",
       " {'answer_start': [170], 'text': ['Hottest Female Singer of All Time']},\n",
       " {'answer_start': [443], 'text': ['Madame Tussauds Wax Museums']},\n",
       " {'answer_start': [134], 'text': ['Her mother']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [188], 'text': [\"Destiny's Style\"]},\n",
       " {'answer_start': [404], 'text': ['2007']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [535], 'text': ['Tyra Banks']},\n",
       " {'answer_start': [551], 'text': ['People']},\n",
       " {'answer_start': [0], 'text': ['The Bey Hive']},\n",
       " {'answer_start': [83], 'text': ['The Beyontourage']},\n",
       " {'answer_start': [321], 'text': ['Twitter']},\n",
       " {'answer_start': [4], 'text': ['Bey Hive']},\n",
       " {'answer_start': [87], 'text': ['Beyontourage']},\n",
       " {'answer_start': [4], 'text': ['Bey Hive']},\n",
       " {'answer_start': [87], 'text': ['Beyontourage']},\n",
       " {'answer_start': [184], 'text': ['beehive']},\n",
       " {'answer_start': [158], 'text': ['House of Deréon']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [252], 'text': ['blackface and tribal makeup']},\n",
       " {'answer_start': [3], 'text': ['2006']},\n",
       " {'answer_start': [111], 'text': ['for wearing and using fur']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [249], 'text': ['in blackface and tribal makeup']},\n",
       " {'answer_start': [237], 'text': [\"L'Officiel\"]},\n",
       " {'answer_start': [158], 'text': ['House of Deréon.']},\n",
       " {'answer_start': [213], 'text': ['French fashion magazine']},\n",
       " {'answer_start': [80], 'text': ['African-American']},\n",
       " {'answer_start': [108], 'text': ['Emmett Price']},\n",
       " {'answer_start': [335], 'text': [\"L'Oréal\"]},\n",
       " {'answer_start': [615], 'text': ['natural pictures be used']},\n",
       " {'answer_start': [436], 'text': ['it is categorically untrue']},\n",
       " {'answer_start': [33], 'text': ['costuming']},\n",
       " {'answer_start': [108], 'text': ['Emmett Price']},\n",
       " {'answer_start': [335], 'text': [\"L'Oréal\"]},\n",
       " {'answer_start': [386], 'text': ['Feria hair color advertisements']},\n",
       " {'answer_start': [505], 'text': ['H&M']},\n",
       " {'answer_start': [215], 'text': ['The Guardian']},\n",
       " {'answer_start': [700], 'text': ['2013']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [238], 'text': ['Artist of the Decade']},\n",
       " {'answer_start': [700], 'text': ['2013']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [738], 'text': ['Baz Luhrmann']},\n",
       " {'answer_start': [31], 'text': ['Jody Rosen']},\n",
       " {'answer_start': [215], 'text': ['The Guardian']},\n",
       " {'answer_start': [723], 'text': ['Time 100 list']},\n",
       " {'answer_start': [738], 'text': ['Baz Luhrmann']},\n",
       " {'answer_start': [1078], 'text': ['2014']},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [385], 'text': ['Gwyneth Paltrow']},\n",
       " {'answer_start': [562], 'text': ['Pepsi']},\n",
       " {'answer_start': [552], 'text': [\"Beyoncé's Pepsi commercial\"]},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [10], 'text': ['work']},\n",
       " {'answer_start': [501], 'text': ['Country Strong']},\n",
       " {'answer_start': [292], 'text': ['White Rabbits']},\n",
       " {'answer_start': [358], 'text': ['Milk Famous']},\n",
       " {'answer_start': [385], 'text': ['Gwyneth Paltrow']},\n",
       " {'answer_start': [501], 'text': ['Country Strong.']},\n",
       " {'answer_start': [517], 'text': ['Nicki Minaj']},\n",
       " {'answer_start': [19], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [225], 'text': ['two']},\n",
       " {'answer_start': [304], 'text': ['8 million']},\n",
       " {'answer_start': [959], 'text': ['fly']},\n",
       " {'answer_start': [1073], 'text': ['July 2014']},\n",
       " {'answer_start': [19], 'text': ['Crazy in Love']},\n",
       " {'answer_start': [218], 'text': ['earned two Grammy Awards']},\n",
       " {'answer_start': [297], 'text': ['around 8 million copies']},\n",
       " {'answer_start': [702], 'text': ['Drake']},\n",
       " {'answer_start': [155], 'text': ['Rolling Stone']},\n",
       " {'answer_start': [702], 'text': ['Drake']},\n",
       " {'answer_start': [940], 'text': ['a species of horse fly']},\n",
       " {'answer_start': [73], 'text': ['15 million']},\n",
       " {'answer_start': [111], 'text': ['118 million']},\n",
       " {'answer_start': [387], 'text': ['64']},\n",
       " {'answer_start': [152], 'text': ['60 million']},\n",
       " {'answer_start': [0], 'text': ['Beyoncé']},\n",
       " {'answer_start': [73], 'text': ['15 million']},\n",
       " {'answer_start': [111], 'text': ['118 million']},\n",
       " {'answer_start': [152], 'text': ['60 million']},\n",
       " {'answer_start': [1052], 'text': ['2008 World Music Awards']},\n",
       " {'answer_start': [387], 'text': ['64 certifications']},\n",
       " {'answer_start': [68], 'text': ['over 15 million']},\n",
       " {'answer_start': [106], 'text': ['over 118 million']},\n",
       " {'answer_start': [261],\n",
       "  'text': ['The Recording Industry Association of America']},\n",
       " {'answer_start': [387], 'text': ['64']},\n",
       " {'answer_start': [1048], 'text': ['the 2008 World Music Awards']},\n",
       " {'answer_start': [16], 'text': ['20']},\n",
       " {'answer_start': [159], 'text': ['Alison Krauss']},\n",
       " {'answer_start': [231], 'text': ['52']},\n",
       " {'answer_start': [586], 'text': ['six']},\n",
       " {'answer_start': [949], 'text': ['two']},\n",
       " {'answer_start': [16], 'text': ['20 Grammy Awards']},\n",
       " {'answer_start': [231], 'text': ['52 nominations']},\n",
       " {'answer_start': [306], 'text': ['2010']},\n",
       " {'answer_start': [705], 'text': ['Adele']},\n",
       " {'answer_start': [16], 'text': ['20']},\n",
       " {'answer_start': [231], 'text': ['52']},\n",
       " {'answer_start': [247], 'text': ['\"Single Ladies (Put a Ring on It)\"']},\n",
       " {'answer_start': [756], 'text': ['Dreamgirls']},\n",
       " {'answer_start': [24], 'text': ['Pepsi']},\n",
       " {'answer_start': [172], 'text': ['50 million']},\n",
       " {'answer_start': [206],\n",
       "  'text': ['The Center for Science in the Public Interest (CSPINET)']},\n",
       " {'answer_start': [535], 'text': ['70']},\n",
       " {'answer_start': [36], 'text': ['2002']},\n",
       " {'answer_start': [101],\n",
       "  'text': ['Britney Spears, Pink, and Enrique Iglesias']},\n",
       " {'answer_start': [191], 'text': ['endorse Pepsi']},\n",
       " {'answer_start': [210],\n",
       "  'text': ['Center for Science in the Public Interest']},\n",
       " {'answer_start': [24], 'text': ['Pepsi']},\n",
       " {'answer_start': [171], 'text': ['$50 million']},\n",
       " {'answer_start': [206],\n",
       "  'text': ['The Center for Science in the Public Interest (CSPINET)']},\n",
       " {'answer_start': [437], 'text': ['NetBase']},\n",
       " {'answer_start': [24], 'text': ['Tommy Hilfiger']},\n",
       " {'answer_start': [0], 'text': ['Beyoncé']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [577], 'text': ['2013']},\n",
       " {'answer_start': [750], 'text': ['400 million']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [452], 'text': ['2011']},\n",
       " {'answer_start': [535], 'text': ['Pulse']},\n",
       " {'answer_start': [654], 'text': ['six editions']},\n",
       " {'answer_start': [172], 'text': ['Diamonds']},\n",
       " {'answer_start': [255], 'text': ['2010.']},\n",
       " {'answer_start': [247], 'text': ['Heat']},\n",
       " {'answer_start': [654], 'text': ['six']},\n",
       " {'answer_start': [450], 'text': ['18']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [433], 'text': ['since the age of 18']},\n",
       " {'answer_start': [168], 'text': ['70 staff']},\n",
       " {'answer_start': [236], 'text': ['out of court']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [28], 'text': ['Starpower: Beyoncé']},\n",
       " {'answer_start': [109], 'text': ['GateFive']},\n",
       " {'answer_start': [168], 'text': ['70']},\n",
       " {'answer_start': [267], 'text': ['June 2013']},\n",
       " {'answer_start': [136], 'text': ['fashion retailer Topshop']},\n",
       " {'answer_start': [209], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [299], 'text': ['activewear']},\n",
       " {'answer_start': [698], 'text': ['fall of 2015']},\n",
       " {'answer_start': [153], 'text': ['Topshop']},\n",
       " {'answer_start': [706], 'text': ['2015']},\n",
       " {'answer_start': [75], 'text': ['Parkwood Entertainment']},\n",
       " {'answer_start': [153], 'text': ['Topshop']},\n",
       " {'answer_start': [123], 'text': ['London']},\n",
       " {'answer_start': [299], 'text': ['activewear']},\n",
       " {'answer_start': [3], 'text': ['March 30, 2015']},\n",
       " {'answer_start': [230], 'text': ['Jay Z']},\n",
       " {'answer_start': [3], 'text': ['March 30, 2015']},\n",
       " {'answer_start': [105], 'text': ['music streaming service']},\n",
       " {'answer_start': [763], 'text': ['low payout of royalties']},\n",
       " {'answer_start': [129], 'text': ['Tidal.']},\n",
       " {'answer_start': [274], 'text': ['Aspiro']},\n",
       " {'answer_start': [230], 'text': ['Jay Z']},\n",
       " {'answer_start': [717], 'text': ['Spotify']},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [218], 'text': ['Agnèz Deréon']},\n",
       " {'answer_start': [408], 'text': ['Beyond Productions']},\n",
       " {'answer_start': [670],\n",
       "  'text': ['sportswear, denim offerings with fur, outerwear and accessories that include handbags and footwear']},\n",
       " {'answer_start': [834], 'text': ['US and Canada']},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [91], 'text': ['2005']},\n",
       " {'answer_start': [205], 'text': ['grandmother, Agnèz Deréon']},\n",
       " {'answer_start': [572], 'text': [\"in Destiny's Child's shows and tours\"]},\n",
       " {'answer_start': [12], 'text': ['her mother']},\n",
       " {'answer_start': [526], 'text': ['Deréon.']},\n",
       " {'answer_start': [51], 'text': ['shoe']},\n",
       " {'answer_start': [741], 'text': ['Brazil']},\n",
       " {'answer_start': [294], 'text': ['2009']},\n",
       " {'answer_start': [258], 'text': ['House of Deréon collection']},\n",
       " {'answer_start': [360], 'text': ['Sasha Fierce for Deréon']},\n",
       " {'answer_start': [638], 'text': ['May 27, 2010']},\n",
       " {'answer_start': [32], 'text': ['House of Brands']},\n",
       " {'answer_start': [159], 'text': ['Beyoncé Fashion Diva']},\n",
       " {'answer_start': [360], 'text': ['Sasha Fierce for Deréon']},\n",
       " {'answer_start': [690], 'text': ['C&A']},\n",
       " {'answer_start': [570], 'text': [\"Dillard's\"]},\n",
       " {'answer_start': [110], 'text': ['Topshop']},\n",
       " {'answer_start': [250], 'text': ['autumn 2015']},\n",
       " {'answer_start': [147], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [126], 'text': ['50']},\n",
       " {'answer_start': [287], 'text': ['April 2016']},\n",
       " {'answer_start': [110], 'text': ['Topshop']},\n",
       " {'answer_start': [147], 'text': ['Parkwood Topshop Athletic Ltd']},\n",
       " {'answer_start': [52], 'text': ['activewear']},\n",
       " {'answer_start': [6], 'text': ['Hurricane Katrina']},\n",
       " {'answer_start': [191], 'text': ['250,000']},\n",
       " {'answer_start': [321], 'text': ['Ike']},\n",
       " {'answer_start': [61], 'text': ['the Survivor Foundation']},\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset[\"train\"][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/erolm_a/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-e31338153c845b21.arrow\n",
      "Loading cached processed dataset at /home/erolm_a/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-3d1f03e4f978c026.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'length', 'question', 'title'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['answers', 'context', 'id', 'length', 'question', 'title'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset.map(lambda x: {\"length\": len(x[\"context\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n",
       " {'answer_start': [188], 'text': ['a copper statue of Christ']},\n",
       " {'answer_start': [279], 'text': ['the Main Building']},\n",
       " {'answer_start': [381], 'text': ['a Marian place of prayer and reflection']},\n",
       " {'answer_start': [92], 'text': ['a golden statue of the Virgin Mary']}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset[\"train\"][\"answers\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140261613437904 acquired on /home/erolm_a/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 579kB/s]\n",
      "INFO:filelock:Lock 140261613437904 released on /home/erolm_a/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening under the hood, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]''answer_end'])\n",
    "\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForMaskedLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f6705d6c8a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-trained model (weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# If you have a GPU, put everything on cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertForMaskedLM' is not defined"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what happens with 2?\n",
    "\n",
    "tokenized_text[masked_index+2] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', '[MASK]', 'puppet', '##eer', '[SEP]']\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens]).to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors, output_hidden_states=True)\n",
    "    \n",
    "    for hid in outputs.hidden_states:\n",
    "        print(hid.size())\n",
    "    predictions = outputs.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['henson', 'a']\n"
     ]
    }
   ],
   "source": [
    "predicted_index_1 = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_index_2 = torch.argmax(predictions[0, masked_index+2]).item()\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens([predicted_index_1, predicted_index_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merolm_a\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.15<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">rural-terrain-20</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/erolm_a/EntitiesAsExperts\" target=\"_blank\">https://wandb.ai/erolm_a/EntitiesAsExperts</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/erolm_a/EntitiesAsExperts/runs/1k8ic0h6\" target=\"_blank\">https://wandb.ai/erolm_a/EntitiesAsExperts/runs/1k8ic0h6</a><br/>\n",
       "                Run data is saved locally in <code>/nfs/colla-framework/notebooks/wandb/run-20210130_190427-1k8ic0h6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models.training import login\n",
    "\n",
    "login()\n",
    "# TODO: create config update file\n",
    "import wandb\n",
    "wandb.config.squad_epochs = 1\n",
    "wandb.config.squad_batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "squad_metric, squad_v2_metric = datasets.load_metric('squad'), datasets.load_metric('squad_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-d9f717600cc1b8b0.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7/cache-47852b2343608493.arrow\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from tools.dataloaders import SQuADDataloader\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "squad_dataset = SQuADDataloader()\n",
    "\n",
    "\n",
    "def squad_collate_fn(rows):\n",
    "    keys = rows[0].keys()\n",
    "    return {key: [row[key] for row in rows] for key in keys}\n",
    "\n",
    "squad_train_dataset = squad_dataset.train_dataset\n",
    "\n",
    "FULL_FINETUNING=False\n",
    "if not FULL_FINETUNING:\n",
    "    squad_dev_size = int(0.1*len(squad_dataset.train_dataset))\n",
    "    squad_dev_indices = np.random.choice(len(squad_dataset.train_dataset), size=squad_dev_size)\n",
    "    squad_train_sampler = SubsetRandomSampler(squad_dev_indices,\n",
    "                                              generator=torch.Generator().manual_seed(42))\n",
    "    squad_train_dataloader = DataLoader(squad_train_dataset,\n",
    "                                        sampler=squad_train_sampler,\n",
    "                                        batch_size=wandb.config.squad_batch_size,\n",
    "                                        collate_fn=squad_collate_fn)\n",
    "\n",
    "else:\n",
    "    squad_train_dataloader = DataLoader(squad_train_dataset,\n",
    "                                        batch_size=wandb.config.squad_batch_size,\n",
    "                                        collate_fn=squad_collate_fn)\n",
    "\n",
    "squad_validation_dataset = squad_dataset.validation_dataset\n",
    "squad_validation_dataloader = DataLoader(squad_validation_dataset,\n",
    "                                         batch_size=wandb.config.squad_batch_size,\n",
    "                                         collate_fn=squad_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7f8ef0063588>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering, BertModel\n",
    "from torch.nn import Module\n",
    "\n",
    "bert_normal = BertModel.from_pretrained('bert-base-uncased')\n",
    "config = bert_normal.config\n",
    "config.num_labels = 2\n",
    "model_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "class BQAShim(Module):\n",
    "    def __init__(self, model_qa):\n",
    "        super().__init__()\n",
    "        self.model_qa = model_qa\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, start_positions=None, end_positions=None):\n",
    "        output = self.model_qa(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, start_positions=start_positions, end_positions=end_positions)\n",
    "        return output[\"loss\"], output[\"start_logits\"], output[\"end_logits\"]\n",
    "\n",
    "model_qa_shimmed = BQAShim(model_qa).to('cuda')\n",
    "wandb.watch(model_qa_shimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1095 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:752: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not return a \"\n",
      "100%|██████████| 1095/1095 [03:31<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss at epoch 0: 2.2860278220753694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1322/1322 [02:59<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average eval loss at epoch 0: 1.357912663680342\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-afc398272704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m train_model(model_qa_shimmed, squad_train_dataloader, squad_validation_dataloader,\n\u001b[0;32m---> 67\u001b[0;31m                 parse_batch, optimizer, scheduler, squad_epochs, my_metric, gradient_accumulation_factor=1)\n\u001b[0m",
      "\u001b[0;32m/nfs/colla-framework/notebooks/models/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, validation_dataloader, load_from_dataloader, optimizer, scheduler, epochs, metric, gradient_accumulation_factor, automatic_mixed_precision)\u001b[0m\n\u001b[1;32m    176\u001b[0m         tqdm.write(\n\u001b[1;32m    177\u001b[0m             f\"Average eval loss at epoch {epoch}: {avg_validation_loss}\")\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mavg_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_validation_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: compute() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from models.training import train_model, get_optimizer, get_schedule, MetricWrapper\n",
    "import torch\n",
    "\n",
    "squad_epochs = wandb.config.squad_epochs\n",
    "\n",
    "def parse_batch(batch):\n",
    "    input_ids = torch.tensor(batch['input_ids'])\n",
    "    attention_mask = torch.FloatTensor(batch['attention_mask'])\n",
    "    token_type_ids = torch.tensor(batch['token_type_ids'])\n",
    "    start = torch.tensor(batch['answer_start'])\n",
    "    end = torch.tensor(batch['answer_end'])\n",
    "    \n",
    "    return (input_ids, attention_mask, token_type_ids, start, end), (batch,)\n",
    "\n",
    "class SQuADMetric(MetricWrapper):\n",
    "    def __init__(self, squad_dataset: SQuADDataloader):\n",
    "        self.squad_dataset = squad_dataset\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.squad_metric = datasets.load_metric('squad')\n",
    "        self.loss = 0.0\n",
    "    \n",
    "    def add_batch(self, inputs, outputs, loss):\n",
    "        self.loss += float(loss)\n",
    "        \n",
    "        batch_input = inputs[-1]\n",
    "\n",
    "        # outputs = total_loss, answer_start_logits, answer_end_logits\n",
    "        answer_start_logits = outputs[1].detach().cpu()\n",
    "        answer_end_logits = outputs[1].detach().cpu()\n",
    "\n",
    "        answer_starts = torch.argmax(answer_start_logits, 1).tolist()\n",
    "        answer_ends = torch.argmax(answer_end_logits, 1).tolist()\n",
    "\n",
    "        input_ids = inputs[0].detach().cpu().tolist()\n",
    "\n",
    "        prediction_texts = self.squad_dataset.reconstruct_sentences(input_ids, answer_starts, answer_ends)\n",
    "\n",
    "        predictions = [{\n",
    "            \"id\": id,\n",
    "            \"prediction_text\": prediction_text,\n",
    "        } for id, prediction_text in zip(batch_input[\"id\"], prediction_texts)]\n",
    "\n",
    "\n",
    "        references = [{\n",
    "            \"id\": id,\n",
    "            \"answers\": answers\n",
    "        } for id, answers in zip(batch_input[\"id\"], batch_input['answers'])]\n",
    "\n",
    "        self.squad_metric.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "    # return validation loss\n",
    "    def compute(self, epoch: int) -> float:\n",
    "        metric_loss = self.squad_metric.compute()\n",
    "        wandb.log({'exact_match': metric_loss['exact_match'],\n",
    "                     'epoch': epoch,\n",
    "                     'f1': metric_loss['f1'],\n",
    "                     'val_loss': self.loss})\n",
    "\n",
    "my_metric = SQuADMetric(squad_dataset)\n",
    "\n",
    "optimizer = get_optimizer(model_qa)\n",
    "scheduler = get_schedule(squad_epochs, optimizer, squad_train_dataloader)\n",
    "\n",
    "train_model(model_qa_shimmed, squad_train_dataloader, squad_validation_dataloader,\n",
    "                parse_batch, optimizer, scheduler, squad_epochs, my_metric, gradient_accumulation_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
