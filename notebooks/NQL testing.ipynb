{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with NQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N85d540c9388e4f6ab862834743327855 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tools.dumps import get_filename_path, wrap_open\n",
    "from tools.sparql_wrapper import SPARQLDataProviders # for the strip_namespaces() static method\n",
    "                                                    # TODO: move it somewhere else\n",
    "from rdflib import Graph\n",
    "from rdflib.util import guess_format\n",
    "\n",
    "wdpg_path = get_filename_path(\"output/wdpg_10000_common.ttl\")\n",
    "wdpg_format = guess_format(wdpg_path)\n",
    "\n",
    "wdpg_graph = Graph()\n",
    "wdpg_graph.parse(wdpg_path, format=wdpg_format)\n",
    "\n",
    "#print(len(wdpg_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, v, o = next(wdpg_graph.triples((None, None, None)))\n",
    "\n",
    "with wrap_open(\"output/wdpg_10000_common.tsv\", \"w\") as tsv:\n",
    "    for t in wdpg_graph.triples((None, None, None)):\n",
    "        (s_s, v_s, o_s) = [SPARQLDataProviders.strip_namespace(elem) for elem in t]\n",
    "        tsv.write(\"\\t\".join([v_s, s_s, o_s]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.w3.org/ns/lemon/ontolex#lexicalForm\tkgl:5z3sgveco6b7a\tkgl:5z3sgveco6b7a-F0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with wrap_open(\"output/wdpg_10000_common.tsv\") as tsv:\n",
    "    print(tsv.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Until I get spark to work I guess I'll just use pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with wrap_open(\"output/wdpg_10000_common.tsv\") as f:\n",
    "    wdpg_df = pd.read_csv(f, sep='\\t', names=['v', 's', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, extract the types\n",
    "rel_types = wdpg_df['v'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_types = rel_types.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.rdflib_extras import extract_domain_codomain\n",
    "\n",
    "ontology_domcodom = extract_domain_codomain(\"../ontology.owl\")\n",
    "ontology_domcodom['kglprop:definition'] = (\"kgl:Sense\", \"rdfs:Literal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation kglprop:form between kgl:Lexeme and kgl:Form\n",
      "relation kglprop:grammaticalCategory between kgl:Form and kgl:GrammaticalCategory\n",
      "relation kglprop:language between kgl:Lexeme and kgl:Language\n",
      "relation kglprop:pos between kgl:Lexeme and kgl:POS\n",
      "relation kglprop:semanticPointer between kgl:Synset and kgl:Synset\n",
      "relation kglprop:sense between kgl:Lexeme and kgl:Sense\n",
      "relation kglprop:synset between kgl:Lexeme and kgl:Synset\n",
      "relation kglprop:definition between kgl:Sense and rdfs:Literal\n"
     ]
    }
   ],
   "source": [
    "# Then, extract the relation types. Since we are using RDF\n",
    "# we'd have to parse our ontology\n",
    "# The small problem is that our ontology is quite incomplete.\n",
    "\n",
    "# TODO: Move this somewhere else\n",
    "\n",
    "import nql\n",
    "context = nql.NeuralQueryContext()\n",
    "\n",
    "for prop, (domain, codomain) in ontology_domcodom.items():\n",
    "    print(f\"relation {prop} between {domain} and {codomain}\")\n",
    "    context.declare_relation(prop, domain, codomain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch no. 0\n",
      "Processed batch no. 1\n",
      "Processed batch no. 2\n",
      "Processed batch no. 3\n",
      "Processed batch no. 4\n",
      "Processed batch no. 5\n",
      "Processed batch no. 6\n",
      "Processed batch no. 7\n",
      "Processed batch no. 8\n",
      "Processed batch no. 9\n",
      "Processed batch no. 10\n",
      "Processed batch no. 11\n",
      "Processed batch no. 12\n",
      "Processed batch no. 13\n",
      "Processed batch no. 14\n",
      "Processed batch no. 15\n",
      "Processed batch no. 16\n",
      "Processed batch no. 17\n",
      "Processed batch no. 18\n",
      "Processed batch no. 19\n",
      "Processed batch no. 20\n",
      "Processed batch no. 21\n",
      "Processed batch no. 22\n",
      "Processed batch no. 23\n",
      "Processed batch no. 24\n",
      "Processed batch no. 25\n",
      "Processed batch no. 26\n",
      "Processed batch no. 27\n",
      "Processed batch no. 28\n",
      "Processed batch no. 29\n",
      "Processed batch no. 30\n",
      "Processed batch no. 31\n",
      "Processed batch no. 32\n",
      "Processed batch no. 33\n",
      "Processed batch no. 34\n",
      "Processed batch no. 35\n",
      "Processed batch no. 36\n",
      "Processed batch no. 37\n",
      "Processed batch no. 38\n",
      "Processed batch no. 39\n",
      "Processed batch no. 40\n",
      "Processed batch no. 41\n",
      "Processed batch no. 42\n",
      "Processed batch no. 43\n",
      "Processed batch no. 44\n",
      "Processed batch no. 45\n",
      "Processed batch no. 46\n",
      "Processed batch no. 47\n",
      "Processed batch no. 48\n",
      "Processed batch no. 49\n",
      "Processed batch no. 50\n",
      "Processed batch no. 51\n",
      "Processed batch no. 52\n",
      "Processed batch no. 53\n",
      "Processed batch no. 54\n",
      "Processed batch no. 55\n",
      "Processed batch no. 56\n",
      "Processed batch no. 57\n",
      "Processed batch no. 58\n",
      "Processed batch no. 59\n",
      "Processed batch no. 60\n",
      "Processed batch no. 61\n",
      "Processed batch no. 62\n",
      "Processed batch no. 63\n",
      "Processed batch no. 64\n",
      "Processed batch no. 65\n",
      "Processed batch no. 66\n",
      "Processed batch no. 67\n",
      "Processed batch no. 68\n",
      "Processed batch no. 69\n",
      "Processed batch no. 70\n",
      "Processed batch no. 71\n",
      "Processed batch no. 72\n",
      "Processed batch no. 73\n",
      "Processed batch no. 74\n",
      "Processed batch no. 75\n",
      "Processed batch no. 76\n",
      "Processed batch no. 77\n",
      "Processed batch no. 78\n",
      "Processed batch no. 79\n",
      "Processed batch no. 80\n",
      "Processed batch no. 81\n",
      "Processed batch no. 82\n",
      "Processed batch no. 83\n",
      "Processed batch no. 84\n",
      "Processed batch no. 85\n",
      "Processed batch no. 86\n",
      "Processed batch no. 87\n",
      "Processed batch no. 88\n",
      "Processed batch no. 89\n",
      "Processed batch no. 90\n",
      "Processed batch no. 91\n",
      "Processed batch no. 92\n",
      "Processed batch no. 93\n",
      "Processed batch no. 94\n",
      "Processed batch no. 95\n",
      "Processed batch no. 96\n",
      "Processed batch no. 97\n",
      "Processed batch no. 98\n",
      "Processed batch no. 99\n",
      "Processed batch no. 100\n",
      "Processed batch no. 101\n",
      "Processed batch no. 102\n",
      "Processed batch no. 103\n",
      "Processed batch no. 104\n",
      "Processed batch no. 105\n",
      "Processed batch no. 106\n",
      "Processed batch no. 107\n",
      "Processed batch no. 108\n",
      "Processed batch no. 109\n",
      "Processed batch no. 110\n",
      "Processed batch no. 111\n",
      "Processed batch no. 112\n",
      "Processed batch no. 113\n",
      "Processed batch no. 114\n",
      "Processed batch no. 115\n",
      "Processed batch no. 116\n",
      "Processed batch no. 117\n",
      "Processed batch no. 118\n",
      "Processed batch no. 119\n",
      "Processed batch no. 120\n",
      "Processed batch no. 121\n",
      "Processed batch no. 122\n",
      "Processed batch no. 123\n",
      "Processed batch no. 124\n",
      "Processed batch no. 125\n",
      "Processed batch no. 126\n",
      "Processed batch no. 127\n",
      "Processed batch no. 128\n",
      "Processed batch no. 129\n",
      "Processed batch no. 130\n",
      "Processed batch no. 131\n",
      "Processed batch no. 132\n",
      "Processed batch no. 133\n",
      "Processed batch no. 134\n",
      "Processed batch no. 135\n",
      "Processed batch no. 136\n",
      "Processed batch no. 137\n",
      "Processed batch no. 138\n",
      "Processed batch no. 139\n",
      "Processed batch no. 140\n",
      "Processed batch no. 141\n",
      "Processed batch no. 142\n",
      "Processed batch no. 143\n",
      "Processed batch no. 144\n",
      "Processed batch no. 145\n",
      "Processed batch no. 146\n",
      "Processed batch no. 147\n",
      "Processed batch no. 148\n",
      "Processed batch no. 149\n",
      "Processed batch no. 150\n",
      "Processed batch no. 151\n",
      "Processed batch no. 152\n",
      "Processed batch no. 153\n",
      "Processed batch no. 154\n",
      "Processed batch no. 155\n",
      "Processed batch no. 156\n",
      "Processed batch no. 157\n",
      "Processed batch no. 158\n",
      "Processed batch no. 159\n",
      "Processed batch no. 160\n",
      "Processed batch no. 161\n",
      "Processed batch no. 162\n",
      "Processed batch no. 163\n",
      "Processed batch no. 164\n",
      "Processed batch no. 165\n",
      "Processed batch no. 166\n",
      "Processed batch no. 167\n",
      "Processed batch no. 168\n",
      "Processed batch no. 169\n",
      "Processed batch no. 170\n",
      "Processed batch no. 171\n",
      "Processed batch no. 172\n",
      "Processed batch no. 173\n",
      "Processed batch no. 174\n"
     ]
    }
   ],
   "source": [
    "wdpg_filtered = wdpg_df[wdpg_df['v'].isin(pd.Series(list(ontology_domcodom.keys())))]\n",
    "\n",
    "for idx, (_, row) in enumerate(wdpg_filtered.iterrows()):\n",
    "    context.load_kg([[\"\\t\".join(row)]])\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"Processed batch no. {idx // 1000}\")\n",
    "\n",
    "#wdpg_filtered = wdpg_df.loc[wdpg_df['v'] in ontology_domcodom.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.one(\"kgl:el6c3yc77ptlm-S0\", \"kgl:Sense\").follow(\"kglprop:sense\", -1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constancy\n",
    "\n",
    "example_word = context.one(\"kgl:el6c3yc77ptlm\", \"kgl:Lexeme\")\n",
    "example_word.follow(\"kglprop:form\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.one(\"kgl:el6c3yc77ptlm-F0\", \"kgl:Form\").follow(\"kglprop:form\", -1).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor([[31026  6427]], shape=(1, 2), dtype=int64), values=tf.Tensor([1.], shape=(1,), dtype=float32), dense_shape=tf.Tensor([31027 13428], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "context.get_tf_tensor(\"kglprop:form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "forms = context.all(\"kgl:Form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "x.filtered_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
